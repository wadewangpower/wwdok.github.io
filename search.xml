<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>RNN、LSTM、Transformer</title>
    <url>/2021/01/30/RNN%E3%80%81LSTM%E3%80%81Transformer/</url>
    <content><![CDATA[<p>最近频繁被Tansformer刷屏，于是我决定学习一下Transformer，看看它的厉害之处在哪里。</p>
<p>本篇博客就像是一个链接中枢，我会放很多我认为不错的教程链接，因为到现在，网上已经有不少好教程了，我不会再摘抄一遍它们的内容，我只会放上链接，加上一点个人的理解补充。</p>
<p>Shusen Wang做了一个Transformer的youtube系列视频：《<a href="https://www.youtube.com/watch?v=aButdUV0dxI&amp;list=PLvOO0btloRntpSWSxFbwPIjIum3Ub4GSC">Transformer模型</a>》，但是在学习这个系列视频之前，还需要对RNN、LSTM、Attention这些专业术语有所了解，这就需要再去看一下他的另一个YouTube系列视频：<a href="https://www.youtube.com/watch?v=NWcShtqr8kc&amp;list=PLvOO0btloRnuTUGN4XqO85eKPeFSZsEqK">《RNN模型与NLP应用》</a>（建议1.5倍速观看哈）。</p>
<p>本博客将以这两个系列视频为主线展开，对其中的一些知识点进行补充说明。</p>
<ul>
<li><p>《RNN模型与NLP应用(1/9)：数据处理基础》：这一节比较简单，无需补充说明</p>
</li>
<li><p>《RNN模型与NLP应用(2/9)：文本处理与词嵌入》：</p>
<p>本节首次提到了embedding这个重要概念，但介绍的不多，更多embedding的资料可以参考这篇<a href="https://blog.csdn.net/weixin_42078618/article/details/84553940">博客</a>以及网上的其他一些资料。</p>
<p>视频中有一页ppt是下图这样的，我这里注释了x1和x2，这样从左边的Parameter matrix到右边的坐标位置就很简单易懂了。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2021/01/30/d6jkSXZOEMvplri.png" alt="image-20210130172025692" style="zoom: 67%;" /></p>
<ul>
<li><p>《RNN模型与NLP应用(3/9)：Simple RNN模型》：</p>
<p>这一节跟下一节视频用到的一些插图来自这篇英语博客<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">《Understanding LSTM Networks》</a>，这篇博客强烈推荐阅读，英语不太好的可以搭配看这篇翻译版<a href="https://www.jianshu.com/p/95d5c461924c">博客</a>。插图中C和h的含义，C代表Cell state （也有人说是Conveyor）， h代表Hidden state。</p>
<p>由下图圆圈可知，上一个RNN模块传给下一个RNN模块的东西就是hidden state，同一个东西拷贝了两份而已：</p>
<p><img src="https://i.loli.net/2021/01/30/SfY6Ja4EPqpOTu1.png" alt="image-20210130231449052" style="zoom:50%;" /></p>
<p>而下面要介绍的LSTM每个模块输出的不仅有hidden state，还有cell state。</p>
</li>
<li><p>《RNN模型与NLP应用(4/9)：LSTM模型》：</p>
<p>补充一些个人对这篇英语博客的理解：sigmoid函数输出（0，1）中间的数值，适合用来决定矩阵元素被保留多少，而tanh函数输出范围是（-1，1），适合用来决定更新量是多少，因为更新肯定有增有减，所以用tanh。</p>
</li>
<li><p>《RNN模型与NLP应用(5/9)：多层RNN、双向RNN、预训练》：</p>
</li>
</ul>
<p>这一节主要讲了通过RNN的变体和预训练来提高模型的推理准确率，视频中8:27提到embedding的参数很大，而训练数据很少，容易导致过拟合，这一点我深有感受，因为我之前用kinetics400的模型训练我的4个动作类别，就感受到了过拟合。</p>
<ul>
<li>《RNN模型与NLP应用(6/9)：Text Generation (自动文本生成)》：</li>
</ul>
<p>这一节讲得很通俗易懂，无需补充说明</p>
<ul>
<li>《RNN模型与NLP应用(7/9)：机器翻译与Seq2Seq模型》：</li>
</ul>
<p>Seq2Seq这个模型的原理一个视频的讲解量应该不够，大家可以再看一下B站的这个<a href="https://www.bilibili.com/video/BV1Eb411J7Qm?p=1">视频</a>作为补充，同样建议1.5倍速播放。</p>
<ul>
<li>《RNN模型与NLP应用(8/9)：Attention (注意力机制)》：</li>
</ul>
<p>不是说LSTM就是用来解决记忆久远信息的吗，怎么Seq2Seq还会有记不住太长信息的问题？我的理解是因为不管Encoder输入的词量是多少，Encoder和Decoder中间的那个语义向量Thought Vector的长度总是固定不变的，如果Encoder输入的词量非常多，那么Thought   Vector就容不下那么多语义信息。</p>
<p><img src="https://camo.githubusercontent.com/e63bf968fbd0095c5474d241434a72b4a59df473860857a3871039a73eab2bae/68747470733a2f2f73757269796164656570616e2e6769746875622e696f2f696d672f736571327365712f73657132736571322e706e67" alt="img"></p>
<p>借用下图说明一下这一点，图中800×800和3000×3000代表输入Encoder的不同长度的词量，100KB就是Thought Vector的长度，可见过于密集的输入信息最后编码成Thought Vector必然会变得稀疏：</p>
<p><img src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-yasuo.png" alt="Encoder-Decoder的缺点：输入过长会损失信息" style="zoom:50%;" /></p>
<p>而Attention会考虑到每一个输入词向量产生的$h_i$，有多少个输入词向量，后面Decoder更新每个$s_i$时就考虑多少个Encoder的$h_i$。</p>
<ul>
<li><p>《RNN模型与NLP应用(9/9)：Self-Attention (自注意力机制)》：</p>
<p>听过前面的视频，这个视频的内容不难理解。</p>
</li>
</ul>
<p>———————————————————————分割线————————————————————————-</p>
<p>了解完RNN和LSTM，现在你应该对NLP领域的一些名词有些概念了，开始学习Transformer。</p>
<p>网上有很多讲解Transoformer的博客，但很多博客的内容和图片都来自这篇英文博客《<a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>》，这位<a href="http://jalammar.github.io/">Jay Alammar</a>的博客主页还有很多利用可视化图片视频讲解机器学习原理的博客，推荐各位去看一看！</p>
<p>在看完Shusen Wan的<a href="https://www.youtube.com/watch?v=aButdUV0dxI&amp;list=PLvOO0btloRntpSWSxFbwPIjIum3Ub4GSC">Transformer模型</a>系列视频，我还是很难完全理解positional encoding的原理，在youtube搜索positional encoding时，我遇到了另一个宝藏博主 - <a href="https://www.youtube.com/channel/UCmUi2gnk9EbJQjvWo2VdZCA">Hedu - Mathematics of Intelligence</a>，目前她推出的<a href="https://www.youtube.com/playlist?list=PL86uXYUJ7999zE8u2-97i4KG_2Zpufkfb">Visual Guide to Transformer Neural Networks</a>系列视频包含3部分：</p>
<ul>
<li><p><a href="https://www.youtube.com/watch?v=dichIcUZfOw&amp;t=611s">Visual Guide to Transformer Neural Networks - (Part 1) Position Embeddings</a></p>
</li>
<li><p><a href="https://www.youtube.com/watch?v=mMa2PmYJlCo">Visual Guide to Transformer Neural Networks - (Part 2) Multi-Head &amp; Self-Attention</a></p>
</li>
<li><p><a href="https://www.youtube.com/watch?v=gJ9kaJsE78k&amp;t=2s">Visual Guide to Transformer Neural Networks - (Part 3) Decoder’s Masked Attention</a></p>
</li>
</ul>
<p>其中(Part 1) Position Embeddings就很生动易懂地解释了位置编码地原理（有一定基础的人可以从6:04分开始观看），而她地(Part 2) Multi-Head &amp; Self-Attention视频则讲清楚了为什么两个矩阵相乘可以用来求相似度（不像其他博客都想当然），总之，你可以先去看看其他人的Transformer博客，再去看Hedu - Mathematics of Intelligence的视频，你就知道她的视频有多好了！</p>
<p>拓展链接：</p>
<p><a href="https://www.youtube.com/watch?v=4Bdc55j80l8">《Illustrated Guide to Transformers Neural Network: A step by step explanation》</a></p>
<p><a href="https://www.youtube.com/watch?v=TrdevFK_am4">《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)》</a></p>
<p><a href="https://www.youtube.com/watch?v=TQQlZhbC5ps">《Transformer Neural Networks - EXPLAINED! (Attention is all you need)》</a></p>
<h2 id="Transformer-在CV领域的应用"><a href="#Transformer-在CV领域的应用" class="headerlink" title="Transformer 在CV领域的应用"></a>Transformer 在CV领域的应用</h2><p>facebook 提出了用transformer执行目标检测的<strong>DETR</strong> (<strong>DE</strong>tection <strong>TR</strong>ansformer)</p>
<p><a href="https://github.com/facebookresearch/detr">https://github.com/facebookresearch/detr</a></p>
<p>我在跑colab notebook时，发现<code>detr_demo.ipynb</code>的效果不是很好，如下图所示：</p>
<p><img src="https://user-images.githubusercontent.com/43233772/110281620-0a946580-8018-11eb-91d4-4b123419e9df.png" alt="image"></p>
<p>而 <code>detr_attention.ipynb</code> 的效果则正常得多了：</p>
<p><img src="https://user-images.githubusercontent.com/43233772/110487672-c3e26080-8128-11eb-96e1-7090395ee6ba.png" alt="下载"></p>
<p>测试原图在<a href="https://drive.google.com/file/d/1X8fC8ZtlH_S7WQVE6jSeXL7jr4oQPDYD/view?usp=sharing">这里</a>。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>DeepStream</title>
    <url>/2020/12/20/DeepStream/</url>
    <content><![CDATA[<p>门户网站：<a href="https://developer.nvidia.com/deepstream-sdk">https://developer.nvidia.com/deepstream-sdk</a></p>
<h1 id="DeepStream-入门"><a href="#DeepStream-入门" class="headerlink" title="DeepStream 入门"></a><strong>DeepStream</strong> 入门</h1><h2 id="欢迎使用DeepStream文档"><a href="#欢迎使用DeepStream文档" class="headerlink" title="欢迎使用DeepStream文档"></a>欢迎使用DeepStream文档</h2><h3 id="NVIDIA-DeepStream概述"><a href="#NVIDIA-DeepStream概述" class="headerlink" title="NVIDIA DeepStream概述"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#nvidia-deepstream-overview">NVIDIA DeepStream概述</a></h3><p>DeepStream是一个流分析工具包，用于构建AI驱动的应用程序。它以流数据作为输入, 这些流数据来自USB / CSI摄像机，来自视频文件或基于RTSP的流，并使用AI和计算机视觉从像素生成洞察力，以更好地了解环境。DeepStream SDK可以用作许多视频分析解决方案的基础层，例如了解智慧城市中的交通和行人，医院中的健康和安全监控，零售中的自检和分析，检测制造工厂中的组件缺陷等。</p>
<p><img src="https://i.loli.net/2020/12/20/NfboknCLUWxQ1mh.jpg" alt="img"></p>
<p>DeepStream通过Python bindings支持C / C ++和Python的应用程序开发。为了使入门更加容易，DeepStream附带了C / C ++和Python中的多个参考应用程序。请参阅<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html">C / C ++示例应用程序详细信息</a>和<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html">Python示例应用程序详细信息</a>，以了解有关可用应用程序的更多信息。有关某些DeepStream参考应用程序，请参见<a href="https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps">NVIDIA-AI-IOT</a> Github页面。</p>
<p>核心SDK由几个硬件加速器插件组成，这些插件使用各种加速器，例如VIC，GPU，DLA，NVDEC和NVENC。通过在专用加速器中执行所有计算繁重的操作，DeepStream可以为视频分析应用程序实现最高性能。DeepStream的关键功能之一是边缘和云之间的安全双向通信。DeepStream附带了几种现成的安全协议，例如使用用户名/密码的SASL /普通身份验证和2路TLS身份验证。要了解有关这些安全功能的更多信息，请阅读 <a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_IoT.html"><strong>IoT</strong></a> 章节。要了解有关双向功能的更多信息，请参阅本指南中的“<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_IoT.html#bi-directional-label">双向消息传递</a>”部分。</p>
<p>DeepStream建立在CUDA-X堆栈的多个NVIDIA库的基础上（如上图所示），例如有CUDA，TensorRT，Triton Inference服务器和多媒体库。TensorRT加速了NVIDIA GPU上的AI推理。DeepStream在插件中抽象了这些库，使开发人员可以轻松地构建视频分析管道，而不必学习所有单独的库。</p>
<p>DeepStream针对NVIDIA GPU进行了优化，应用程序可以部署在运行Jetson平台的嵌入式边缘设备上，也可以部署在较大的边缘或数据中心GPU（例如T4）上。可以使用NVIDIA容器运行时将DeepStream应用程序部署在容器中。这些容器可在NGC（NVIDIA GPU Cloud Registry）上找到。要了解有关使用docker进行部署的更多信息，请参阅Docker容器一章。可以使用GPU上的Kubernetes在边缘上编排（orchestrate）DeepStream应用程序。NGC上提供了用于部署DeepStream应用程序的示例<a href="https://ngc.nvidia.com/catalog/helm-charts/nvidia:video-analytics-demo">Helm chart</a>。</p>
<h3 id="DeepStream图架构"><a href="#DeepStream图架构" class="headerlink" title="DeepStream图架构"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#deepstream-graph-architecture">DeepStream图架构</a></h3><p>DeepStream是使用开源GStreamer框架构建的优化图形架构。下图显示了典型的视频分析应用程序，从输入视频到输出结果，所有单独的块都是使用到的各种插件。底部是在整个应用程序中使用的不同硬件引擎。插件之间的零内存复制以及使用各种加速器的最佳内存管理确保了最高性能。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/93942a719fb53d5de777aee577f2c268.png" alt=""></p>
<p>DeepStream以GStreamer插件的形式提供了构建基块，可用于构建有效的视频分析管道。有15个以上的插件可以通过硬件加速完成各种任务。</p>
<ol>
<li><p>数据流可以通过RTSP或来自本地文件系统或直接来自摄像机的网络来传输。使用CPU捕获流。一旦帧进入内存，就使用NVDEC加速器发送它们以进行解码。用于解码的插件称为<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvvideo4linux2.html">Gst-nvvideo4linux2</a>。</p>
</li>
<li><p>解码后，有一个可选的图像预处理步骤。预处理可以是图像变形或色彩空间转换。<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvdewarper.html">Gst-nvdewarper</a>插件可以使鱼眼镜头或360度相机的图像变形。<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvvideoconvert.html">Gst-nvvideoconvert</a>插件可以在框架上执行颜色格式转换。这些插件使用GPU或VIC（视觉图像合成器）。</p>
</li>
<li><p>下一步是批处理帧以获得最佳推理性能。批处理使用<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvstreammux.html">Gst-nvstreammux</a>插件完成。</p>
</li>
<li><p>批处理帧后，将其发送以进行推理。可以使用NVIDIA的推理加速器运行时TensorRT进行推理，也可以使用Triton推理服务器在本机框架（如TensorFlow或PyTorch）中进行推理。本地TensorRT推理是使用<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html">Gst-nvinfer</a>插件实现，用Triton推理是通过<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinferserver.html">Gst-nvinferserver</a>插件实现。对于Jetson AGX Xavier和Xavier NX，推理可以使用GPU或DLA（深度学习加速器）。</p>
</li>
<li><p>推断之后，下一步可能涉及跟踪对象。SDK中有多个内置参考跟踪器，范围从高性能到高精度。使用<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvtracker.html">Gst-nvtracker</a>插件执行对象跟踪。</p>
</li>
<li><p>为了创建可视化工件，例如边界框，分割蒙版，标签，有一个名为<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvdsosd.html">Gst-nvdsosd</a>的可视化插件。</p>
</li>
<li><p>最后，要输出结果，DeepStream提供了各种选项：在屏幕上用边框显示输出，将输出保存到本地磁盘，通过RTSP进行流传输或仅将元数据发送到云。为了将元数据发送到云，DeepStream使用Gst-nvmsgconv和Gst-nvmsgbroker插件。<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvmsgconv.html">Gst-nvmsgconv</a>将元数据转换为架构有效负载，而<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvmsgbroker.html">Gst-nvmsgbroker</a>建立与云的连接并发送遥测数据。有几种内置的代理协议，例如Kafka，MQTT，AMQP和Azure IoT。你可以创建自定义代理适配器（broker adapters）。</p>
</li>
</ol>
<h3 id="DeepStream参考应用"><a href="#DeepStream参考应用" class="headerlink" title="DeepStream参考应用"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#deepstream-reference-app">DeepStream参考应用</a></h3><p>为了上手deepstream，开发人员可以使用我们提供的参考应用程序，这些应用程序包括了它们的源代码。我么这里称呼端到端应用程序称为deepstream-app。该应用程序是完全可配置的-它允许用户配置任何类型和数量的源，配置运行推理的神经网络类型，它预先内置了一个推理插件来进行目标检测，还有一个配置目标跟踪器的选项。对于输出，用户可以选择在屏幕上渲染，保存输出文件或通过RTSP传输视频。</p>
<p><img src="https://img-blog.csdnimg.cn/20201220103254473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NjQ5MDcy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这是开始学习DeepStream功能的很好的参考应用程序。<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_ref_app_deepstream.html">DeepStream参考应用程序-deepstream-app</a>一章将更详细地介绍此应用程序。该应用程序的源代码位于<code>/opt/nvidia/deepstream/deepstream-5.0/sources/apps/sample_apps/deepstream-app</code>。该应用程序适用于所有AI模型，并在各个README文件中提供详细说明。性能基准测试也使用此应用程序运行。</p>
<h3 id="建立应用程序入门"><a href="#建立应用程序入门" class="headerlink" title="建立应用程序入门"></a>建立应用程序入门</h3><p>对于希望构建自定义应用程序的开发人员而言，刚开始开发deepstream-app可能会有些不知所措。SDK附带了几个简单的应用程序，开发人员可以在其中学习DeepStream的基本概念，构造一个简单的管道，然后逐步构建更复杂的应用程序。</p>
<p><img src="https://i.loli.net/2020/12/20/y46GqoNKIYJ2TDt.png" alt="img"></p>
<p>开发人员可以从deepstream-test1开始，它几乎像DeepStream的hello world。在此应用程序中，开发人员将学习如何使用各种DeepStream插件构建GStreamer管道。他们将从文件中获取视频，进行解码，批处理，然后进行对象检测，最后在屏幕上呈现这些框。deepstream-test2在test1的目标检测后面增加了目标分类。deepstream-test3展示了如何添加多个视频源，最后test4将演示如何使用消息代理插件为IoT服务。这4个入门应用程序在本机C / C ++和Python中均可用。要了解有关DeepStream中这些应用程序和其他示例应用程序的更多信息，请参见<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html">C / C ++示例应用程序源详细信息</a>和<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html">Python示例应用程序源详细信息</a>.</p>
<h3 id="用Python的DeepStream"><a href="#用Python的DeepStream" class="headerlink" title="用Python的DeepStream"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#deepstream-in-python">用Python的DeepStream</a></h3><p>NVIDIA引入了GStreamer框架的Python绑定 - Gst-Python，以帮助您使用Python构建高性能的AI应用程序。</p>
<p><img src="https://i.loli.net/2020/12/20/oNgUtAyYQ4r8sPM.png" alt="img"></p>
<p>DeepStream Python应用程序使用Gst-Python API构造管道，并使用probe函数访问管道中各个点的数据。数据类型全部是原生C语言（上图灰色部分），并且需要通过PyBindings或NumPy层才能从Python应用程序访问它们。张量数据是推断后得出的原始张量输出。如果要检测对象，则需要通过解析和聚类算法对该张量数据进行后处理，以在检测到的对象周围创建边界框。要开始使用Python，请参阅本指南中的<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html">Python示例应用程序源详细信息</a>以及DeepStream Python API指南中的“ DeepStream Python”。</p>
<h2 id="快速入门指南"><a href="#快速入门指南" class="headerlink" title="快速入门指南"></a>快速入门指南</h2><h3 id="Jetson准备"><a href="#Jetson准备" class="headerlink" title="Jetson准备"></a>Jetson准备</h3><h4 id="安装Jetson-SDK组件"><a href="#安装Jetson-SDK组件" class="headerlink" title="安装Jetson SDK组件"></a>安装Jetson SDK组件</h4><p>从以下位置下载NVIDIA SDK Manager，您将用它来安装JetPack 4.4 GA（对应于L4T 32.4.3版本）：</p>
<p><a href="https://developer.nvidia.com/embedded/jetpack">https://developer.nvidia.com/embedded/jetpack</a></p>
<ul>
<li>NVIDIA SDK Manager是一个图形界面应用程序，可刷新并安装JetPack软件包。</li>
<li>根据主机系统的不同，刷新过程大约需要10-30分钟。</li>
<li>如果您使用的是Jetson Nano或Jetson Xavier NX开发者套件，则可以从<a href="https://developer.nvidia.com/embedded/jetpack下载SD卡镜像，它已经随附了CUDA，TensorRT和cuDNN。">https://developer.nvidia.com/embedded/jetpack下载SD卡镜像，它已经随附了CUDA，TensorRT和cuDNN。</a></li>
</ul>
<h4 id="安装依赖项"><a href="#安装依赖项" class="headerlink" title="安装依赖项"></a>安装依赖项</h4><p>输入以下命令以安装必备软件包：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt install \</span><br><span class="line">libssl1.0.0 \</span><br><span class="line">libgstreamer1.0-0 \</span><br><span class="line">gstreamer1.0-tools \</span><br><span class="line">gstreamer1.0-plugins-good \</span><br><span class="line">gstreamer1.0-plugins-bad \</span><br><span class="line">gstreamer1.0-plugins-ugly \</span><br><span class="line">gstreamer1.0-libav \</span><br><span class="line">libgstrtspserver-1.0-0 \</span><br><span class="line">libjansson4&#x3D;2.11-1</span><br></pre></td></tr></table></figure>
<h4 id="安装librdkafka（为消息代理启用Kafka协议适配器）"><a href="#安装librdkafka（为消息代理启用Kafka协议适配器）" class="headerlink" title="安装librdkafka（为消息代理启用Kafka协议适配器）"></a>安装librdkafka（为消息代理启用Kafka协议适配器）</h4><ol>
<li>从GitHub克隆librdkafka存储库：</li>
</ol>
<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;edenhill&#x2F;librdkafka.git</span><br></pre></td></tr></table></figure>
</blockquote>
<ol>
<li><p>配置和构建库：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd librdkafka</span><br><span class="line">$ git reset --hard 7101c2310341ab3f4675fc565f64f0967e135a6a</span><br><span class="line">.&#x2F;configure</span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>将生成的库复制到deepstream目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo mkdir -p &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream-5.0&#x2F;lib</span><br><span class="line">$ sudo cp &#x2F;usr&#x2F;local&#x2F;lib&#x2F;librdkafka* &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream-5.0&#x2F;lib</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="安装NVIDIA-V4L2-GStreamer插件"><a href="#安装NVIDIA-V4L2-GStreamer插件" class="headerlink" title="安装NVIDIA V4L2 GStreamer插件"></a>安装NVIDIA V4L2 GStreamer插件</h4><ol>
<li><p>在文本编辑器中打开apt源配置文件，例如： <code>$ sudo vi /etc/apt/sources.list.d/nvidia-l4t-apt-source.list</code></p>
</li>
<li><p>在如下所示的deb命令中更改存储库名称并下载URL：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb https:&#x2F;&#x2F;repo.download.nvidia.com&#x2F;jetson&#x2F;common r32.4 main</span><br><span class="line">deb https:&#x2F;&#x2F;repo.download.nvidia.com&#x2F;jetson&#x2F;&lt;platform&gt; r32.4 main</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><p>其中<platform>标识平台的处理器：</p>
<p><code>t186</code> 用于Jetson TX2系列，<code>t194</code> 适用于Jetson AGX Xavier系列或Jetson Xavier NX，<code>t210</code> 用于Jetson Nano或Jetson TX1</p>
</li>
</ul>
<p>例如，如果您的平台是Jetson Xavier NX：</p>
<ul>
<li>deb <a href="https://repo.download.nvidia.com/jetson/common">https://repo.download.nvidia.com/jetson/common</a> r32.4 main</li>
<li>deb <a href="https://repo.download.nvidia.com/jetson/t194">https://repo.download.nvidia.com/jetson/t194</a> r32.4 main</li>
</ul>
</blockquote>
<ol>
<li><p>保存并关闭源配置文件。</p>
</li>
<li><p>输入命令：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install --reinstall nvidia-l4t-gstreamer</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果apt提示您选择配置文件，请选择<code>Y</code>（YES）（以使用NVIDIA更新版本的文件）。</p>
<p>注意：从SDK Manager刷新Jetson OS后，应更新NVIDIA V4L2 GStreamer插件。</p>
</blockquote>
<h4 id="安装DeepStream-SDK"><a href="#安装DeepStream-SDK" class="headerlink" title="安装DeepStream SDK"></a>安装DeepStream SDK</h4><ul>
<li><p><strong>方法1</strong>：使用SDK Manager</p>
<p>从<code>Additional SDKs</code>中选择DeepStreamSDK以及 JP 4.4 软件组件进行安装。</p>
</li>
<li><p><strong>方法2</strong>：使用DeepStream tar包</p>
<ol>
<li><p>将DeepStream 5.0 Jetson tar包<code>deepstream_sdk_v5.0.1_jetson.tbz2</code>下载到Jetson设备。</p>
</li>
<li><p>输入以下命令以提取并安装DeepStream SDK：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo tar -xvf deepstream_sdk_v5.0.1_jetson.tbz2 -C &#x2F;</span><br><span class="line">$ cd &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream-5.0</span><br><span class="line">$ sudo .&#x2F;install.sh</span><br><span class="line">$ sudo ldconfig</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>方法3</strong>：使用DeepStream Debian软件包</p>
<p>将DeepStream 5.0 Jetson Debian软件包<code>deepstream-5.0_5.0.1-1_arm64.deb</code>下载到Jetson设备。然后输入命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install .&#x2F;deepstream-5.0_5.0.1-1_arm64.deb</span><br></pre></td></tr></table></figure>
<p>注意：如果使用<code>dpkg</code>命令安装DeepStream SDK Debian软件包，则必须在安装DeepStream deb软件包之前安装以下软件包：</p>
<blockquote>
<ul>
<li><code>libgstrtspserver-1.0-0</code></li>
<li><code>libgstreamer-plugins-base1.0-dev</code></li>
</ul>
</blockquote>
</li>
<li><p><strong>方法4</strong>：使用apt服务器</p>
<ol>
<li><p>使用类似于以下命令的命令在文本编辑器中打开apt源配置文件。</p>
<p><code>$ sudo vi /etc/apt/sources.list.d/nvidia-l4t-apt-source.list</code></p>
</li>
<li><p>在如下所示的deb命令中更改存储库名称并下载URL：</p>
<p> <code>deb https://repo.download.nvidia.com/jetson/common r32.4 main</code></p>
</li>
<li><p>保存并关闭源配置文件。</p>
<ol>
<li><p>输入命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install deepstream-5.0</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>方法5</strong>：使用NGC上的DeepStream Docker容器。请参阅<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_docker_containers.html">Docker容器</a>部分以了解有关使用Docker容器开发和部署DeepStream的信息。</p>
</li>
</ul>
<h4 id="运行deepstream-app（参考应用程序）"><a href="#运行deepstream-app（参考应用程序）" class="headerlink" title="运行deepstream-app（参考应用程序）"></a>运行deepstream-app（参考应用程序）</h4><ol>
<li>导航到<code>samples</code>目录</li>
<li>输入以下命令以运行参考应用程序：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ deepstream-app -c &lt;path_to_config_file&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>注意：</p>
<ul>
<li><path_to_config_file>是参考应用程序配置文件之一的路径名，您可以在<code>/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/</code>目录下找到示例配置文件。 输入此命令<code>$ deepstream-app --help</code>以查看应用程序用法。</li>
<li>要保存TensorRT Engine / Plan文件，请运行以下命令：<br><code>$ sudo deepstream-app -c &lt;路径配置文件&gt;</code></li>
</ul>
<p>​       ???这不和上面一样吗，到底这个命令能干啥？暂时不知道</p>
<ul>
<li>要在2D平铺显示视图中显示标签，请在源上单击鼠标左键以展开感兴趣的源。 要返回平铺显示，请在窗口中的任意位置单击鼠标右键。</li>
<li>也支持键盘来选择源。 在运行应用程序的控制台上，按<code>z</code>键，然后按所需的行索引（0到9），然后按列索引（0到9）以展开源。 要恢复2D平铺显示视图，请再次按<code>z</code>。</li>
</ul>
<h4 id="提高时钟"><a href="#提高时钟" class="headerlink" title="提高时钟"></a>提高时钟</h4><p>安装DeepStream SDK之后，请在Jetson设备上运行以下命令以提高时钟：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo nvpmodel -m 0</span><br><span class="line">$ sudo jetson_clocks</span><br></pre></td></tr></table></figure>
<h4 id="运行预编译的示例应用程序"><a href="#运行预编译的示例应用程序" class="headerlink" title="运行预编译的示例应用程序"></a>运行预编译的示例应用程序</h4><ol>
<li><p>导航到<code>sources/apps/sample_apps</code>选定的应用程序目录中。</p>
</li>
<li><p>按照目录的README文件运行该应用程序。</p>
<blockquote>
<p>注意:</p>
<p>如果应用程序遇到错误且无法创建Gst元素，请删除GStreamer缓存，然后重试。要删除GStreamer缓存，请输入以下命令：</p>
<p> <code>$ rm $&#123;HOME&#125;/.cache/gstreamer-1.0/registry.aarch64.bin</code></p>
<p>当运行应用程序用到的模型没有现有的引擎文件时，可能要花费几分钟的时间，具体取决于要生成引擎文件的平台和模型。为了方便以后运行，可以再次使用这些生成的引擎文件以加快加载速度。</p>
</blockquote>
</li>
</ol>
<h3 id="适用于Ubuntu的dGPU设置"><a href="#适用于Ubuntu的dGPU设置" class="headerlink" title="适用于Ubuntu的dGPU设置"></a>适用于Ubuntu的dGPU设置</h3><p>本节说明在安装DeepStream SDK之前如何准备使用了NVIDIA dGPU设备的<code>Ubuntu x86_64</code>系统。</p>
<blockquote>
<p>注意 :</p>
<p>本文档使用术语dGPU（“discrete GPU”）来指代NVIDIA GPU扩展卡产品，例如NVIDIA Tesla®T4和P4，NVIDIA GeForce® GTX 1080和NVIDIA GeForce® RTX2080。此版本的DeepStream SDK运行在x86_64平台上的特定的dGPU产品。支持NVIDIA驱动程序450.51和NVIDIA TensorRT™7.0及更高版本。</p>
</blockquote>
<p>您必须安装以下组件：Ubuntu 18.04，GStreamer 1.14.1，NVIDIA驱动程序450.51，CUDA 10.2，TensorRT 7.0.X。</p>
<p>以下内容我就不翻译了，我改成用自己的话来写。<br>我的环境是Ubuntu18.04, tensorrt 7.2.1.6, cuda 10.2.89。<br><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html">official docs</a>里有详细的安装步骤教程，分<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html">jetson版</a>和<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html">dGPU版</a>，我最初是想在ubuntu上先安装deepstream，走完一个流程看看，所以我选择的是dGPU的安装教程。文档上提供了4种安装方式，我选择的是第2种方式，用tar包，第1种用deb包的方式我这边有报错。注意点有：</p>
<ol>
<li>确保安装路径是：/opt/nvidia/deepstream/deepstream-5.0，<code>sudo tar -xvf deepstream_sdk_v5.0.1_jetson.tbz2 -C /</code>这句命令最后的<code>-C /</code>是关键。</li>
<li>执行 sudo ldconfig时，我遇到了下面的警告：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(base) weidawang@weidawang-TUF-Gaming-FX506LU-FX506LU:/opt/nvidia/deepstream/deepstream-<span class="number">5.0</span>$ sudo ldconfig</span><br><span class="line">[sudo] weidawang 的密码： </span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_cnn_train.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_ops_train.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_adv_train.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_adv_infer.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_ops_infer.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_cnn_infer.so<span class="number">.8</span> 不是符号链接</span><br></pre></td></tr></table></figure>
<p>按<a href="https://blog.csdn.net/langb2014/article/details/54376716/">这篇博客</a>的思路，我这边依次执行下面的命令就好了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8</span><br></pre></td></tr></table></figure>
<ol>
<li>安装好后我试着用<code>deepstream-app --version-all</code>验证是否安装成功，它的输出是：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">(gst-plugin-scanner:6378): GStreamer-WARNING **: 08:44:44.000: Failed to load plugin <span class="string">&#x27;/usr/lib/x86_64-linux-gnu/gstreamer-1.0/deepstream/libnvdsgst_inferserver.so&#x27;</span>: libtrtserver.so: cannot open shared object file: No such file or directory</span><br><span class="line">deepstream-app version 5.0.0</span><br><span class="line">DeepStreamSDK 5.0.0</span><br><span class="line">CUDA Driver Version: 11.1</span><br><span class="line">CUDA Runtime Version: 10.2</span><br><span class="line">TensorRT Version: 7.2</span><br><span class="line">cuDNN Version: 8.0</span><br><span class="line">libNVWarp360 Version: 2.0.1d3</span><br></pre></td></tr></table></figure>
<p>这个GStreamer-WARNING在<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_troubleshooting.html#errors-occur-when-deepstream-app-fails-to-load-plugin-gst-nvinferserver-on-dgpu-only">Troubleshooting</a>里有提到，就是说在ubuntu电脑上安装出现这个警告是预料之中的，如果不需要Triton可以不用管它。</p>
<p>我原以为这样就安装成功了，后来在跑<a href="https://github.com/NVIDIA-AI-IOT/deepstream_pose_estimation">deepstream_pose_estimation</a>的时候才意识到我没有完全正确安装，相关的nvidia forum <a href="https://forums.developer.nvidia.com/t/failed-to-load-plugin-usr-lib-x86-64-linux-gnu-gstreamer-1-0-deepstream-libnvdsgst-osd-so/163662">topic</a>在这里，我方法1 2 3都有报错，最后通过<a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_docker_containers.html">方法4</a>，即docker成功安装好了，又体验到了用docker安装的爽。。。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull nvcr.io&#x2F;nvidia&#x2F;deepstream:5.0.1-20.09-devel</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -it -v /home/weidawang/volume/deepstream:/home/weidawang/volume/deepstream nvcr.io/nvidia/deepstream:5.0.1-20.09-devel</span><br></pre></td></tr></table></figure>
<p>安装好deepstream的docker镜像之后，我选择用<a href="https://github.com/NVIDIA-AI-IOT/deepstream_pose_estimation">deepstream_pose_estimation</a>来练手，详细的教程徐需要看它的<a href="https://developer.nvidia.com/blog/creating-a-human-pose-estimation-application-with-deepstream-sdk">blog</a>。<br>使用这个deepstream_pose_estimation时，有几个注意点：</p>
<ol>
<li>克隆下来的仓库已经包含了pose_estimation.onnx，不需要再下载</li>
<li>作者说要做这一步：<code>sudo cp libnvds_osd.so /opt/nvidia/deepstream/deepstream-5.0/lib</code></li>
<li>用<code>exit</code>退出容器后, 如果想要重新进入容器，先<code>docker ps -a</code>列出以往你运行过的容器，然后 <code>docker start [container_name]</code>，最后再 <code>docker attach [container_name]</code>进入容器</li>
</ol>
<h3 id="DeepStream-Triton-Inference-Server使用指南"><a href="#DeepStream-Triton-Inference-Server使用指南" class="headerlink" title="DeepStream Triton Inference Server使用指南"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#deepstream-triton-inference-server-usage-guidelines">DeepStream Triton Inference Server使用指南</a></h3><h4 id="dGPU"><a href="#dGPU" class="headerlink" title="dGPU"></a>dGPU</h4><ol>
<li><p>拉DeepStream Triton Inference Server docker镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull nvcr.io&#x2F;nvidia&#x2F;deepstream:5.0.1-20.09-triton</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 docker镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -it --rm -v &#x2F;tmp&#x2F;.X11-unix:&#x2F;tmp&#x2F;.X11-unix -e DISPLAY&#x3D;$DISPLAY nvcr.io&#x2F;nvidia&#x2F;deepstream:5.0.1-20.09-triton</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>个人建议，不要加<code>--rm</code>， 否则你退出容器后，你的所有操作都会被情况，<code>docker ps -a</code>都不能找到你刚用过的容器。</p>
<h4 id="Jetson"><a href="#Jetson" class="headerlink" title="Jetson"></a>Jetson</h4><p>Triton Inference Server共享库是Jetson上DeepStream的预装部分。安装Triton Inference Server不需要额外的步骤。</p>
<p>对于这两个平台，要运行这些示例，请遵循README文件的<em>Running the Triton Inference Server samples</em>部分的步骤。</p>
<h2 id="Docker容器"><a href="#Docker容器" class="headerlink" title="Docker容器"></a>Docker容器</h2><p>DeepStream 5.0为dGPU和Jetson平台均提供了Docker容器。这些容器通过将所有相关的依赖关系打包在容器内，提供了一种方便的，即用的方式来部署DeepStream应用程序。相关的Docker映像托管在NGC Web门户<a href="https://ngc.nvidia.com/">https://ngc.nvidia.com</a>上的NVIDIA容器注册表中。他们使用了<code>nvidia-docker</code>软件包，该软件包使您能够从容器访问所需的GPU资源。</p>
<blockquote>
<p>注意：用于dGPU和Jetson的DeepStream 5.0容器是不同的，因此您必须为您的平台获取正确的映像。</p>
</blockquote>
<h3 id="dGPU的Docker容器"><a href="#dGPU的Docker容器" class="headerlink" title="dGPU的Docker容器"></a>dGPU的Docker容器</h3><p>NGC Web门户中的“<a href="https://ngc.nvidia.com/catalog/containers?orderBy=modifiedDESC&amp;pageNumber=0&amp;query=&amp;quickFilter=containers&amp;filters=">容器”</a>页面提供了有关拉取和运行容器以及其内容的说明。dGPU容器称为<code>deepstream</code>，而jetson容器称为<code>deepstream-l4t</code>。与DeepStream 3.0中的容器不同，dGPU DeepStream 5.0容器在容器内支持DeepStream应用程序开发。它包含与DeepStream 5.0 SDK相同的构建工具和开发库。在典型情况下，您将在DeepStream容器中构建，执行和调试DeepStream应用程序。准备好应用程序后，您可以使用DeepStream 5.0容器作为基础映像来创建自己的Docker容器，以容纳应用程序文件（二进制文件，库，模型，配置文件等）。这是一个示例片段用于创建自己的Docker容器的Dockerfile：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Replace with required `container type` e.g. base, devel etc in the following line</span></span><br><span class="line"><span class="keyword">FROM</span> nvcr.io/nvidia/ deepstream:<span class="number">5.0</span>.<span class="number">1</span>-<span class="number">20.09</span>-&lt;container type&gt;</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> myapp  /root/apps/myapp</span></span><br><span class="line"><span class="comment"># To get video driver libraries at runtime (libnvidia-encode.so/libnvcuvid.so)</span></span><br><span class="line"><span class="keyword">ENV</span> NVIDIA_DRIVER_CAPABILITIES $NVIDIA_DRIVER_CAPABILITIES,video</span><br></pre></td></tr></table></figure>
<p>该Dockerfile将您的应用程序（从目录<code>mydsapp</code>）复制到容器（<code>pathname /root/apps</code>）中。请注意，您必须确保NGC的DeepStream 5.0映像位置正确。</p>
<p>下表列出了随DeepStream 5.0发布的dGPU的Docker容器：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>容器</th>
<th>容器拉取命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>base docker（仅包含运行时库和GStreamer插件。可以作为为DeepStream应用程序构建一个自定义docker的基础）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-base</code></td>
</tr>
<tr>
<td>devel docker（包含整个SDK以及用于构建DeepStream应用程序的开发环境）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-devel</code></td>
</tr>
<tr>
<td>Triton Inference Server docker（安装了Triton Inference Server和其依赖项以及用于构建DeepStream应用程序的开发环境）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-triton</code></td>
</tr>
<tr>
<td>DeepStream IoT docker（仅包含Deepstream-test5-app，所有其他参考应用程序被删除了）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-iot</code></td>
</tr>
<tr>
<td>DeepStream samples docker（包含运行时库，GStreamer插件，参考应用程序以及示例流，模型和配置）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-samples</code></td>
</tr>
</tbody>
</table>
</div>
<p>有关<code>nvcr.io</code>身份验证等信息，请参阅DeepStream 5.0发行说明。</p>
<p>请参见NGC上的<a href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream">dGPU容器</a>以了解更多运行dGPU容器细节和说明。</p>
<h3 id="Jetson的Docker容器"><a href="#Jetson的Docker容器" class="headerlink" title="Jetson的Docker容器"></a>Jetson的Docker容器</h3><p>从JetPack 4.2.1版本开始，已添加了用于Jetson的<a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson">NVIDIA Container Runtime</a>，使您能够在Jetson设备上运行启用GPU的容器。使用此功能，可以使用NGC上的Docker映像在Jetson设备上的容器内运行DeepStream 5.0。拉出容器并按照<a href="https://ngc.nvidia.com/catalog/containers?orderBy=modifiedDESC&amp;pageNumber=0&amp;query=&amp;quickFilter=containers&amp;filters=">NGC容器</a>上的说明执行页。DeepStream容器希望将CUDA，TensorRT和VisionWorks安装在Jetson设备上，因为它是从主机安装在容器内的。在启动DeepStream容器之前，请确保已在Jetson上使用JetPack安装了这些实用程序。请注意，Jetson Docker容器仅用于部署。它们不支持容器内的DeepStream软件开发。您可以在Jetson目标上本地构建应用程序，并通过将二进制文件添加到Docker映像中来为其创建容器。或者，您可以按照在x86工作站上构建Jetson容器中的说明从工作站生成Jetson容器NVIDIA Container Runtime for Jetson文档中的“部分”。下表列出了随DeepStream 5.0发布的Jetson的Docker容器：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>容器</th>
<th>容器拉取命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base docker（仅包含运行时库和GStreamer插件。可用作构建DeepStream应用程序的自定义docker的基础）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream-l4t:5.0.1-20.09-base</code></td>
</tr>
<tr>
<td>DeepStream IoT docker（仅安装了deepstream-test5-app，并删除了所有其他参考应用程序。）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream-l4t:5.0.1-20.09-iot</code></td>
</tr>
<tr>
<td>DeepStream samples docker（包含运行时库，GStreamer插件，参考应用程序以及示例流，模型和配置）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream-l4t:5.0.1-20.09-samples</code></td>
</tr>
</tbody>
</table>
</div>
<p>有关<code>nvcr.io</code>身份验证等信息，请参阅DeepStream 5.0发行说明。</p>
<p>见NGC<a href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream-l4t">Jetson container</a> 更多运行jetson容器的细节和说明。</p>
<h1 id="DeepStream样例"><a href="#DeepStream样例" class="headerlink" title="DeepStream样例"></a><strong>DeepStream样例</strong></h1><h2 id="C-C-示例应用程序源详细信息"><a href="#C-C-示例应用程序源详细信息" class="headerlink" title="C / C ++示例应用程序源详细信息"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html#c-c-sample-apps-source-details">C / C ++示例应用程序源详细信息</a></h2><div class="table-container">
<table>
<thead>
<tr>
<th>参考测试应用</th>
<th>源目录内的路径</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sample test application 1</td>
<td>sources/apps/sample_apps/deepstream-test1</td>
<td>如何对单个H.264流使用DeepStream元素的示例：filesrc→decode→nvstreammux→nvinfer（primary detector）→nvdsosd→renderer。</td>
</tr>
<tr>
<td>Sample test application 2</td>
<td>sources/apps/sample_apps/deepstream-test2</td>
<td>如何对单个H.264流使用DeepStream元素的示例：filesrc→decode→nvstreammux→nvinfer（primary detector）→nvtracker→nvinfer（secondary classifier）→nvdsosd→renderer。</td>
</tr>
<tr>
<td>Sample test application 3</td>
<td>sources/apps/sample_apps/deepstream-test3</td>
<td>基于deepstream-test1构建，演示如何：在管道中使用多个来源；使用uridecodebin接受任何类型的输入（例如RTSP /文件）、任何GStreamer支持的容器格式以及任何编解码器；配置Gst-nvstreammux以生成一批帧并推断出这些帧以提高资源利用率；提取流元数据，其中包含有关批处理缓冲区中帧的有用信息</td>
</tr>
<tr>
<td>Sample test application 4</td>
<td>sources/apps/sample_apps/deepstream-test4</td>
<td>基于deepstream-test1构建，演示如何：在管道中使用Gst-nvmsgconv和Gst-nvmsgbroker插件；创建NVDS_META_EVENT_MSG类型的元数据并将其attach到缓冲区；将NVDS_META_EVENT_MSG用于不同类型的对象，例如车辆和人；Implement “copy” and “free” functions for use if metadata is extended through the extMsg field</td>
</tr>
<tr>
<td>Sample test application 5</td>
<td>sources/apps/sample_apps/deepstream-test5</td>
<td>建立在deepstream-app的基础上，演示如何：在多流管道中使用Gst-nvmsgconv和Gst-nvmsgbroker插件；如何从配置文件中将Gst-nvmsgbroker插件配置为接收器插件（适用于KAFKA，Azure等）；如何处理来自RTSP服务器或摄像机的RTCP sender reports，以及如何将Gst Buffer PTS转换为UTC时间戳。                                              有关更多详细信息，请参考位于<code>deepstream_test5_app_main.c</code>中的RTCP sender report回调函数<code>test5_rtcp_sender_report_callback()</code>的注册和用法。GStreamer callback registration with rtpmanager element’s “handle-sync” signal is documented in <code>apps-common/src/deepstream_source_bin.c</code>。</td>
</tr>
<tr>
<td>AMQP协议测试应用</td>
<td>sources/libs/amqp_protocol_adaptor</td>
<td>用于测试AMQP协议的应用程序。</td>
</tr>
<tr>
<td>Azure MQTT测试应用程序</td>
<td>sources/libs /azure_protocol_adaptor</td>
<td>测试应用程序以显示使用MQTT的Azure IoT device2edge消息传递和device2cloud消息传递。</td>
</tr>
<tr>
<td>DeepStream参考应用程序</td>
<td>sources/apps/sample_apps/deepstream-app</td>
<td>DeepStream参考应用程序的源代码。</td>
</tr>
<tr>
<td>UFF SSD detector</td>
<td>sources/objectDetector_SSD</td>
<td>SSD检测器模型的配置文件和自定义库实现。</td>
</tr>
<tr>
<td>Faster RCNN detector</td>
<td>sources/objectDetector_FasterRCNN</td>
<td>FasterRCNN模型的配置文件和自定义库实现。</td>
</tr>
<tr>
<td>Yolo detector</td>
<td>sources/objectDetector_Yolo</td>
<td>Yolo模型（当前为Yolo v2，v2 tiny，v3和v3 tiny）的配置文件和自定义库实现。</td>
</tr>
<tr>
<td>Dewarper示例</td>
<td>apps / sample_apps / deepstream-dewarper-test</td>
<td>演示单个或多个360度摄像机流的扭曲功能。从CSV文件中读取相机校准参数，并在显示屏上渲染过道和斑点表面。</td>
</tr>
<tr>
<td>光流示例</td>
<td>apps / sample_apps / deepstream-nvof-test</td>
<td>演示单个或多个流的光流功能。本示例使用两个GStreamer插件（Gst-nvof和Gst-nvofvisual）。Gst-nvof元素生成MV（运动矢量）数据并将其作为用户元数据附加。Gst-nvofvisual元素使用预定义的色轮矩阵可视化MV数据。</td>
</tr>
<tr>
<td>自定义元数据示例</td>
<td>apps / sample_apps / deepstream-user-metadata-test</td>
<td>演示如何向DeepStream的任何组件中添加自定义或用户特定的元数据。测试代码将一个填充有用户数据的16字节数组附加到所选组件。数据在另一个组件中检索。</td>
</tr>
<tr>
<td>MJPEG和JPEG解码器以及推理示例</td>
<td>apps / sample_apps / deepstream-image-decode-test</td>
<td>建立在deepstream-test3的基础上，以演示图像解码而不是视频。本示例使用自定义解码箱，因此可以将MJPEG编解码器用作输入。</td>
</tr>
<tr>
<td>图像/视频分割示例</td>
<td>apps / sample_apps / deepstream-segmentation-test</td>
<td>演示使用语义或工业神经网络对多流视频或图像进行分段并将输出呈现到显示器。</td>
</tr>
<tr>
<td>在Gst-nvstreammux之前处理元数据</td>
<td>apps / sample_apps / deepstream-gst-metadata-test</td>
<td>演示如何在DeepStream管道中的Gst-nvstreammux插件之前设置元数据，以及如何在Gst-nvstreammux之后访问元数据。</td>
</tr>
<tr>
<td>GST-Nvinfer张量元流示例</td>
<td>apps / sample_apps / deepstream-infer-tensor-meta-app</td>
<td>演示如何将nvinfer张量输出作为元数据传递和访问。</td>
</tr>
<tr>
<td>性能演示</td>
<td>apps / sample_apps / deepstream-perf-demo</td>
<td>对目录中的所有流顺序执行单通道级联推理和对象跟踪。</td>
</tr>
<tr>
<td>分析示例</td>
<td>apps / sample_apps / deepstream-nvdsanalytics-test</td>
<td>演示批处理分析，例如ROI过滤，线交叉，方向检测和拥挤</td>
</tr>
<tr>
<td>OpenCV示例</td>
<td>apps / sample_apps / deepstream-opencv-test</td>
<td>演示在dsexample插件中使用OpenCV</td>
</tr>
<tr>
<td>图像作为元数据示例</td>
<td>Apps / sample_apps / deepstream-image-meta-test</td>
<td>演示如何将编码的图像附加为元数据并以jpeg格式保存图像。</td>
</tr>
<tr>
<td>Appsrc和Appsink示例</td>
<td>apps / sample_apps / deepstream-appsrc-test</td>
<td>演示AppSrc和AppSink的用法，分别使用和提供非DeepStream代码中的数据。</td>
</tr>
<tr>
<td>迁移学习的例子</td>
<td>apps / sample_apps / deepstream-transfer-learning-app</td>
<td>演示了一种将图像保存为置信度较小的对象的机制，该机制可用于进一步训练</td>
</tr>
<tr>
<td>Mask-RCNN示例</td>
<td>apps / sample_apps / deepstream-mrcnn-test</td>
<td>使用Mask-RCNN模型演示实例分割</td>
</tr>
</tbody>
</table>
</div>
<h3 id="插件和库源详细信息"><a href="#插件和库源详细信息" class="headerlink" title="插件和库源详细信息"></a>插件和库源详细信息</h3><p>下表描述了源目录的内容，但参考测试应用程序除外，它们在下面分别列出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>插件或库</th>
<th>源目录内的路径</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>DsExample GStreamer插件</td>
<td>gst-plugins / gst-dsexample</td>
<td>用于将自定义算法集成到DeepStream SDK图中的模板插件。</td>
</tr>
<tr>
<td>GStreamer Gst-nvmsgconv插件</td>
<td>gst-plugins / gst-nvmsgconv</td>
<td>GStreamer Gst-nvmsgconv插件的源代码，用于将元数据转换为架构格式。</td>
</tr>
<tr>
<td>GStreamer Gst-nvmsgbroker插件</td>
<td>gst-plugins / gst-nvmsgbroker</td>
<td>GStreamer Gst-nvmsgbroker插件的源代码，用于将数据发送到服务器。</td>
</tr>
<tr>
<td>GStreamer Gst-nvinfer插件</td>
<td>gst-plugins / gst-nvinfer</td>
<td>用于推断的GStreamer Gst-nvinfer插件的源代码。</td>
</tr>
<tr>
<td>GStreamer Gst-nvdsosd插件</td>
<td>gst-plugins / gst-nvdsosd</td>
<td>GStreamer Gst-nvdsosd插件的源代码，用于绘制bbox，文本和其他对象。</td>
</tr>
<tr>
<td>NvDsInfer库</td>
<td>libs / nvdsinfer</td>
<td>NvDsInfer库的源代码，由Gst-nvinfer GStreamer插件使用。</td>
</tr>
<tr>
<td>NvMsgConv库</td>
<td>libs / nvmsgsconv</td>
<td>Gst-nvmsgconv GStreamer插件所需的NvMsgConv库的源代码。</td>
</tr>
<tr>
<td>Kafka协议适配器</td>
<td>libs / kafka_protocol_adapter</td>
<td>Kafka的协议适配器。</td>
</tr>
<tr>
<td>nvdsinfer_customparser</td>
<td>libs / nvdsinfer_customparser</td>
<td>用于检测器和分类器的定制模型输出解析示例。</td>
</tr>
<tr>
<td>GST-V4L2</td>
<td>请参阅下面的注释 <a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html#f1">1</a></td>
<td>v4l2编解码器的源代码。</td>
</tr>
</tbody>
</table>
</div>
<p>脚注1：</p>
<p>DeepStream软件包中不存在Gst-<a href="https://baike.baidu.com/item/V4L2">v4l2</a>源。要下载，请按照下列步骤操作：</p>
<p>转到：<a href="https://developer.nvidia.com/embedded/downloads">https</a> : <a href="https://developer.nvidia.com/embedded/downloads">//developer.nvidia.com/embedded/downloads</a>。在字段中输入<code>Search filter``L4T sources</code>为L4T Release选择适当的项目<code>32.4.3</code>。下载文件并将其解压缩以获取<code>.tbz2</code>文件，展开<code>.tbz2</code>文件，<code>Gst-v4l2</code>源文件在<code>gst-nvvideo4linux2_src.tbz2</code></p>
<h2 id="Python示例应用程序源详细信息"><a href="#Python示例应用程序源详细信息" class="headerlink" title="Python示例应用程序源详细信息"></a><a href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html#python-sample-apps-source-details">Python示例应用程序源详细信息</a></h2><h3 id="Python绑定"><a href="#Python绑定" class="headerlink" title="Python绑定"></a>Python绑定</h3><p>本节提供有关使用Python进行DeepStream应用程序开发的详细信息。DeepStream 5.0 SDK中包含Python绑定，可在以下位置找到示例应用程序： <a href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps">https://github.com/NVIDIA-AI-IOT/deepstream_python_apps</a>。在此处阅读有关PyDS API的更多信息：<a href="https://docs.nvidia.com/metropolis/deepstream/python-api/">https://docs.nvidia.com/metropolis/deepstream/python-api/</a> </p>
<h3 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h3><ul>
<li><p>Ubuntu 18.04</p>
</li>
<li><p>DeepStream SDK 5.0或更高版本</p>
</li>
<li><p>Python 3.6</p>
</li>
<li><p>Gst Python v1.14.5</p>
<p>如果Jetson上缺少Gst python安装，请使用以下命令进行安装：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo apt-get install python-gi-dev</span><br><span class="line">$ export GST_LIBS&#x3D;&quot;-lgstreamer-1.0 -lgobject-2.0 -lglib-2.0&quot;</span><br><span class="line">$ export GST_CFLAGS&#x3D;&quot;-pthread -I&#x2F;usr&#x2F;include&#x2F;gstreamer-1.0 -I&#x2F;usr&#x2F;include&#x2F;glib-2.0 -I&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;glib-2.0&#x2F;include&quot;</span><br><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;GStreamer&#x2F;gst-python.git</span><br><span class="line">$ cd gst-python</span><br><span class="line">$ git checkout 1a8f48a</span><br><span class="line">$ .&#x2F;autogen.sh PYTHON&#x3D;python3</span><br><span class="line">$ .&#x2F;configure PYTHON&#x3D;python3</span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="运行示例应用程序"><a href="#运行示例应用程序" class="headerlink" title="运行示例应用程序"></a>运行示例应用程序</h3><ol>
<li><p>在以下位置<code>&lt;DeepStream 5.0 ROOT&gt;/sources</code>克隆仓库<code>deepstream_python_apps</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;NVIDIA-AI-IOT&#x2F;deepstream_python_apps</span><br></pre></td></tr></table></figure>
</li>
<li><p>这将创建以下目录：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;DeepStream 5.0 ROOT&gt;&#x2F;sources&#x2F;deepstream_python_apps</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python应用程序位于apps`目录下。进入每个应用程序目录，并按照自述文件中的说明进行操作。</p>
<p>注意 : 应用程序配置文件包含模型的相对路径。</p>
</li>
</ol>
<h3 id="管道建设"><a href="#管道建设" class="headerlink" title="管道建设"></a>管道建设</h3><p>可以使用Gst Python（GStreamer框架的Python绑定）构造DeepStream管道。有关管道构造示例，请参见示例应用程序的主要功能。</p>
<h3 id="元数据访问"><a href="#元数据访问" class="headerlink" title="元数据访问"></a>元数据访问</h3><p>DeepStream MetaData包含推理结果和分析中使用的其他信息。元数据被附加到每个管道组件接收到的<code>Gst Buffer</code>。SDK元数据文档和API指南中详细描述了元数据格式。SDK MetaData库是用C / C ++开发的。Python绑定提供了从Python应用程序对MetaData的访问。绑定在已编译的模块中提供，可用于x86_64和Jetson平台。<code>pyds.so</code>模块位于DeepStream SDK安装目录<code>/lib</code>下。示例应用程序通过common / utils.py获取此模块的导入路径。<code>/lib</code>目录还包括setup.py，用于将模块安装到标准路径中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream&#x2F;lib</span><br><span class="line">python3 setup.py install</span><br></pre></td></tr></table></figure>
<p>由于python用法是可选的，因此当前没有通过SDK安装程序自动完成。绑定通常遵循与C / C ++库相同的API，以下几节详细介绍了一些例外。</p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>MetaData的内存由Python和C / C ++代码路径共享。例如，可以通过用Python编写的探测函数添加MetaData项，并且需要由用C / C ++编写的下游插件访问。deepstream-test4应用程序包含此类用法。Python垃圾收集器无法查看C / C ++中的内存引用，因此无法安全地管理此类共享内存的生存期。由于这种复杂性，Python通常通过引用实现对MetaData内存的访问，而无需声明所有权。</p>
<h3 id="分配"><a href="#分配" class="headerlink" title="分配"></a>分配</h3><p>在Python中分配MetaData对象时，绑定将提供分配功能，以确保该对象具有适当的内存所有权。如果使用了构造函数，则垃圾回收器在其Python引用终止时将声明该对象。但是，仍然需要下游的C / C ++代码访问该对象，因此该对象必须在这些Python引用之外仍然存在。示例：要分配<code>NvDsEventMsgMeta</code>实例，请使用此命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">msg_meta &#x3D; pyds.alloc_nvds_event_msg_meta() *# get reference to allocated instance without claiming memory ownership*</span><br></pre></td></tr></table></figure>
<p>不是这个：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">msg_meta &#x3D; NvDsEventMsgMeta() *# memory will be freed by the garbage collector when msg_meta goes out of scope in Python*</span><br></pre></td></tr></table></figure>
<p>分配器可用于以下结构：</p>
<ul>
<li><code>NvDsVehicleObject: alloc_nvds_vehicle_object()</code></li>
<li><code>NvDsPersonObject: alloc_nvds_person_object()</code></li>
<li><code>NvDsFaceObject: alloc_nvds_face_object()</code></li>
<li><code>NvDsEventMsgMeta: alloc_nvds_event_msg_meta()</code></li>
<li><code>NvDsEvent: alloc_nvds_event()</code></li>
<li><code>NvDsPayload: alloc_nvds_payload()</code></li>
<li><code>Generic buffer: alloc_buffer(size)</code></li>
</ul>
<h3 id="字符串访问"><a href="#字符串访问" class="headerlink" title="字符串访问"></a>字符串访问</h3><p>一些MetaData结构包含字符串字段。以下各节提供了有关访问它们的详细信息。</p>
<h4 id="设置字符串字段"><a href="#设置字符串字段" class="headerlink" title="设置字符串字段"></a>设置字符串字段</h4><p>设置字符串字段会导致在基础C ++代码中分配字符串缓冲区。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obj.type &#x3D; &quot;Type&quot;</span><br></pre></td></tr></table></figure>
<p>这将导致分配内存缓冲区，并将字符串“ TYPE”复制到其中。该内存归C代码所有，稍后将释放。要释放Python代码中的缓冲区，请使用：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyds.free_buffer(obj.type)</span><br></pre></td></tr></table></figure>
<p>注意</p>
<p><code>NvOSD_TextParams.display_text</code> 现在，分配新字符串后，字符串会自动释放。</p>
<h4 id="读取字符串字段"><a href="#读取字符串字段" class="headerlink" title="读取字符串字段"></a>读取字符串字段</h4><p>直接读取字符串字段将以int的形式返回该字段的C地址，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">obj &#x3D; pyds.NvDsVehicleObject.cast(data);</span><br><span class="line">print(obj.type)</span><br></pre></td></tr></table></figure>
<p>这将打印一个int表示<code>obj.type</code>C中的地址（这是一个char *）。要检索此字段的字符串值，请使用<code>pyds.get_string()</code>，例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(pyds.get_string(obj.type))</span><br></pre></td></tr></table></figure>
<h3 id="Casting"><a href="#Casting" class="headerlink" title="Casting"></a>Casting</h3><p>一些MetaData实例以GList形式存储。要访问GList节点中的数据，需要将数据字段强制转换为适当的结构。该转换通过针对目标类型的cast（）成员函数完成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NvDsBatchMeta.cast</span><br><span class="line">NvDsFrameMeta.cast</span><br><span class="line">NvDsObjectMeta.cast</span><br><span class="line">NvDsUserMeta.cast</span><br><span class="line">NvDsClassifierMeta.cast</span><br><span class="line">NvDsDisplayMeta.cast</span><br><span class="line">NvDsLabelInfo.cast</span><br><span class="line">NvDsEventMsgMeta.cast</span><br><span class="line">NvDsVehicleObject.cast</span><br><span class="line">NvDsPersonObject.cast</span><br></pre></td></tr></table></figure>
<p>在v0.5版中，提供了独立的强制转换功能。现在，上面的cast（）函数已弃用并取代了这些函数：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">glist_get_nvds_batch_meta</span><br><span class="line">glist_get_nvds_frame_meta</span><br><span class="line">glist_get_nvds_object_meta</span><br><span class="line">glist_get_nvds_user_meta</span><br><span class="line">glist_get_nvds_classifier_meta</span><br><span class="line">glist_get_nvds_display_meta</span><br><span class="line">glist_get_nvds_label_info</span><br><span class="line">glist_get_nvds_event_msg_meta</span><br><span class="line">glist_get_nvds_vehicle_object</span><br><span class="line">glist_get_nvds_person_object</span><br></pre></td></tr></table></figure>
<p>例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">l_frame &#x3D; batch_meta.frame_meta_list</span><br><span class="line">frame_meta &#x3D; pyds.NvDsFrameMeta.cast(l_frame.data)</span><br></pre></td></tr></table></figure>
<h3 id="回调功能注册"><a href="#回调功能注册" class="headerlink" title="回调功能注册"></a>回调功能注册</h3><p>添加到NvDsUserMeta的自定义元数据需要自定义复制和发布功能。MetaData库依赖于这些自定义功能来对自定义结构进行深度复制，并释放已分配的资源。这些函数在NvDsUserMeta结构中注册为回调函数指针。使用以下功能注册回调功能：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyds.set_user_copyfunc(NvDsUserMeta_instance, copy_function)</span><br><span class="line">pyds.set_user_releasefunc(NvDsUserMeta_instance, free_func)</span><br></pre></td></tr></table></figure>
<p>注意</p>
<p>在应用程序退出之前，需要在绑定库中取消注册回调。绑定库当前保留对已注册函数的全局引用，并且这些引用不能超过绑定库在应用程序退出时发生的卸载。使用以下函数注销所有回调：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pyds.unset_callback_funcs()</span><br></pre></td></tr></table></figure>
<p>有关回调注册和注销的示例，请参见deepstream-test4示例应用程序。</p>
<p><strong>限制</strong>：绑定库当前仅为每个应用程序支持一组回调函数。将使用最后注册的功能。</p>
<h3 id="优化和实用程序"><a href="#优化和实用程序" class="headerlink" title="优化和实用程序"></a>优化和实用程序</h3><p>通常，Python解释比运行已编译的C / C ++代码要慢。为了提供更好的性能，某些操作在C中实现，并通过绑定接口公开。目前这是实验性的，并将随着时间的推移而扩展。提供以下优化功能：</p>
<ul>
<li><p><code>pyds.NvOSD_ColorParams.set(double red, double green, double blue, double alpha)</code></p>
<blockquote>
<p>这是一个简单的函数，其执行与以下操作相同的操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">txt_params.text_bg_clr.red &#x3D; red</span><br><span class="line">txt_params.text_bg_clr.green &#x3D; green</span><br><span class="line">txt_params.text_bg_clr.blue &#x3D; blue</span><br><span class="line">txt_params.text_bg_clr.alpha &#x3D; alpha</span><br></pre></td></tr></table></figure>
<p>这些操作是在deepstream_test_4.py中的每个对象上执行的，从而导致合计处理时间减慢了管道的速度。将此功能推入C层有助于提高性能。</p>
</blockquote>
</li>
<li><p><code>generate_ts_rfc3339 (buffer, buffer_size)</code></p>
<blockquote>
<p>此函数使用根据RFC3339生成的时间戳填充输入缓冲区： <code>%Y-%m-%dT%H:%M:%S.nnnZ\0</code></p>
</blockquote>
</li>
</ul>
<h3 id="图像数据访问"><a href="#图像数据访问" class="headerlink" title="图像数据访问"></a>图像数据访问</h3><p>解码后的图像可以<code>NumPy</code>通过该<code>get_nvds_buf_surface</code>函数作为数组访问。API指南中记录了此功能。有关<code>deepstream-imagedata-multistream</code>图像数据使用的示例，请参见示例应用程序。</p>
<h2 id="样本应用程序源详细信息"><a href="#样本应用程序源详细信息" class="headerlink" title="样本应用程序源详细信息"></a>样本应用程序源详细信息</h2><p>下表显示了<a href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps下Python示例应用程序的位置">https://github.com/NVIDIA-AI-IOT/deepstream_python_apps下Python示例应用程序的位置</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参考测试应用</th>
<th>GitHub存储库中的路径</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单测试应用程序1</td>
<td>apps / deepstream-test1</td>
<td>如何对单个H.264流使用DeepStream元素的简单示例：filesrc→解码→nvstreammux→nvinfer（主检测器）→nvdsosd→渲染器。</td>
</tr>
<tr>
<td>简单测试应用程序2</td>
<td>apps / deepstream-test2</td>
<td>如何对单个H.264流使用DeepStream元素的简单示例：filesrc→解码→nvstreammux→nvinfer（主检测器）→nvtracker→nvinfer（辅助分类器）→nvdsosd→渲染器。</td>
</tr>
<tr>
<td>简单测试应用程序3</td>
<td>apps / deepstream-test3</td>
<td>基于deepstream-test1（简单测试应用程序1）构建，以演示如何：在管道中使用多个来源使用uridecodebin接受任何类型的输入（例如RTSP /文件），任何GStreamer支持的容器格式以及任何编解码器配置Gst-nvstreammux以生成一批帧并推断出这些帧以提高资源利用率提取流元数据，其中包含有关批处理缓冲区中帧的有用信息</td>
</tr>
<tr>
<td>简单测试应用程序4</td>
<td>应用程序/ deepstream-test4</td>
<td>基于deepstream-test1构建单个H.264流：filesrc，decode，nvstreammux，nvinfer，nvdsosd，renderer演示如何：在管道中使用Gst-nvmsgconv和Gst-nvmsgbroker插件创建NVDS_META_EVENT_MSG类型的元数据并将其附加到缓冲区将NVDS_META_EVENT_MSG用于不同类型的对象，例如车辆和人如果通过extMsg字段扩展了元数据，则实现“复制”和“免费”功能以供使用</td>
</tr>
<tr>
<td>USB摄像头源应用</td>
<td>apps / deepstream-test1-usbcam</td>
<td>简单测试应用程序1已修改为处理来自USB摄像机的单个流。</td>
</tr>
<tr>
<td>RTSP输出应用</td>
<td>apps / deepstream-test1-rtsp-out</td>
<td>简单测试应用程序1已修改为通过RTSP输出可视化流。</td>
</tr>
<tr>
<td>图像数据访问应用</td>
<td>apps / deepstream-imagedata-multistream</td>
<td>以简单的测试应用程序3为基础，演示如何：在管道中将解码的帧作为NumPy数组访问检查检测到的对象的检测置信度（需要DBSCAN或NMS群集）使用OpenCV注释框架并将其保存到文件</td>
</tr>
<tr>
<td>SSD检测器输出解析器应用</td>
<td>apps / deepstream-ssd-parser</td>
<td>演示如何对Triton Inference Server的推理输出执行自定义后处理：在Triton Inference Server上使用SSD模型进行对象检测通过配置文件设置为Triton Inference Server启用自定义后处理和原始张量导出访问管道中的推断输出张量以在Python中进行后处理将检测到的对象添加到元数据将OSD可视化输出到MP4文件</td>
</tr>
</tbody>
</table>
</div>
<h3 id="DeepStream参考应用程序-deepstream-test5应用程序"><a href="#DeepStream参考应用程序-deepstream-test5应用程序" class="headerlink" title="DeepStream参考应用程序-deepstream-test5应用程序"></a>DeepStream参考应用程序-deepstream-test5应用程序</h3><p>除常规推理管道外，Test5应用程序还支持以下功能：</p>
<ul>
<li>将消息发送到后端服务器。</li>
<li>充当使用者以从后端服务器接收消息。</li>
<li>基于从服务器收到的消息触发基于事件的记录。</li>
<li>OTA模型更新。</li>
</ul>
<h4 id="支持物联网协议和云配置"><a href="#支持物联网协议和云配置" class="headerlink" title="支持物联网协议和云配置"></a>支持物联网协议和云配置</h4><p><code>nvmsgbroker</code>DeepStream插件指南中列出了插件支持的IoT协议（如KAFKA，Azure，AMQP等）的详细信息。DeepStream Public文档可参考特定于所使用协议的设置IoT中心/服务器/经纪人。与<code>type=6</code>for<code>nvmsgconv</code>和<code>nvmsgbroker</code>configuration相关联的[sink]组密钥在：ref：config-groups-label中讨论。</p>
<h4 id="消息使用者"><a href="#消息使用者" class="headerlink" title="消息使用者"></a>消息使用者</h4><p><code>deepstream-test5-app</code>可以配置为充当云消息的消息使用者。解析收到的消息后，可以根据消息的内容触发特定的操作。例如，保存智能记录上下文的NvDsSrcParentBin <em>作为参数传递，该参数<code>start_cloud_to_device_messaging()</code>用于触发智能记录的启动/停止。默认情况下，已实现基于事件的记录以演示消息使用方的用法。用户需要实现自定义逻辑，以处理其他类型的接收消息。请参阅`deepstream_c2d_msg</em><code>文件以获取有关实现的更多详细信息。要订阅云消息，请相应地配置</code>[message-consumer]`组。</p>
<h4 id="智能记录-基于事件的记录"><a href="#智能记录-基于事件的记录" class="headerlink" title="智能记录-基于事件的记录"></a>智能记录-基于事件的记录</h4><p>可以将Test5应用程序配置为基于从服务器收到的事件来记录原始视频提要。这样，无需始终保存数据，此功能仅允许记录感兴趣的事件。请参阅《 DeepStream插件手册》，并在“组”下。当前，test5应用仅支持源类型= 4（RTSP）。类似的方法也可以用于其他类型的源。可通过两种方式触发智能记录事件：<code>gst-nvdssr.h ``header file for more details about smart record. Event based recording can be enabled by setting ``smart-record``[sourceX]</code></p>
<p>1.通过云消息。要通过云消息触发智能记录，应将Test5应用程序配置为充当消息使用者。可以通过相应地配置[message-consumerX]组来完成。配置消息使用者之后，应在需要基于事件的记录的源上启用智能记录。可以按照以下步骤进行操作： <code>smart-record=1</code> 预计以下最小json消息将触发智能记录的开始/停止。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> &#123;</span><br><span class="line">command: string   &#x2F;&#x2F; &lt;start-recording &#x2F; stop-recording&gt;</span><br><span class="line">start: string     &#x2F;&#x2F; &quot;2020-05-18T20:02:00.051Z&quot;</span><br><span class="line">end: string       &#x2F;&#x2F; &quot;2020-05-18T20:02:02.851Z&quot;,</span><br><span class="line">sensor: &#123;</span><br><span class="line">id: string</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>2.通过当地活动。set <code>smart-record=2</code>，这将通过云消息以及本地事件启用智能记录。为了演示通过本地事件进行的基于事件的记录，默认情况下，应用程序每十秒钟触发一次启动/停止事件。此间隔和其他参数是可配置的。</p>
<h4 id="OTA模型更新"><a href="#OTA模型更新" class="headerlink" title="OTA模型更新"></a>OTA模型更新</h4><p>Test5应用程序可以动态更新正在运行的管道中的模型。为此，该应用程序提供了命令行选项<code>-o</code>。如果使用<code>-o</code>（ota_override_file）选项启动了test5应用，则将监视对该文件的任何更改，并基于该文件中的更改，使用新模型即时更新正在运行的管道。</p>
<h4 id="使用OTA功能"><a href="#使用OTA功能" class="headerlink" title="使用OTA功能"></a>使用OTA功能</h4><p>执行以下操作以使用OTA功能：</p>
<ol>
<li><code>deepstream-test5-app</code>使用选项运行<code>-o &lt;ota_override_file&gt;</code></li>
<li>在DS应用程序运行时，<code>&lt;ota_override_file&gt;</code>使用新的模型详细信息进行更新并保存</li>
<li>文件内容更改被检测到<code>deepstream-test5-app</code>，然后开始模型更新过程。当前，仅模型更新功能受支持为OTA功能的一部分。</li>
</ol>
<p><strong>即时模型更新的假设</strong>：</p>
<ol>
<li>新模型必须具有与先前模型相同的网络参数配置（例如，网络分辨率，网络体系结构，类数）</li>
<li>开发人员将提供的新模型的引擎文件或缓存文件</li>
<li>对于其它更新的值的配置参数等，，，，等等，如果在覆盖文件提供，将不具有模型开关之后的任何效果。<code>primary gie``group-threshold``bbox color``gpu-id``nvbuf-memory-type</code></li>
<li><code>Secondary gie</code> 模型更新未通过验证，仅主模型更新通过了验证。</li>
<li>在动态模型更新过程中，不应观察到丢帧/无推断的帧</li>
<li>如果模型更新失败，错误消息将打印在控制台上，并且管道应继续在旧模型配置下运行</li>
<li>需要config-file参数来抑制配置文件解析错误打印，在模型切换过程中不使用该配置文件中的值</li>
</ol>
]]></content>
      <tags>
        <tag>NVIDIA</tag>
      </tags>
  </entry>
  <entry>
    <title>NCNN</title>
    <url>/2020/11/29/NCNN/</url>
    <content><![CDATA[<h1 id="Windows安装NCNN"><a href="#Windows安装NCNN" class="headerlink" title="Windows安装NCNN"></a>Windows安装NCNN</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/Tencent/ncnn.git</span><br><span class="line">$ <span class="built_in">cd</span> ncnn</span><br><span class="line">$ git submodule update --init</span><br></pre></td></tr></table></figure>
<p>如果按教程说的<code>git submodule update --init</code>会很慢，可以去这里<a href="https://gitee.com/wwdok/glslang下载这个仓库到ncnn/glslang里面。">https://gitee.com/wwdok/glslang下载这个仓库到ncnn/glslang里面。</a></p>
<h3 id="Build-for-Windows-x64-using-Visual-Studio-Community-2019"><a href="#Build-for-Windows-x64-using-Visual-Studio-Community-2019" class="headerlink" title="Build for Windows x64 using Visual Studio Community 2019"></a>Build for Windows x64 using Visual Studio Community 2019</h3><p>【<a href="https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-visual-studio-community-2017">offcial tutorial</a>】</p>
<p>从Visual Studio文件夹里打开命令行窗口，如下图橙圈所示，不要简单地通过cmd打开命令行窗口，否则后面执行cmake语句时会遇到cl.exe找不到的问题。同时，以管理员身份打开，以便后面运行<code>mkdir</code>命令。</p>
<p><img src="https://pic.downk.cc/item/5fa667791cd1bbb86be8cc89.jpg" width=70% style="zoom: 50%;" ></p>
<h2 id="Build-protobuf-library"><a href="#Build-protobuf-library" class="headerlink" title="Build protobuf library:"></a>Build protobuf library:</h2><p>Download protobuf-3.4.0(6.03MB) from <a href="https://github.com/google/protobuf/archive/v3.4.0.zip">https://github.com/google/protobuf/archive/v3.4.0.zip</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; <span class="built_in">cd</span> &lt;protobuf-root-dir&gt;</span><br><span class="line">&gt; mkdir build_folder</span><br><span class="line">&gt; <span class="built_in">cd</span> build_folder</span><br><span class="line">&gt; cmake -G<span class="string">&quot;NMake Makefiles&quot;</span> -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=%<span class="built_in">cd</span>%/install -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_MSVC_STATIC_RUNTIME=OFF ../cmake</span><br><span class="line">&gt; nmake</span><br><span class="line">&gt; nmake install</span><br></pre></td></tr></table></figure>
<p>备注：</p>
<ul>
<li><p>nmake的用法：<a href="https://www.cnblogs.com/juluwangshier/p/11789311.html">WINDOWS CMAKE与NMAKE</a></p>
</li>
<li><p>因为 protobuf-3.4.0 目录下已经有一个BUILD文件，所以<code>mkdir build</code>不了，我改成build_forlder</p>
</li>
</ul>
<p><img src="https://pic.downk.cc/item/5fa6732a1cd1bbb86beb2387.jpg" width=70%></p>
<ul>
<li>第四句命令最后是<code>../cmake</code>，代表指向<code>protobuf-3.4.0/cmake</code>。</li>
<li>cmake语句中含有<code>%cd%</code>，代表当前的路径。</li>
</ul>
<p>我的实际安装过程是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">**********************************************************************</span><br><span class="line">** Visual Studio 2019 Developer Command Prompt v16.7.5</span><br><span class="line">** Copyright (c) 2020 Microsoft Corporation</span><br><span class="line">**********************************************************************</span><br><span class="line">[vcvarsall.bat] Environment initialized <span class="keyword">for</span>: <span class="string">&#x27;x64&#x27;</span></span><br><span class="line"></span><br><span class="line">C:\Windows\System32&gt;<span class="built_in">cd</span> C:\MachineLearning\CV\protobuf-3.4.0</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0&gt;mkdir build</span><br><span class="line">子目录或文件 build 已经存在。</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0&gt;mkdir build_folder</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0&gt;<span class="built_in">cd</span> build_folder</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0\build_folder&gt;cmake -G<span class="string">&quot;NMake Makefiles&quot;</span> -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=%<span class="built_in">cd</span>%/install -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_MSVC_STATIC_RUNTIME=OFF ../cmake</span><br><span class="line">-- The C compiler identification is MSVC 19.27.29112.0</span><br><span class="line">-- The CXX compiler identification is MSVC 19.27.29112.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/Hostx64/x64/cl.exe</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/Hostx64/x64/cl.exe - works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">......</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: C:/MachineLearning/CV/protobuf-3.4.0/build_folder</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0\build_folder&gt;nmake</span><br><span class="line"></span><br><span class="line">Microsoft (R) 程序维护实用工具 14.27.29112.0 版</span><br><span class="line">版权所有 (C) Microsoft Corporation。  保留所有权利。</span><br><span class="line"></span><br><span class="line">Scanning dependencies of target libprotobuf-lite</span><br><span class="line">[  0%] Building CXX object CMakeFiles/libprotobuf-lite.dir/src/google/protobuf/arena.cc.obj</span><br><span class="line">arena.cc</span><br><span class="line">[  1%] Building CXX object CMakeFiles/libprotobuf-lite.dir/src/google/protobuf/arenastring.cc.obj</span><br><span class="line">arenastring.cc</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[100%] Linking CXX executable protoc.exe</span><br><span class="line">[100%] Built target protoc</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0\build_folder&gt;nmake install</span><br><span class="line"></span><br><span class="line">Microsoft (R) 程序维护实用工具 14.27.29112.0 版</span><br><span class="line">版权所有 (C) Microsoft Corporation。  保留所有权利。</span><br><span class="line"></span><br><span class="line">[ 12%] Built target libprotobuf-lite</span><br><span class="line">[ 52%] Built target libprotobuf</span><br><span class="line">[ 52%] Built target js_embed</span><br><span class="line">[ 99%] Built target libprotoc</span><br><span class="line">[100%] Built target protoc</span><br><span class="line">Install the project...</span><br><span class="line">-- Install configuration: <span class="string">&quot;Release&quot;</span></span><br><span class="line">-- Installing: C:/MachineLearning/CV/protobuf-3.4.0/build_folder/install/lib/libprotobuf-lite.lib</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">-- Installing: C:/MachineLearning/CV/protobuf-3.4.0/build_folder/install/cmake/tests.cmake</span><br></pre></td></tr></table></figure>
<h2 id="Build-ncnn-library"><a href="#Build-ncnn-library" class="headerlink" title="Build ncnn library"></a>Build ncnn library</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">&gt; mkdir -p build</span><br><span class="line">&gt; <span class="built_in">cd</span> build</span><br><span class="line">&gt; cmake -G<span class="string">&quot;NMake Makefiles&quot;</span> -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=%<span class="built_in">cd</span>%/install -DProtobuf_INCLUDE_DIR=%protobuf-root-dir%/build/install/include -DProtobuf_LIBRARIES=%protobuf-root-dir%/build/install/lib/libprotobuf.lib -DProtobuf_PROTOC_EXECUTABLE=%protobuf-root-dir%/build/install/bin/protoc.exe -DNCNN_VULKAN=ON ..</span><br><span class="line">&gt; nmake</span><br><span class="line">&gt; nmake install</span><br></pre></td></tr></table></figure>
<p>备注：我这里在命令行里使用了%protobuf-root-dir%，而不是官方的 <protobuf-root-dir> ，因为我设置了下面的环境变量：</p>
<p><img src="https://pic.downk.cc/item/5fa676231cd1bbb86beba7e7.jpg" style="zoom:80%;" ></p>
<p>如果上面的语句<code>-DNCNN_VULKAN=ON</code>没有关掉的话，在安装ncnn前需要安装vulkan，否则安装ncnn时会遇到Cmake错误。但去官网下载的话速度很慢，还容易断开网络连接，一个方法是去csdn上下载别人上传的，</p>
<p>另外一个办法是用这位网友的<a href="https://blog.csdn.net/tankweight/article/details/103704682?utm_medium=distribute.pc_relevant_download.none-task-blog-baidujs-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-baidujs-1.nonecase">shell脚本</a>执行下载，你直接复制粘贴到文本里的话，部门代码可能复制会出错，一个窍门是查看该网页的源代码，将出错的那句代码用源代码里的复制粘贴代替</p>
<p><img src="https://pic.downk.cc/item/5fa6c7ca1cd1bbb86bfe19f7.jpg"></p>
<p>完整shell脚本我还是放一下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">curl: (28) Connection timed out after 15000 milliseconds</span></span><br><span class="line"><span class="meta">#</span><span class="bash">curl: (28) Operation timed out after 20001 milliseconds with 0 out of 0 bytes received</span></span><br><span class="line"><span class="meta">#</span><span class="bash">curl: (56) OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 104</span></span><br><span class="line">attempt_counter=0</span><br><span class="line">max_attempts=20</span><br><span class="line">run_step=0</span><br><span class="line">max_run_steps=2</span><br><span class="line">fileUrl=&#x27;https://sdk.lunarg.com/sdk/download/1.1.130.0/linux/vulkansdk-linux-x86_64-1.1.130.0.tar.gz?Human=true&#x27;</span><br><span class="line">until $(curl --connect-timeout 30 --retry-delay 10 --retry-max-time 25 -C - -o 1.1.130.0.tar.gz $fileUrl); do</span><br><span class="line">if [ attemptcounter−eq&#123;max_attempts&#125; ];</span><br><span class="line">then</span><br><span class="line">echo &quot;Max attempts reached&quot;</span><br><span class="line">exit 1</span><br><span class="line">elif [ runstep−eq&#123;max_run_steps&#125; ];</span><br><span class="line">then</span><br><span class="line">echo retrying in 45 seconds...</span><br><span class="line">run_step=0</span><br><span class="line">sleep 45</span><br><span class="line">else</span><br><span class="line">echo Transfer distrupted,retrying in 20 seconds...</span><br><span class="line">sleep 20</span><br><span class="line">fi</span><br><span class="line">printf &#x27;.&#x27;</span><br><span class="line">attempt_counter=(attempt_counter+1)</span><br><span class="line">run_step=(run_step+1)</span><br><span class="line">done</span><br><span class="line">printf &#x27;OK\n&#x27;</span><br></pre></td></tr></table></figure>
<p>然后在命令行窗口执行 <code>sh downloadVulkanSDK.sh</code> 即可。</p>
<h2 id="格式转换"><a href="#格式转换" class="headerlink" title="格式转换"></a>格式转换</h2><p>将其他框架的模型格式转换为ncnn格式。在ncnn/tool目录下就包含了这些转换工具：</p>
<p><img src="https://pic.downk.cc/item/5f9ed3a11cd1bbb86b1f24fb.jpg" style="zoom:80%;" ></p>
<p>也有网页版在线转换工具：<a href="https://convertmodel.com/">https://convertmodel.com/</a></p>
<h1 id="NCNN-Wiki"><a href="#NCNN-Wiki" class="headerlink" title="NCNN Wiki"></a>NCNN Wiki</h1><ul>
<li><p><a href="https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure">《param and model file structure》</a>    </p>
<p>引言：<code>.param</code>文件不仅可以通过Netron打开查看图形化结构，也可以用VSCode打开用文字的形式查看网络结构，但你打开后需要理解param里的每一行每一列的含义，这就需要查看这篇wiki了。</p>
<p>其中，层参数词典分成4小块来看可能逻辑上更清晰一点，也可以搭配这篇<a href="https://liumin.blog.csdn.net/article/details/103247724">博客</a>理解。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201121221315580.png" alt="image-20201121221315580" style="zoom:80%;" /></p>
<p>看源码更深入理解这个数组类型key的用法：</p>
</li>
</ul>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201121221545507.png" alt="image-20201121221545507" style="zoom:80%;" /></p>
<p>注意，如果参数键的值用的是<a href="https://github.com/Tencent/ncnn/wiki/operation-param-weight-table">参数表</a>里的默认值，那.param里就没有写出来了。</p>
<p><strong><a href="https://github.com/dog-qiuqiu/Yolo-Fastest">Yolo-Fastest</a></strong>：About ncnn_sample Compile Guide：<a href="https://github.com/dog-qiuqiu/Yolo-Fastest/issues/25">https://github.com/dog-qiuqiu/Yolo-Fastest/issues/25</a></p>
<p><a href="https://blog.csdn.net/qq_36113487/article/details/100676205">《onnx2ncnn并在pc端调用ncnn模型》</a></p>
<h2 id="NCNN使用案例"><a href="#NCNN使用案例" class="headerlink" title="NCNN使用案例"></a>NCNN使用案例</h2><ul>
<li><a href="https://github.com/nihui/ncnn-android-mobilenetssd">ncnn-android-mobilenetssd</a></li>
</ul>
<p>如果你想要在SDK&lt;24的情况下使用这个App，因为vulkan最低要求SDK是24，那怎么对下下来的项目进行修改，使其能用正常运行呢</p>
<p>首先你想到的是Ctrl + Shift + R，把ncnn-android-vulkan-lib全部替换成ncnn-android-lib，但当你build的时候，会遇到下面报错：</p>
<p>ninja: error: ‘C:/AndroidDev/ncnn-android-mobilenetssd/app/src/main/jni/ncnn-android-lib/armeabi-v7a/libglslang.a’, needed by ‘C:/AndroidDev/ncnn-android-mobilenetssd/app/build/intermediates/cmake/debug/obj/armeabi-v7a/libmobilenetssdncnn.so’, missing and no known rule to make it</p>
<p>简单说就是缺少<code>libglslang.a</code>， 但这个东西是vulkan才需要的，所以我们代码上要改成不需要这个，在CMakeLists.txt里注释掉</p>
<p>它：</p>
<p><img src="https://pic.downk.cc/item/5fb0c5efdf3cf7596d331566.jpg"></p>
<p>然后在mobilenetssdncnn_jni.cpp里注释掉一些报错的代码：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//    ncnn::create_gpu_instance();</span></span><br><span class="line"><span class="comment">//    ncnn::destroy_gpu_instance();</span></span><br><span class="line">    <span class="comment">// use vulkan compute</span></span><br><span class="line"><span class="comment">//    if (ncnn::get_gpu_count() != 0)</span></span><br><span class="line"><span class="comment">//        opt.use_vulkan_compute = true;</span></span><br><span class="line"><span class="comment">//    if (use_gpu == JNI_TRUE &amp;&amp; ncnn::get_gpu_count() == 0)</span></span><br><span class="line"><span class="comment">//    &#123;</span></span><br><span class="line"><span class="comment">//        return NULL;</span></span><br><span class="line"><span class="comment">//        //return env-&gt;NewStringUTF(&quot;no vulkan capable gpu&quot;);</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"><span class="comment">//        ex.set_vulkan_compute(use_gpu);</span></span><br></pre></td></tr></table></figure>
<p>在此解读一下ncnn android示例里的<strong>CMakeLists.txt</strong>：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import ncnn library</span></span><br><span class="line"><span class="keyword">add_library</span>(ncnn STATIC IMPORTED)</span><br><span class="line"><span class="comment"># change this folder path to yours</span></span><br><span class="line"><span class="keyword">set_target_properties</span>(ncnn PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libncnn.a)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="keyword">include</span>/ncnn)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(glslang STATIC IMPORTED)</span><br><span class="line"><span class="keyword">add_library</span>(OGLCompiler STATIC IMPORTED)</span><br><span class="line"><span class="keyword">add_library</span>(OSDependent STATIC IMPORTED)</span><br><span class="line"><span class="keyword">add_library</span>(SPIRV STATIC IMPORTED)</span><br><span class="line"><span class="keyword">set_target_properties</span>(glslang PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libglslang.a)</span><br><span class="line"><span class="keyword">set_target_properties</span>(OGLCompiler PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libOGLCompiler.a)</span><br><span class="line"><span class="keyword">set_target_properties</span>(OSDependent PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libOSDependent.a)</span><br><span class="line"><span class="keyword">set_target_properties</span>(SPIRV PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libSPIRV.a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># openmp</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -fopenmp&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fopenmp&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_SHARED_LINKER_FLAGS <span class="string">&quot;$&#123;CMAKE_SHARED_LINKER_FLAGS&#125; -fopenmp&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">DEFINED</span> ANDROID_NDK_MAJOR <span class="keyword">AND</span> <span class="variable">$&#123;ANDROID_NDK_MAJOR&#125;</span> <span class="keyword">GREATER</span> <span class="number">20</span>)</span><br><span class="line">    <span class="keyword">set</span>(CMAKE_SHARED_LINKER_FLAGS <span class="string">&quot;$&#123;CMAKE_SHARED_LINKER_FLAGS&#125; -static-openmp&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -fomit-frame-pointer -fstrict-aliasing -ffast-math&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fomit-frame-pointer -fstrict-aliasing -ffast-math&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -fvisibility=hidden&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fvisibility=hidden -fvisibility-inlines-hidden&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable rtti and exceptions</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fno-rtti -fno-exceptions&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(mobilenetssdncnn SHARED mobilenetssdncnn_jni.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(mobilenetssdncnn</span><br><span class="line">    ncnn</span><br><span class="line">    glslang SPIRV OGLCompiler OSDependent</span><br><span class="line">    android</span><br><span class="line">    z</span><br><span class="line">    log</span><br><span class="line">    jnigraphics</span><br><span class="line">    vulkan</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>add_library()这个命令顾名思义就是添加库，添加已有的库，但它的功能不止这个，它还可以先生成库再添加进去，cmake的<a href="https://cmake.org/cmake/help/latest/command/add_library.html">官方帮助文档</a>也说明了这两点，ncnn这里前面几个用到的是<a href="https://cmake.org/cmake/help/latest/command/add_library.html#imported-libraries">imported-libraries</a>的形式，一般这种形式的add_library后面都会跟着set_target_properties来指定PROPERTIES IMPORTED_LOCATION，而最后一个<code>add_library(mobilenetssdncnn SHARED mobilenetssdncnn_jni.cpp)</code>就是更常见的<a href="https://cmake.org/cmake/help/latest/command/add_library.html#normal-libraries">Normal Libraries</a>形式，也就是将指定的cpp源文件先生成目标文件，然后添加到工程中去。</p>
<p>include_directories() 命令是为了让 CMake 在编译时期能找到头文件，里面传进去的就是头文件地址。</p>
<p><a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html">target_link_libraries()</a>会把 ncnn、glslang、SPIRV、OGLCompiler、OSDependent、android、z、log、jnigraphics、vulkan这些库都链接到mobilenetssdncnn身上，最后一起打包进libmobilenetssdncnn.so里。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201122154500293.png" alt="image-20201122154500293"></p>
<ul>
<li><a href="https://github.com/nihui/ncnn-android-yolov5">ncnn-android-yolov5</a></li>
</ul>
<h1 id="加速ncnn"><a href="#加速ncnn" class="headerlink" title="加速ncnn"></a>加速ncnn</h1><ol>
<li><p><strong>使用bf16</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/112564372">【official blog】</a></p>
</li>
</ol>
<p>bf16的首字母b是brain，因为这个格式是google-brain团队发明的。</p>
<p><strong>fp16</strong> V.S. <strong>fp32</strong> V.S. <strong>bf16</strong></p>
<p><img src="https://pic.downk.cc/item/5fb1372c1a64424b032677af.jpg"></p>
<p>就是把float 32bit后面的16bit直接砍掉，跟tflite的int8有点类似，不过int8砍的更猛</p>
<p>启用这个功能只需要在jni cpp里加上</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">net.opt.use_packing_layout &#x3D; true;</span><br><span class="line">net.opt.use_bf16_storage &#x3D; true;</span><br></pre></td></tr></table></figure>
<p>就打开了。</p>
<p>以ncnn-android-yolov5为例：</p>
<p><img src="https://pic.downk.cc/item/5fb13821607ab2c3ed8bf1d8.jpg"></p>
<ol>
<li><p>图优化</p>
<p>图优化就是把融合、去除、替代网络结构中的层（如下图蓝框所示）。进入ncnn/build/tools目录，可以发现tools目录下存在ncnnoptimize的可执行文件，它的使用命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;ncnnoptimize [inparam] [inbin] [outparam] [outbin] [flag]</span><br></pre></td></tr></table></figure>
<p>最后那个参数flag是代表<code>storage_type</code>，从下图的源码（ncnn\tools\ncnnoptimize.cpp）截图可以看出，0代表fp32存储方式， 1代表fp16储存方式，</p>
</li>
</ol>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201121121202.png" alt="20201121121202" style="zoom:80%;" /></p>
<p>你可能看网上一些ncnnoptimize教程，还会出现flag=65536，同样从源码中可以看出其效果跟flag=1一样，都是fp16的存储方式。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201121135135709.png" alt="image-20201121135135709" style="zoom: 80%;" /></p>
<p>ncnnoptimize优化器是优化整个网络模型，即将一个模型匹配优化器中所有适用的优化方法，进而优化整个ncnn网络模型；而不是单独可选择的优化方法。</p>
<ol>
<li><p><strong>int 8 量化</strong></p>
<p>《<a href="https://www.cnblogs.com/wanggangtao/p/11352948.html">NCNN量化之ncnn2table和ncnn2int8</a>》</p>
<p><a href="https://zhuanlan.zhihu.com/p/71881443">《NCNN Conv量化详解（一）》</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/72375164">《NCNN量化详解（二）》</a></p>
<pre><code>[《yolov3：ncnn之int8量化》](https://zhuanlan.zhihu.com/p/299722824)

 量化工具：[EasyQuant](https://github.com/deepglint/EasyQuant)（[技术答疑](https://github.com/deepglint/EasyQuant/issues/3)）、[eq-ncnn](https://github.com/deepglint/eq-ncnn)
</code></pre></li>
<li><p><strong>手工优化ncnn模型结构</strong></p>
</li>
</ol>
<p><a href="https://zhuanlan.zhihu.com/p/93017149?from_voters_page=true">https://zhuanlan.zhihu.com/p/93017149?from_voters_page=true</a></p>
<h1 id="其他NCNN技巧"><a href="#其他NCNN技巧" class="headerlink" title="其他NCNN技巧"></a>其他NCNN技巧</h1><ul>
<li>如何加密ncnn模型 : <a href="https://zhuanlan.zhihu.com/p/268327784">https://zhuanlan.zhihu.com/p/268327784</a></li>
</ul>
<p>CSDN NCNN 专栏</p>
<p><a href="https://blog.csdn.net/sinat_31425585/category_9312419.html">https://blog.csdn.net/sinat_31425585/category_9312419.html</a></p>
<p><a href="https://blog.csdn.net/shanglianlm/category_9529596.html">https://blog.csdn.net/shanglianlm/category_9529596.html</a></p>
]]></content>
      <tags>
        <tag>NCNN</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOV5</title>
    <url>/2020/10/21/YOLOV5/</url>
    <content><![CDATA[<h2 id="YOLOV5-Github-Repo"><a href="#YOLOV5-Github-Repo" class="headerlink" title="YOLOV5 Github Repo"></a><a href="https://github.com/ultralytics/yolov5"><strong>YOLOV5 Github Repo</strong></a></h2><p><strong>推荐阅读：</strong></p>
<p>《<a href="https://zhuanlan.zhihu.com/p/172121380"><strong>深入浅出Yolo系列之Yolov5核心基础知识完整讲解</strong></a>》</p>
<p><strong><a href="https://zhuanlan.zhihu.com/p/183838757">《进击的后浪yolov5深度可视化解析》</a></strong></p>
<p><strong><a href="https://zhuanlan.zhihu.com/p/159371985">《目标检测之yolov5深度讲解》</a></strong></p>
<p>YOLOV5的特色：</p>
<ol>
<li>集成了混合精度训练</li>
<li><a href="https://github.com/ultralytics/yolov5/issues/607">超参数可以自己演化</a></li>
<li>矩形图像训练和推理</li>
<li><a href="https://github.com/ultralytics/yolov5/issues/1289">集成了Weights &amp; Biases</a></li>
<li><a href="https://github.com/ultralytics/yolov5/issues/304">支持模型剪枝稀疏</a></li>
</ol>
<h2 id="经验与建议："><a href="#经验与建议：" class="headerlink" title="经验与建议："></a><strong>经验与建议</strong>：</h2><ol>
<li>下载pytorch和torchvision去官网获取命令行下载，不要自己输个pip install pytorch/torchvision就下载，很可能会报错。</li>
</ol>
<p><img src="https://pic.downk.cc/item/5fa632791cd1bbb86bdd8aef.png" width=70%></p>
<p>细心的你可能会发现通过conda和pip安装pytorch，命令行内容是不一样的，一个是torch一个是pytorch，这个问题我在这个<a href="https://github.com/pytorch/pytorch/issues/47333">issue</a>里问过pytorch的贡献者，总之不需要通过conda 和 pip安装两次。</p>
<p>如果通过这个命令行安装太慢，那你可以选择去清华镜像源网站（<a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/）下载自己对应的版本，然后把压缩包放到`C:\ProgramData\Anaconda3\pkgs`里面，以管理员身份打开命令行窗口，cd到这里，然后运行`conda">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/）下载自己对应的版本，然后把压缩包放到`C:\ProgramData\Anaconda3\pkgs`里面，以管理员身份打开命令行窗口，cd到这里，然后运行`conda</a> install —offline pytorch-1.7.0-py3.7_cuda101_cudnn7_0.tar.bz2<code>。torchvision以此类推。最后运行</code>python -m torch.utils.collect_env`检查一下安装是否成功（需要先安装typing_extensions）。</p>
<p>注意，通过pip来安装或升级torch和torchvision时，务必在最后加上<code>-f https://download.pytorch.org/whl/torch_stable.html</code>, 比如升级torchvision到0.8.1的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install torchvision&#x3D;&#x3D;0.8.1 -f https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.html --user</span><br></pre></td></tr></table></figure>
<ol>
<li><p>自己训练模型在准备数据集时，需要注意图片和标签的文件夹结构和命名，我采用的是像coco128.yaml一样的指定文件夹路径而不是指定txt文件路径，这样的话图片文件夹和标签文件夹需要命名为images和labels，且属于同级目录，子目录均包括train和val，相关的文档在<a href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#3-organize-directories">这里</a>，相关的代码是dataset.py里的<code>img2label_paths()</code>和<code>LoadImagesAndLabels()</code>。因为我的这个数据集也会用nanodet训练，它们理论上应该共用一个图片文件夹，但是一开始nanodet的数据集格式是VOC，所以图片文件夹叫<code>JPEGImages</code>, 但是yolov5这边得叫images，为了不改nanodet的训练脚本，也不重复一遍图片，我用了软链接的方法，windows下软链接命令和运行成功的样子是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mklink &#x2F;d D:\MachineLearning\DataSet\SpecialVehicle\images D:\MachineLearning\DataSet\SpecialVehicle\JPEGImages</span><br><span class="line">为 D:\MachineLearning\DataSet\SpecialVehicle\images &lt;&lt;&#x3D;&#x3D;&#x3D;&gt;&gt; D:\MachineLearning\DataSet\SpecialVehicle\JPEGImages 创建的符号链接</span><br></pre></td></tr></table></figure>
<p>详细的windows软链接使用方法见<a href="https://blog.csdn.net/qq_37861937/article/details/79064841">这里</a>。</p>
</li>
<li><p>训练开始时会让你输入wandb API key，这个去<code>https://wandb.ai/settings</code>里找，复制过来后粘贴，粘贴完是不可见的，所以不要以为“咦怎么粘贴不了”。另外，当你使用yolov5训练新的目标检测任务时，请更改train.py里的wandb.init()里的project名，这样你在wandb网站上才能区分不同的目标检测任务。</p>
</li>
<li><p>你可能第一次训练完100个epochs后发现，曲线像下图这样还没平稳，感觉精度还可以更高，损失还可以更低：</p>
</li>
</ol>
<p><img src="https://pic.downk.cc/item/5fa6329d1cd1bbb86bdd9188.png"></p>
<p>然后你想重新训练，可以的。</p>
<p>第一，修改命令行的epochs为你第二次想要训练的次数；</p>
<p>第二，修改超参数。打开hyp.custom.yaml,里面有一堆超参数让我们设置，最下面几个是数据增强，第二次重新训练不需要改变，要改变的是 <code>lr0</code> 和 <code>warmup_epochs</code> 。<code>lr0</code>要改成多少，要看你上次训练结束时的学习率是多少，从下图可以看出我这里是0.002，所以<code>lr0</code>从0.01改成0.002，然后<code>warmup_epochs</code>改成0，设成0后，就是说我们第二次就不需要这个热身的过程了，因为我们的意图很明显，就是让曲线顺延着第一次训练结果走下去，如果热身的话，学习率就会像下图那样会有个短期的下降。这个warm up就是让你的学习率一开始以较小的值训练，经过几个epoch后，学习率再恢复到你设置的<code>lr0</code>学习率，这就是图中曲线一开始是一个爬坡样子的原因。</p>
<p><img src="https://pic.downk.cc/item/5fa632b31cd1bbb86bdd9551.png"></p>
<p>如果第二次训练不改这两个超参数会怎么样呢？会导致你第二次所有epoch跑完了，检测效果跟第一次结束时差不多，甚至更糟糕！</p>
<p>其他参数,<code>momentum</code>是控制历史权重值的影响因子，一般是0.9几；<code>weight_decay</code> 是加在正则项前面的，设置得大一点可以减小过拟合。</p>
<p>这么一直接着训练下去，怎么判断模型是否到底过拟合了没有呢？我是看验证集val的曲线，只要它还没出现损失上升，就认为它还没过拟合</p>
<p><img src="https://pic.downk.cc/item/5fa632c81cd1bbb86bdd9980.png"></p>
<p>4.我们可以利用训练好的模型去检测新增的数据集，将检测结果导出生成txt文件，然后在labelimg里再把annotation的保存目录改到txt文件保存的目录就可以了，这样labelimg就会把检测结果矩形框显示出来，你只需要调整一下矩形框，就可以完成标注了，实现半自动化标注。</p>
<p>作者<strong><a href="https://github.com/glenn-jocher">glenn-jocher</a></strong>说：</p>
<blockquote>
<p>如果你是想把检测结果导出为txt格式的文件，使用下面代码：<br><code>python detect.py --save-txt</code></p>
<p>如果你是想把检测结果导出为coco json格式的文件，使用下面代码：<br><code>python test.py --save-json</code></p>
</blockquote>
<p>不过我看了一下<code>detect.py</code>和<code>test.py</code>的源码，发现两者都支持导出json，但只有detext.py支持导出txt，然后，两者指定输入图片的位置也不太一样，detect.py通过<code>--source</code>指定输入图片位置，比较好理解，默认位于<code>inference/images</code>，而test.py是通过<code>--data</code>指定输入图片位置，两者都可以通过<code>--save-dir</code>指定输出文件位置，但<code>detect.py</code>的输出位置默认位于<code>inference/output</code>， <code>test.py</code>的输出位置默认位于<code>runs/test</code>。</p>
<p><code>detect.py</code>  :</p>
<p><img src="https://pic.downk.cc/item/5fa635511cd1bbb86bde1741.jpg"></p>
<p>有两个参数比较有意思，一个是 <code>--classes</code>，可用来过滤不想要的类别，一个是<code>--augment</code>，用于检测时提高检测结果的准确度，相关的讨论在<a href="https://github.com/ultralytics/yolov5/issues/303">这里</a>， 更多TTA的资料（<a href="https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it">link1</a>， <a href="https://github.com/qubvel/ttach">link2</a>）。</p>
<p><code>test.py</code> ：</p>
<p><img src="https://pic.downk.cc/item/5fa635c61cd1bbb86bde2bbf.jpg"></p>
<p>我这边实际运行的命令行是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python detect.py --weights weights&#x2F;iguard-best.pt --source D:\MachineLearning\DataSet\iGuardDataset\IMAGE\temp --save-dir D:\MachineLearning\DataSet\iGuardDataset\IMAGE\temp\labels --save-txt --augment</span><br></pre></td></tr></table></figure>
<p>导出的结果如下所示，我看了一下效果很好：</p>
<p><img src="https://pic.downk.cc/item/5fabe49d1cd1bbb86b0a3641.jpg"></p>
<p>现在的yolov5（2020年11月12日）导出的txt有问题，在每一行最后面它都有个空格，这在LabelImg里打开是会报错的，得把这个空格删掉，我提了个<a href="https://github.com/ultralytics/yolov5/issues/1355">issue</a>告诉作者这个问题，后来作者让我PR了，再后来我发现 —source 跟 —save-dir 不能是同一个目录，否则代码会删除所有 —source 里的图片，不过那时候作者告诉我yolov5的文件结构已经重新设计了，大家请自己去看一下最新的detect.py里的参数。</p>
<ol>
<li>如果你的epochs设置的很大，但你看到val loss还没结束就已经升高很多了，于是你中断了训练，这时候模型文件会比预训练模型大很多，因为为了后面你resume，模型文件里还保存了下次接着训练的数据，所以你确定后面不会再训练的话，可以把模型里的多余数据删除，方法是在根目录下新建一个py脚本，运行以下代码：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> utils.general <span class="keyword">import</span> strip_optimizer</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">save_dir = <span class="string">r&#x27;E:/Repo/yolov5/runs/train/exp&#x27;</span></span><br><span class="line"></span><br><span class="line">last = os.path.join(save_dir, <span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;last.pt&#x27;</span>)</span><br><span class="line">best = os.path.join(save_dir, <span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;best.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#strip_optimizer(last)  # 一般resume是从加载last.pt开始，如果best.pt够用的话，last.pt就先留着</span></span><br><span class="line">strip_optimizer(best)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># actual_anchors.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">r&quot;E:\Repo\yolov5\runs\train\exp\weights\best.pt&quot;</span>)[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line"></span><br><span class="line">m = model.model[-<span class="number">1</span>]  <span class="comment"># Detect()</span></span><br><span class="line">m.anchors  <span class="comment"># in stride units</span></span><br><span class="line">m.anchor_grid  <span class="comment"># in pixel units</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.anchor_grid.view(-<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h2 id="疑问与解释"><a href="#疑问与解释" class="headerlink" title="疑问与解释"></a><strong>疑问与解释</strong></h2><ol>
<li>10月份的比7月份的版本在models/yolov5s.yaml里面的backbone多了   [-1, 3, BottleneckCSP, [1024, False]]，这一行是什么意思，什么作用，有什么影响？</li>
</ol>
<ol>
<li><p><strong>yolov5在哪里配置数据增强？</strong></p>
<p> SSD是在config里通过选择数据增强项来配置数据增强用哪些，yolov5的数据增强配置在<code>data/hyp.scratch.yaml</code>里,这里除了有数据增强的参数外，还有诸如学习率等超参数的配置。yolov5默认支持并应用多种图像增强：</p>
<p> <img src="https://pic.downk.cc/item/5f9ce17c1cd1bbb86b89a76d.jpg" width =70%></p>
</li>
<li><p><strong>怎样减小false positive？</strong></p>
<p> false positive高的话就是说图像中没目标却把一些背景识别成目标，这个issue里讨论了这个问题，下面的兄弟的做法是：<br> <img src="https://pic.downk.cc/item/5fa632de1cd1bbb86bdd9db0.png"></p>
<p> （1）把无目标的图片放到images文件夹里</p>
<p> （2）创建一个同名的txt文件，里面是空的</p>
<p> 但是也不宜过多，因为带目标的图片里那些背景部分其实就是很多的负样本了。</p>
</li>
<li><p><strong>为什么yolov5使用GIOU而不是使用CIOU？</strong></p>
<p> 这个问题在这个<a href="https://github.com/ultralytics/yolov5/issues/1113">issue</a>里也有讨论</p>
</li>
<li><p><strong>yolov5是如何实现输入矩形图像进行训练和推理的？</strong></p>
<p> mobilenetssd要求训练和推理的输入图片得是正方形，但yolov5不一样，推理时默认就是输入矩形图像（填充到长宽是32的最小倍数），因为作者glenn-jocher发现把输入图像处理为32倍数的矩形图像比处理成正方形图像更省推理时间【<a href="https://github.com/ultralytics/yolov3/issues/232">相关issue</a>】，而且转换成onnx格式后依然支持矩形推理【<a href="https://github.com/ultralytics/yolov3/issues/232#issuecomment-597760790">相关comment</a>】。<br> 矩形推理的图像处理思路是：把图像的长边缩放成<code>img_size</code>规定的大小,然后短边填充到一个最小32倍数的值。</p>
<p> 训练的话也可以做到，但需要你输入命令行参数进行确认，在train.py里有这么一句：</p>
<p> <code>parser.add_argument(&#39;--rect&#39;, action=&#39;store_true&#39;, help=&#39;rectangular training&#39;)</code></p>
<p> 从<code>action=&#39;store_true&#39;</code>可知，你只要在命令行里加上<code>--rect</code>(不需要输 <code>--rect True</code>)就可以进行矩形图像训练了。</p>
<p> 矩形训练时的预处理比矩形推理时的预处理更复杂一些，因为训练时需要先找到这一个batch里面最方形的那张图像，然后按上面所说的思路把这张最方形图像变成矩形后，接下来所有同批次的图像就要变成跟它一样大小。</p>
<p> 但这种处理方式有个小漏洞，就是当一个批次里，有一张图像很正方形时，那么最后整个batch里的所有图像都会被填充成类正方形【<a href="https://github.com/ultralytics/yolov3/issues/232#issuecomment-562870597">相关comment</a>】。</p>
<p> yolov5/utils/dataloader.py里有个<a href="https://github.com/ultralytics/yolov5/blob/481d46cffb0b7a2ec5cec76d9ec85357128b28ea/utils/datasets.py#L721">letterbox()</a>函数，就是用来预处理图像供神经网络矩形训练和矩形推理，维基百科对letterbox的释义是：</p>
<p> <img src=https://img-blog.csdnimg.cn/20201030101451732.png width=50%></p>
<p> 不过yolov5这里不是填充纯黑色，从letterbox()函数里的color=(114, 114, 114)可知是用灰色填充。</p>
</li>
<li><p>怎么看训练时runs文件夹里的labels.jpg那张图？</p>
<p> <img src="https://pic.downk.cc/item/5fa632371cd1bbb86bdd7757.png" width=70%></p>
<p> runs里的这张labels图展示了标签的分布情况，第一幅图展示的是数据集各个类别的标签数量，我只有一个类型，且其数目有4000多个，第二幅图展示的是标签/矩形框的中心点在图片中的位置，我这里的图说明大部分矩形框分布在中心，但四周的分布也有不少，总体比较平均，第三幅图说明的是矩形框的宽和高相对原图的宽和高的比例，我这里的类别是香烟，目标比较小，所以大部分是（0.05，0.05）左右。</p>
</li>
<li><p><strong>如何将yolov5的模型转换成ncnn的格式和tflite格式</strong></p>
</li>
</ol>
<p>转换成ncnn格式：作者告诉我有两种方式<a href="https://github.com/cmdbug/YOLOv5_NCNN/issues/26">https://github.com/cmdbug/YOLOv5_NCNN/issues/26</a></p>
<p>文中谈到yolo focus op在转换时会有问题，focus的示意图和代码如下，代码中像<code>x[..., 1::2, ::2]</code>等4个切片操作会生成下图4种颜色的tensor，这里面的<code>1: :2</code> 或<code>: : 2</code> 的含义就是 <code>start:stop:step</code> ，但第一个<code>...</code>我不确定是什么，感觉跟 <code>: : :</code> 一样，代表所有channel。</p>
<p><a href="https://imgchr.com/i/Bof1fg"><img src="https://s1.ax1x.com/2020/11/08/Bof1fg.png" alt="Bof1fg.png"></a></p>
<p>在<a href="https://arxiv.org/pdf/2003.13630.pdf">TResNet paper</a>. p2. 它又被叫做 SpaceToDepth</p>
<p>代码演示：<a href="https://github.com/ultralytics/yolov5/issues/804#issuecomment-678141008">https://github.com/ultralytics/yolov5/issues/804#issuecomment-678141008</a></p>
<ol>
<li>yolov5使用的LambdaLR的工作原理是什么</li>
</ol>
<p>相关代码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># train.py</span><br><span class="line">    if opt.linear_lr:</span><br><span class="line">        lf &#x3D; lambda x: (1 - x &#x2F; (epochs - 1)) * (1.0 - hyp[&#39;lrf&#39;]) + hyp[&#39;lrf&#39;]  # linear</span><br><span class="line">    else:</span><br><span class="line">        lf &#x3D; one_cycle(1, hyp[&#39;lrf&#39;], epochs)  # cosine 1-&gt;hyp[&#39;lrf&#39;]</span><br><span class="line">    scheduler &#x3D; lr_scheduler.LambdaLR(optimizer, lr_lambda&#x3D;lf)</span><br><span class="line">    </span><br><span class="line"># general.py </span><br><span class="line">def one_cycle(y1&#x3D;0.0, y2&#x3D;1.0, steps&#x3D;100):</span><br><span class="line">    # lambda function for sinusoidal ramp from y1 to y2</span><br><span class="line">    return lambda x: ((1 - math.cos(x * math.pi &#x2F; steps)) &#x2F; 2) * (y2 - y1) + y1</span><br></pre></td></tr></table></figure>
<p>当y1=1.0， y2=0.2时，lambda x: ((1 - math.cos(x <em> math.pi / steps)) / 2) </em> (y2 - y1) + y1的函数图如下：</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_16_46_35_image-20210325164632607.png" alt="image-20210325164632607"></p>
<p>从wandb上的学习率变化图可以看出，学习率在热身阶段先从极小值升高到0.01，再按sin函数下降到0.002。这两个值看起来是不是是上面函数的极值1和0.2的0.01倍。</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_16_51_38_image-20210325165136718.png" alt="image-20210325165136718"></p>
<p>因为一般学习率主要还受optimizer里设置的学习率的影响：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># train.py</span><br><span class="line">if opt.adam:</span><br><span class="line">        optimizer &#x3D; optim.Adam(pg0, lr&#x3D;hyp[&#39;lr0&#39;], betas&#x3D;(hyp[&#39;momentum&#39;], 0.999))  # adjust beta1 to momentum</span><br><span class="line">    else:</span><br><span class="line">        optimizer &#x3D; optim.SGD(pg0, lr&#x3D;hyp[&#39;lr0&#39;], momentum&#x3D;hyp[&#39;momentum&#39;], nesterov&#x3D;True)</span><br></pre></td></tr></table></figure>
<p><strong>转换为TFLite格式</strong>：</p>
<p>这里使用<a href="https://github.com/zldrobit">zldrobit</a> 开源的<a href="https://github.com/zldrobit/yolov5/tree/tf-android">fork yolov5的项目</a>，相关的话题在这个<a href="https://github.com/ultralytics/yolov5/pull/1127">issue</a>里。前面的步骤请阅读README.md，这里从下面的内容开始：</p>
<p>注意，在windows上面，执行以下语句前需要先执行<code>set PYTHONPATH=.</code>，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\MachineLearning\CV\Object_Detection\yolov5-tf-android&gt;set PYTHONPATH&#x3D;.</span><br></pre></td></tr></table></figure>
<p>不执行这句的话，会遇到报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;models&#x2F;tf.py&quot;, line 17, in &lt;module&gt;</span><br><span class="line">    from models.common import Conv, Bottleneck, SPP, DWConv, Focus, BottleneckCSP, Concat, autopad, C3</span><br><span class="line">ModuleNotFoundError: No module named &#39;models&#39;</span><br></pre></td></tr></table></figure>
<ul>
<li>Export TensorFlow models (GraphDef and saved model) using:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python models&#x2F;tf.py --weights weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --img 640</span><br></pre></td></tr></table></figure>
<ul>
<li>Export non-quantized TFLite models using:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --no-tfl-detect</span><br></pre></td></tr></table></figure>
<p>with <code>--no-tfl-detect</code>, the generated TFLite model can be used in the Android project.</p>
<ul>
<li>Export quantized TFLite models using:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --no-tfl-detect --tfl-int8 --source D:\MachineLearning\DataSet\coco\val2017 --ncalib 100</span><br></pre></td></tr></table></figure>
<p>导出的模型位于 <code>--weight</code>的同级目录下。</p>
<p>这句代码我在执行过程中遇到了Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED的报错，参考<a href="https://blog.csdn.net/qq_40394402/article/details/109012083">这里</a>解决了问题，解决方法是在tf.py文件里、import tensorflow as tf后面加上下面代码块里的后两行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">if tf.__version__.startswith(&#39;1&#39;):</span><br><span class="line">    tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line">gpus &#x3D; tf.config.experimental.list_physical_devices(device_type&#x3D;&#39;GPU&#39;)</span><br><span class="line">tf.config.experimental.set_memory_growth(gpus[0], True)</span><br></pre></td></tr></table></figure>
<p>转换过程中输出的模型结构信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Model: &quot;functional_1&quot;</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">input_1 (InputLayer)            [(None, 640, 640, 3) 0</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__focus (tf_Focus)            (None, 320, 320, 32) 3584        input_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_1 (tf_Conv)            (None, 160, 160, 64) 18688       tf__focus[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp (tf_Bottlene (None, 160, 160, 64) 20352       tf__conv_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_6 (tf_Conv)            (None, 80, 80, 128)  74240       tf__bottleneck_csp[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_1 (tf_Bottle (None, 80, 80, 128)  162560      tf__conv_6[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_15 (tf_Conv)           (None, 40, 40, 256)  295936      tf__bottleneck_csp_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_2 (tf_Bottle (None, 40, 40, 256)  644608      tf__conv_15[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_24 (tf_Conv)           (None, 20, 20, 512)  1181696     tf__bottleneck_csp_2[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf_spp (tf_SPP)                 (None, 20, 20, 512)  658432      tf__conv_24[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_3 (tf_Bottle (None, 20, 20, 512)  1252352     tf_spp[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_31 (tf_Conv)           (None, 20, 20, 256)  132096      tf__bottleneck_csp_3[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__upsample (tf_Upsample)      (None, 40, 40, 256)  0           tf__conv_31[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat (tf_Concat)          (None, 40, 40, 512)  0           tf__upsample[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_2[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_4 (tf_Bottle (None, 40, 40, 256)  380416      tf__concat[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_36 (tf_Conv)           (None, 40, 40, 128)  33280       tf__bottleneck_csp_4[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__upsample_1 (tf_Upsample)    (None, 80, 80, 128)  0           tf__conv_36[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat_1 (tf_Concat)        (None, 80, 80, 256)  0           tf__upsample_1[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_5 (tf_Bottle (None, 80, 80, 128)  96000       tf__concat_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_41 (tf_Conv)           (None, 40, 40, 128)  147968      tf__bottleneck_csp_5[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat_2 (tf_Concat)        (None, 40, 40, 256)  0           tf__conv_41[0][0]</span><br><span class="line">                                                                 tf__conv_36[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_6 (tf_Bottle (None, 40, 40, 256)  314880      tf__concat_2[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_46 (tf_Conv)           (None, 20, 20, 256)  590848      tf__bottleneck_csp_6[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat_3 (tf_Concat)        (None, 20, 20, 512)  0           tf__conv_46[0][0]</span><br><span class="line">                                                                 tf__conv_31[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_7 (tf_Bottle (None, 20, 20, 512)  1252352     tf__concat_3[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__detect (tf_Detect)          ((1, 25200, 85), [(N 229245      tf__bottleneck_csp_5[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_6[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_7[0][0]</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 7,489,533</span><br><span class="line">Trainable params: 7,468,157</span><br><span class="line">Non-trainable params: 21,376</span><br></pre></td></tr></table></figure>
<p>2021/3/22更新：今天我再执行以上步骤时，发现Total params等3个参数量变了，模型的输入尺寸也从之前的640变成了现在的320。</p>
<p>因为我们命令行里指定校准的图片的数量是100张，所以可以看到下图的输出，代码在用val2017里的前100张图片来校准：</p>
<p><img src="https://pic.downk.cc/item/5fa955611cd1bbb86b8bcc2a.jpg"></p>
<p>最后成功导出以下模型：</p>
<p><img src="https://pic.downk.cc/item/5fa954c21cd1bbb86b8ba7b6.jpg" width=70%></p>
<p>检测的结果默认输出在inference/output文件夹，来看看检测的效果，可以看出对小目标的检测效果还不错，就是执行时间很长，一张图片推理需要86秒：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\MachineLearning\CV\Object_Detection\yolov5-tf-android&gt;python detect.py --weight weights&#x2F;yolov5s-int8.tflite --tfl-int8 --img 320</span><br><span class="line">Namespace(agnostic_nms&#x3D;False, augment&#x3D;False, cfg&#x3D;&#39;.&#x2F;models&#x2F;yolov5s.yaml&#39;, classes&#x3D;None, conf_thres&#x3D;0.4, device&#x3D;&#39;&#39;, img_size&#x3D;[640], iou_thres&#x3D;0.5, no_tf_nms&#x3D;False, output&#x3D;&#39;inference&#x2F;output&#39;, save_conf&#x3D;False, save_dir&#x3D;&#39;inference&#x2F;output&#39;, save_txt&#x3D;False, source&#x3D;&#39;inference&#x2F;images&#39;, tfl_detect&#x3D;True, tfl_int8&#x3D;True, update&#x3D;False, view_img&#x3D;False, weights&#x3D;[&#39;weights&#x2F;yolov5s-int8.tflite&#39;])</span><br><span class="line">Using CUDA device0 _CudaDeviceProperties(name&#x3D;&#39;GeForce GTX 1660 Ti&#39;, total_memory&#x3D;6144MB)</span><br><span class="line"></span><br><span class="line">2020-11-09 22:05:35.711239: I tensorflow&#x2F;stream_executor&#x2F;platform&#x2F;default&#x2F;dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll</span><br><span class="line">image 1&#x2F;3 C:\MachineLearning\CV\Object_Detection\yolov5-tf-android\inference\images\IMG_20201107_120437_DRO.jpg: 640x640 5 persons, 4 motorcycles, 2 buss, Done. (86.168s)</span><br><span class="line">image 2&#x2F;3 C:\MachineLearning\CV\Object_Detection\yolov5-tf-android\inference\images\bus.jpg: 640x640 4 persons, 1 buss, Done. (86.730s)</span><br><span class="line">image 3&#x2F;3 C:\MachineLearning\CV\Object_Detection\yolov5-tf-android\inference\images\zidane.jpg: 640x640 2 persons, 1 ties, Done. (85.811s)</span><br><span class="line">Results saved to inference\output</span><br><span class="line">Done. (346.666s)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.downk.cc/item/5fa957341cd1bbb86b8c3239.jpg"></p>
<p>TFLite export is only supported under TensorFlow 2.3.0.</p>
<p><code>auto</code> controls whether to pad a resized input image to square. If True, input image is rectangle, otherwise, imput image is square.</p>
<p>TensorFlow does not require square inference, but require fixed input size.<br>Set <code>auto=False</code> to assure the padded image size equals <code>new_shape</code> in <code>letterbox</code>:</p>
<p><img src="https://pic.downk.cc/item/5fa7f2a11cd1bbb86b416a07.jpg"></p>
<p>Thus, the input image sizes after preprocess are the same.</p>
<p>The input size is fixed while exporting TensorFlow and TFLite:</p>
<p><a href="https://github.com/ultralytics/yolov5/blob/23fe35efeb09e366b142e5a4757031f9b208f528/models/tf.py#L394-L395">yolov5/models/tf.py</a></p>
<p>Lines 394 to 395 in <a href="https://github.com/ultralytics/yolov5/commit/23fe35efeb09e366b142e5a4757031f9b208f528">23fe35e</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = keras.Input(shape=(*opt.img_size, <span class="number">3</span>)) </span><br><span class="line">keras_model = keras.Model(inputs=inputs, outputs=tf_model.predict(inputs)) </span><br></pre></td></tr></table></figure>
<p>Take COCO dataset for example, in int8 calibration, if one use <code>auto=True</code>, different size images will be fed to TFLite model<br>while model’s input size is fixed.<br>For rectangle inference of TFLite int8 calibration, one could set <code>--img-size</code> to a rectangle (e.g. 640x320) while setting <code>auto=False</code>.</p>
<p>You could change it to multiples of 32 using <code>--img</code>.<br>For example, use</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --img 320 --no-tfl-detect --tfl-int8 --source &#x2F;data&#x2F;dataset&#x2F;coco&#x2F;coco2017&#x2F;train2017 --ncalib 100</span><br></pre></td></tr></table></figure>
<p>to generate TF and TFLite models.<br>Then, use one of</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python detect.py --weight weights&#x2F;yolov5s.pb --img 320</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s_saved_model&#x2F; --img 320</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s-fp16.tflite --img 320 --tfl-detect</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s-int8.tflite --img 320 --tfl-int8 --tfl-detect</span><br></pre></td></tr></table></figure>
<p>to detect objects.</p>
<p>Or put the TFLite models to <code>asset</code> folder of Android project, and replace</p>
<p><a href="https://github.com/ultralytics/yolov5/blob/eb626a611aba0a83535ff72e9581014be8402a59/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java#L32-L33">yolov5/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java</a></p>
<p>Lines 32 to 33 in <a href="https://github.com/ultralytics/yolov5/commit/eb626a611aba0a83535ff72e9581014be8402a59">eb626a6</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inputSize &#x3D; 640; </span><br><span class="line">output_width &#x3D; new int[]&#123;80, 40, 20&#125;; </span><br></pre></td></tr></table></figure>
<p>and</p>
<p><a href="https://github.com/ultralytics/yolov5/blob/eb626a611aba0a83535ff72e9581014be8402a59/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java#L42-L43">yolov5/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java</a></p>
<p>Lines 42 to 43 in <a href="https://github.com/ultralytics/yolov5/commit/eb626a611aba0a83535ff72e9581014be8402a59">eb626a6</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inputSize &#x3D; 640; </span><br><span class="line">output_width &#x3D; new int[]&#123;80, 40, 20&#125;; </span><br></pre></td></tr></table></figure>
<p>with</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">inputSize &#x3D; 320;</span><br><span class="line">output_width &#x3D; new int[]&#123;40, 20, 10&#125;;</span><br></pre></td></tr></table></figure>
<p>to build and run the Android project.<br>This reduces around</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">75% time of fp16 model inference on Snapdragon 820 CPU (4 threads) from 1.9s to 0.5s, </span><br><span class="line">70%                              on Snapdragon 820 GPU from 1.3s to 0.4s, </span><br><span class="line">70%      of int8                 on Snapdargon 820 CPU (4 threads) from 1.7s to lesser than 0.5s. </span><br></pre></td></tr></table></figure>
<p>It is possible to export to tflite in a shape of the 16:9 aspect ratio like 320 : 192</p>
<p>Run</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --img 320 192 --no-tfl-detect --tfl-int8 --source &#x2F;data&#x2F;dataset&#x2F;coco&#x2F;coco2017&#x2F;train2017 --ncalib 100</span><br></pre></td></tr></table></figure>
<p>to export a TFLite model of 320 vertical by 192 horizontal input , and run one of</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python detect.py --weight weights&#x2F;yolov5s-int8.tflite --img 320 192 --tfl-detect --tfl-int8</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s-fp16.tflite --img 320 192 --tfl-detect</span><br></pre></td></tr></table></figure>
<p>To access current model anchors:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">model &#x3D; torch.load(&#39;yolov5s.pt&#39;)[&#39;model&#39;]</span><br><span class="line">m &#x3D; model.model[-1]  # Detect()</span><br><span class="line">m.anchors  # in stride units</span><br><span class="line">m.anchor_grid  # in pixel units</span><br><span class="line"></span><br><span class="line">print(m.anchor_grid.view(-1,2))</span><br><span class="line">#tensor([[ 10.,  13.],</span><br><span class="line">        [ 16.,  30.],</span><br><span class="line">        [ 33.,  23.],</span><br><span class="line">        [ 30.,  61.],</span><br><span class="line">        [ 62.,  45.],</span><br><span class="line">        [ 59., 119.],</span><br><span class="line">        [116.,  90.],</span><br><span class="line">        [156., 198.],</span><br><span class="line">        [373., 326.]])</span><br></pre></td></tr></table></figure>
<h1 id="YOLOV5-V-S-YOLOV4"><a href="#YOLOV5-V-S-YOLOV4" class="headerlink" title="YOLOV5 V.S. YOLOV4"></a>YOLOV5 V.S. YOLOV4</h1><p>YOLOV4 V.S. YOLOV5 的<a href="https://www.youtube.com/watch?v=H4Wa6QY28e8">对比视频</a>，从视频看，准确率可能相差不明显，但v5的帧率比v4快了很多。</p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>MQTT</title>
    <url>/2020/10/11/MQTT/</url>
    <content><![CDATA[<h1 id="MQTT简介"><a href="#MQTT简介" class="headerlink" title="MQTT简介"></a><strong>MQTT简介</strong></h1><p>MQTT(Message Queuing Telemetry Transport)</p>
<p>MQTT是一个基于TCP/IP协议簇实现的应用层协议（UDP版本的是MQTT-SN），在Mosquitto中底层的通讯是基于socket实现的。当然，你也可以使用Websocket。这里我不做赘述，虽然两者名字类似但就像Java和Javascript一样实际这是两个完全不同的概念。</p>
<p>topic的形式有点像路径，使用/分割，代表层级。</p>
<h2 id="MQTT的服务质量QoS（Quality-of-Service）"><a href="#MQTT的服务质量QoS（Quality-of-Service）" class="headerlink" title="MQTT的服务质量QoS（Quality of Service）"></a>MQTT的服务质量QoS（Quality of Service）</h2><p>MQTT发布消息QoS保证不是 发布端 到 订阅端 的，是客户端（发布端/订阅端）与服务器之间的，即QoS的作用域分为发布端的QoS 和 订阅端的QoS。订阅端收到MQTT消息的QoS级别，最终取决于发布消息的QoS和主题订阅的QoS。简单说，就是取最低的消息服务质量。</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0qHKWF.png" alt="0qHKWF.png" border="0" width=70%/></p>
<p><strong>QoS 0 消息发布订阅</strong></p>
<p>level 0：只发送一次，不保证消息一定送达。</p>
<p><img src=https://docs.emqx.net/broker/latest/cn/development/_assets/qos0_seq.png width=70%></p>
<p><strong>QoS 1 消息发布订阅</strong></p>
<p>level 1：保证信息会被接收端收到，但可能接收端会重复收到消息。<br>发送端会存储发送的publish信息，直到接收端返回puback应答。<br>publish与puback之间通过比较数据包中的packet identifier完成。<br>在特定的时间内（timeout），发送端没有接收到puback应答，那么发送端就会重新发送publish消息。[<a href="http://blog.sina.com.cn/s/blog_a5e78d1d0102wqkr.html">来源</a>]</p>
<p><img src=https://docs.emqx.net/broker/latest/cn/development/_assets/qos1_seq.png width=70%></p>
<p><strong>QoS 2 消息发布订阅</strong></p>
<p>level 2：确保消息只被接收到一次，因频繁确认以至于效率低。</p>
<p><img src=https://docs.emqx.net/broker/latest/cn/development/_assets/qos2_seq.png></p>
<h2 id="MQTT连接保活心跳"><a href="#MQTT连接保活心跳" class="headerlink" title="MQTT连接保活心跳"></a>MQTT连接保活心跳</h2><p>MQTT客户端向服务器发起CONNECT请求时，通过KeepAlive参数设置保活周期。</p>
<p>客户端在无报文发送时，按KeepAlive周期定时发送2字节的PINGREQ心跳报文，服务端收到PINGREQ报文后，回复2字节的PINGRESP报文。</p>
<p>服务端在1.5个心跳周期内，既没有收到客户端发布订阅报文，也没有收到PINGREQ心跳报文时，主动心跳超时断开客户端TCP连接。</p>
<h1 id="MQTT服务端"><a href="#MQTT服务端" class="headerlink" title="MQTT服务端"></a>MQTT服务端</h1><p>MQTT系统中分服务端和客户端，我们先介绍服务端。MQTT的客户端有很多可以选（详细的MQTT Broker 选型请见<a href="https://www.jianshu.com/p/cf91f4bea071">这篇博客</a>。），如果是小项目，用mosquitto即可，如果是有很多客户端，数据传输量大，则可以考虑用国产的EMQ。我们这里主要介绍mosquitto。</p>
<h2 id="MOSQUITTO的安装使用"><a href="#MOSQUITTO的安装使用" class="headerlink" title="MOSQUITTO的安装使用"></a>MOSQUITTO的安装使用</h2><h3 id="将mosquitto安装在树莓派-Linux上"><a href="#将mosquitto安装在树莓派-Linux上" class="headerlink" title="将mosquitto安装在树莓派/Linux上"></a><strong>将mosquitto安装在树莓派/Linux上</strong></h3><p>[<a href="https://www.youtube.com/watch?v=4LnxB_iJ380">教程1</a>]<br>[<a href="https://bitluni.net/simple-mqtt-broker-setup-on-a-raspberry-pi">教程2</a>]</p>
<p>打开树莓派终端，依次输入以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<p>安装mosquitto 和 mosquitto-clients，mosquitto的作用是充当服务端，mosquitto-clients的作用是充当客户端，为了后面方便调试，我们两者都安装，反正包大小也不到。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt install -y mosquitto mosquitto-clients</span><br></pre></td></tr></table></figure><br>设置开机自动启动<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo systemctl enable mosquitto.service</span><br></pre></td></tr></table></figure><br>测试创建发布者<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mosquitto_pub -t home_automation&#x2F;bedroom&#x2F;light -m &#39;helloWorld&#39;</span><br></pre></td></tr></table></figure><br>打开另一个终端，测试创建订阅者<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mosquitto_sub  -v -t home_automation&#x2F;bedroom&#x2F;light</span><br></pre></td></tr></table></figure></p>
<p>你还可以在windows上跟树莓派通过MQTT通信，<br>首先在windows上安装WSL或Debian GNU Linux，然后也是执行<code>sudo apt-get update</code>  和 <code>sudo apt install -y mosquitto mosquitto-clients</code></p>
<p>最后在<code>mosquitto_pub -t homeautomation/bedroom/light -m &#39;hello from my laptop&#39;</code>里面加入 <code>-h IP</code>， IP就是树莓派的局域网IP地址.</p>
<p>获取树莓派IP。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostname -I</span><br></pre></td></tr></table></figure></p>
<p>在windows电脑上，除了在linux子系统安装mosquitto-clients可以用来模拟一个客户端，还可以通过安装桌面软件来模拟一个客户端。这类桌面软件有<a href="http://mqtt-explorer.com/">MQTT explorer</a>、<a href="https://github.com/emqx/MQTTX/blob/master/README-CN.md">MQTTX</a>，我用过MQTTX，挺不错的，推荐使用。</p>
<p><img src=https://s1.ax1x.com/2020/10/20/BpvDTx.png></p>
<h3 id="将mosquitto安装在windows电脑上"><a href="#将mosquitto安装在windows电脑上" class="headerlink" title="将mosquitto安装在windows电脑上"></a><strong>将mosquitto安装在windows电脑上</strong></h3><p><a href="https://mosquitto.org/download/"><strong>mosquitto下载地址</strong></a></p>
<p>下载安装包：</p>
<p><img src="https://pic.downk.cc/item/5f8adb131cd1bbb86b10c1fa.jpg"></p>
<p>搜索“服务”：</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0LLMgU.png" alt="0LLMgU.png" border="0" /></p>
<p>点击启动此服务：</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0LLrbd.png" alt="0LLrbd.png" border="0" /></p>
<p>启动服务后的样子：</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0LL7an.png" alt="0LL7an.png" border="0" /></p>
<p>这时候你再用MQTT传输大的图片（服务器记得改成’localhost’），速度就很快了。</p>
<h3 id="mosquitto-pub-命令参数说明"><a href="#mosquitto-pub-命令参数说明" class="headerlink" title="mosquitto_pub 命令参数说明"></a><strong>mosquitto_pub 命令参数说明</strong></h3><ol>
<li><p>-d  打印debug信息</p>
</li>
<li><p>-f  将指定文件的内容作为发送消息的内容</p>
</li>
<li><p>-h  指定要连接的域名  默认为localhost</p>
</li>
<li><p>-i  指定要给哪个clientId的用户发送消息</p>
</li>
<li><p>-I  指定给哪个clientId前缀的用户发送消息</p>
</li>
<li><p>-m  消息内容</p>
</li>
<li><p>-n  发送一个空（null）消息</p>
</li>
<li><p>-p  连接端口号</p>
</li>
<li><p>-q  指定QoS的值（0,1,2）</p>
</li>
<li><p>-t  指定topic</p>
</li>
<li><p>-u  指定broker访问用户</p>
</li>
<li><p>-P  指定broker访问密码</p>
</li>
<li><p>-V  指定MQTT协议版本</p>
</li>
<li><p>—will-payload  指定一个消息，该消息当客户端与broker意外断开连接时发出。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-qos  Will的QoS值。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-retain 指定Will消息被当做一个retain消息（即消息被广播后，该消息被保留起来）。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-topic  用户发送Will消息的topic</p>
</li>
</ol>
<h3 id="mosquitto-sub-命令参数说明"><a href="#mosquitto-sub-命令参数说明" class="headerlink" title="mosquitto_sub 命令参数说明"></a><strong>mosquitto_sub 命令参数说明</strong></h3><ol>
<li><p>-c  设定‘clean session’为无效状态，这样一直保持订阅状态，即便是已经失去连接，如果再次连接仍旧能够接收的断开期间发送的消息。</p>
</li>
<li><p>-d  打印debug信息</p>
</li>
<li><p>-h  指定要连接的域名  默认为localhost</p>
</li>
<li><p>-i 指定clientId</p>
</li>
<li><p>-I 指定clientId前缀</p>
</li>
<li><p>-k keepalive 每隔一段时间，发PING消息通知broker，仍处于连接状态。 默认为60秒。</p>
</li>
<li><p>-q 指定希望接收到QoS为什么的消息  默认QoS为0</p>
</li>
<li><p>-R 不显示陈旧的消息</p>
</li>
<li><p>-t 订阅topic</p>
</li>
<li><p>-v 打印消息</p>
</li>
<li><p>—will-payload  指定一个消息，该消息当客户端与broker意外断开连接时发出。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-qos  Will的QoS值。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-retain 指定Will消息被当做一个retain消息（即消息被广播后，该消息被保留起来）。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-topic  用户发送Will消息的topic</p>
</li>
</ol>
<h2 id="使用密码和SSL给MQTT加密"><a href="#使用密码和SSL给MQTT加密" class="headerlink" title="使用密码和SSL给MQTT加密"></a>使用密码和SSL给MQTT加密</h2><p>给mosquitto broker设置密码，输入以下命令后<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mosquitto_passwd -c passwordfile username</span><br></pre></td></tr></table></figure></p>
<p>用户名username自己取一个，回车后命令行窗口会让你设置密码。</p>
<h2 id="另一个MQTT-服务器-EMQ-X"><a href="#另一个MQTT-服务器-EMQ-X" class="headerlink" title="另一个MQTT 服务器 EMQ X"></a>另一个MQTT 服务器 EMQ X</h2><p>除了mosquitto，我们还可以使用EMQ X来作为我们的服务端。使用方法是使用docker安装EMQX，然后打开浏览器，输入，既可以看到服务器的可视化数据面板了，这一点比无可视化界面的mosquitto好。</p>
<p><a href="https://docs.emqx.net/broker/latest/cn/">EMQ X的官方文档</a> </p>
<h1 id="免费公共-MQTT-服务器"><a href="#免费公共-MQTT-服务器" class="headerlink" title="免费公共 MQTT 服务器"></a><strong>免费公共 MQTT 服务器</strong></h1><p>除了上面说的在自己设备上部署MQTT服务器，你还可以使用公共的MQTT服务器，这是一些相关公司免费提供给大家用来测试用的服务器，推荐以下三个免费公共 MQTT 服务器：</p>
<hr>
<ul>
<li>Broker: <strong>mqtt.eclipse.org</strong></li>
<li>1883 : MQTT over unencrypted TCP</li>
<li>8883 : MQTT over encrypted TCP</li>
<li>80 : MQTT over unencrypted WebSockets (note: URL must be /mqtt )</li>
<li>443 : MQTT over encrypted WebSockets (note: URL must be /mqtt )</li>
</ul>
<p>推荐指数：⭐⭐⭐，速度快，端口多</p>
<hr>
<ul>
<li>Broker: <strong>broker.hivemq.com</strong></li>
<li>TCP Port: 1883</li>
<li>Websocket Port: 8000</li>
</ul>
<p>推荐指数：⭐⭐，速度还行</p>
<hr>
<ul>
<li>Broker: <strong>broker.emqx.io</strong></li>
<li>TCP Port: 1883</li>
<li>Websocket Port: 8083</li>
</ul>
<p>推荐指数：⭐，速度慢，有时发送图片还会收不到</p>
<h1 id="MQTT客户端"><a href="#MQTT客户端" class="headerlink" title="MQTT客户端"></a><strong>MQTT客户端</strong></h1><p>如果实际应用中我们通过mosquitto_sub和mosquitto_pub的方式来订阅和发送，一个是每次都要输入麻烦，一个是无法执行复杂逻辑的代码，显然是不切实际的，于是就有了paho-mqtt，这个python包让你用python语言编写MQTT客户端的代码。安装很简单：<code>pip install paho-mqtt</code>。所以，mosquitto_sub和mosquitto_pub只是用来测试，实际生产还是用paho-mqtt来写订阅和发送的代码。</p>
<p>早期版本的paho-mqtt只有下图3个模块，这也代表了它最本质的功能。</p>
<p><img src=https://s1.ax1x.com/2020/10/20/BpzBi6.png></p>
<h2 id="paho-mqtt的基本使用"><a href="#paho-mqtt的基本使用" class="headerlink" title="paho-mqtt的基本使用"></a>paho-mqtt的基本使用</h2><p><strong>导入模块</strong>：<code>Import paho.mqtt.client as mqtt</code></p>
<p><strong>创建客户端实例</strong>：<br><code>client =mqtt.Client(client_name)</code></p>
<p>Client()的定义：<code>Client(client_id=&quot;&quot;, clean_session=True, userdata=None, protocol=MQTTv311, transport=&quot;tcp&quot;)</code>，但我们其实只要输入第一个client id就可以了,甚至我们不输入，系统也会自己为我们创建名称，当然实际使用中，有多个客户端，为了方便区分，还是自己定义一套命名规则比较好。而且万一id名重复了，服务器只会选一个连接。更多参数讲解，请见<a href="http://www.steves-internet-guide.com/client-objects-python-mqtt/">这里</a>。</p>
<p><strong>连接到服务器 :</strong> <code>client.connect(HostIPAddress)</code></p>
<p>connect()方法的定义：<code>connect(host, port=1883, keepalive=60, bind_address=&quot;&quot;)</code>，我们可以保持它们默认，只输入服务器的ip地址就可以了。</p>
<p><strong>发布消息：</strong><code>client.publish(&quot;topic&quot;,&quot;message&quot;)</code></p>
<p>publish()方法的定义：<code>publish(topic, payload=None, qos=0, retain=False)</code>，一般我们是填写topic和message/payload两者就行了。</p>
<p><strong>订阅消息：</strong><code>client.subscribe(&quot;topic&quot;)</code><br>subscribe()方法的定义：<code>subscribe(topic, qos=0)</code>,一般我们只填写topic。</p>
<p><strong>接收信息并处理：</strong></p>
<p>订阅号主题还无法接受消息，还需要靠下面这个回调函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_message</span>(<span class="params">client, userdata, message</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message received &quot;</span> ,<span class="built_in">str</span>(message.payload.decode(<span class="string">&quot;utf-8&quot;</span>)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message topic=&quot;</span>,message.topic)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message qos=&quot;</span>,message.qos)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message retain flag=&quot;</span>,message.retain)</span><br></pre></td></tr></table></figure>
<p>使用<code>client.on_message=on_message</code>把客户端绑定到上面的回调函数。</p>
<p><strong>使用Logging分析问题：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_log</span>(<span class="params">client, userdata, level, buf</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;log: &quot;</span>,buf)</span><br></pre></td></tr></table></figure>
<p>然后再使用<code>client.on_log=on_log</code>把客户端绑定到上面的回调函数。</p>
<p><strong>开始工作</strong></p>
<p>我们可以使用<code>client.loop_start()</code>来使客户端开始不断地订阅或者发布消息，最后可以使用<code>client.stop_start()</code>使其停止工作。或者你想偷懒，使用<code>client.loop_forever
()</code>使客户端一直工作下去。</p>
<p><strong>WebSocket连接</strong></p>
<p>MQTT协议除支持TCP传输层外，还支持WebSocket作为传输层。通过WebSocket浏览器可以直连MQTT消息服务器。</p>
<p>为了告诉客户端使用websockets，要像以下代码一样申明，并且修改websockets专门的端口号</p>
<p><code>client = mqtt.Client(transport=&quot;websockets&quot;)</code></p>
<h1 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h1><h2 id="0-传输文本"><a href="#0-传输文本" class="headerlink" title="0.传输文本"></a>0.传输文本</h2><h2 id="1-传输图片"><a href="#1-传输图片" class="headerlink" title="1.传输图片"></a>1.传输图片</h2><p>除了最基本的，用MQTT传输文本字符信息外，我们还可以用MQTT传输图片，文件结构如下，文件夹下有测试用的不同大小的图片和接收到的图片：</p>
<p><img src="https://pic.downk.cc/item/5f8adc811cd1bbb86b112de4.jpg"></p>
<p>使用上面介绍的几个免费公共 MQTT 服务器传输图片速度会比较慢，程序没有问题，只是等待订阅端收到图片的时间会比较长。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sub.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">MQTT_SERVER = <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;Image&quot;</span></span><br><span class="line">image_format = <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The callback for when the client receives a CONNACK response from the server.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_connect</span>(<span class="params">client, userdata, flags, rc</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connected with result code &quot;</span>+<span class="built_in">str</span>(rc))</span><br><span class="line">    <span class="comment"># 订阅建议放在 on_connect 里，因为如果与 broker 失去连接后重连，仍然会继续订阅该主题</span></span><br><span class="line">    client.subscribe(MQTT_TOPIC)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回调函数，当收到消息时，触发该函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_message</span>(<span class="params">client, userdata, msg</span>):</span></span><br><span class="line">    <span class="comment"># Create a file with write byte permission</span></span><br><span class="line">    start_time = time.strftime(<span class="string">&quot;%Y-%m-%d %H-%M-%S&quot;</span>, time.localtime())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Received Image at &#123;&#125;&quot;</span>.<span class="built_in">format</span>(start_time))</span><br><span class="line">    f = <span class="built_in">open</span>(start_time + image_format, <span class="string">&quot;wb&quot;</span>)</span><br><span class="line">    f.write(msg.payload)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">client = mqtt.Client()</span><br><span class="line">client.on_connect = on_connect</span><br><span class="line">client.on_message = on_message</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置遗嘱消息，当树莓派断电，或者网络出现异常中断时，发送遗嘱消息给其他客户端</span></span><br><span class="line">client.will_set(MQTT_TOPIC,  <span class="string">b&#x27;&#123;&quot;status&quot;: &quot;Off&quot;&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果broker设置了账号和密码，这里我们需要填写账号密码才能连上</span></span><br><span class="line"><span class="comment"># client.username_pw_set(username=&quot;username&quot;,password=&quot;password&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建连接，三个参数分别为 broker 地址，broker 端口号，保活时间</span></span><br><span class="line">client.connect(MQTT_SERVER, <span class="number">1883</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置网络循环堵塞，在调用 disconnect() 或程序崩溃前，不会主动结束程序</span></span><br><span class="line">client.loop_forever()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pub.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MQTT_SERVER = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;Image&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回调函数。当与 MQTT broker 建立连接时，触发该函数。</span></span><br><span class="line"><span class="comment"># rc 是响应码。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_connect</span>(<span class="params">client, userdata, flags, rc</span>):</span></span><br><span class="line">    <span class="keyword">if</span> rc == <span class="number">0</span>:</span><br><span class="line">        client.connected_flag = <span class="literal">True</span>  <span class="comment"># set flag</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;connected OK Returned code =&quot;</span>, rc)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Bad connection Returned code=&quot;</span>,rc)</span><br><span class="line">        client.bad_connection_flag = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client = mqtt.Client()</span><br><span class="line">client.connected_flag = <span class="literal">False</span></span><br><span class="line">client.bad_connection_flag = <span class="literal">False</span></span><br><span class="line">client.on_connect = on_connect  <span class="comment"># bind call back function</span></span><br><span class="line">client.loop_start()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Connecting to broker &quot;</span>, MQTT_SERVER)</span><br><span class="line">client.connect(MQTT_SERVER, <span class="number">1883</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> client.connected_flag:  <span class="comment"># wait in loop</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;In wait loop : &#123;&#125; seconds&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    i = i+<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> client.bad_connection_flag:</span><br><span class="line">    client.loop_stop()    <span class="comment"># Stop loop</span></span><br><span class="line">    sys.exit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> client.connected_flag:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;正常连接后开始做正事</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&quot;2086.jpg&quot;</span>, <span class="string">&quot;rb&quot;</span>)</span><br><span class="line">    fileContent = f.read()</span><br><span class="line">    byteArr = <span class="built_in">bytearray</span>(fileContent)</span><br><span class="line">    <span class="built_in">print</span>(byteArr)</span><br><span class="line"></span><br><span class="line">client.publish(MQTT_TOPIC, payload=byteArr, qos=<span class="number">0</span>, retain=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;send out&quot;</span>)</span><br><span class="line"></span><br><span class="line">client.loop_forever()</span><br><span class="line"></span><br><span class="line"><span class="comment"># client.disconnect()</span></span><br><span class="line"><span class="comment"># client.loop_stop()</span></span><br></pre></td></tr></table></figure>
<p>我们在 on_connect 函数里对响应码进行了判断，为 0 则输出 Connected success 表示连接成功。如果返回的是其它数字，我们就需要对照下面的响应码进行判断。</p>
<ul>
<li>0: 连接成功</li>
<li>1: 连接失败-不正确的协议版本</li>
<li>2: 连接失败-无效的客户端标识符</li>
<li>3: 连接失败-服务器不可用</li>
<li>4: 连接失败-错误的用户名或密码</li>
<li>5: 连接失败-未授权</li>
</ul>
<h2 id="传输视频"><a href="#传输视频" class="headerlink" title="传输视频"></a>传输视频</h2><p>我们先看看用MQTT来在局域网内传输视频流的效果怎么样。</p>
<p>这篇文章<a href="https://medium.com/@pritam.mondal.0711/stream-live-video-from-client-to-server-using-opencv-and-paho-mqtt-674d3327e8b3">《Stream live video from client to server using OpenCV and Paho-MQTT》</a>给的代码很好用，复制过来后改一下Broker的地址就可以马上跑起来了，用windows电脑localhost来做broker传输速度可以，不卡顿，但用公共免费broker就会严重卡顿了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sender.py</span></span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Raspberry PI IP address</span></span><br><span class="line">MQTT_BROKER = <span class="string">&quot;192.168.x.x&quot;</span></span><br><span class="line"><span class="comment"># Topic on which frame will be published</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;home/server&quot;</span></span><br><span class="line"><span class="comment"># Object to capture the frames</span></span><br><span class="line">cap = cv.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Phao-MQTT Clinet</span></span><br><span class="line">client = mqtt.Client()</span><br><span class="line"><span class="comment"># Establishing Connection with the Broker</span></span><br><span class="line">client.connect(MQTT_BROKER)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="comment"># Read Frame</span></span><br><span class="line">        _, frame = cap.read()</span><br><span class="line">        <span class="comment"># Encoding the Frame</span></span><br><span class="line">        _, buffer = cv.imencode(<span class="string">&#x27;.jpg&#x27;</span>, frame)</span><br><span class="line">        <span class="comment"># Converting into encoded bytes</span></span><br><span class="line">        jpg_as_text = base64.b64encode(buffer)</span><br><span class="line">        <span class="comment"># Publishig the Frame on the Topic home/server</span></span><br><span class="line">        client.publish(MQTT_TOPIC, jpg_as_text)</span><br><span class="line">        end = time.time()</span><br><span class="line">        t = end - start</span><br><span class="line">        fps = <span class="number">1</span> / t</span><br><span class="line">        <span class="built_in">print</span>(fps)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    cap.release()</span><br><span class="line">    client.disconnect()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nNow you can restart fresh&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># receiver.py</span></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"></span><br><span class="line">MQTT_BROKER = <span class="string">&quot;192.168.x.x&quot;</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;home/server&quot;</span></span><br><span class="line"></span><br><span class="line">frame = np.zeros((<span class="number">240</span>, <span class="number">320</span>, <span class="number">3</span>), np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The callback for when the client receives a CONNACK response from the server.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_connect</span>(<span class="params">client, userdata, flags, rc</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connected with result code &quot;</span>+<span class="built_in">str</span>(rc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Subscribing in on_connect() means that if we lose the connection and</span></span><br><span class="line">    <span class="comment"># reconnect then subscriptions will be renewed.</span></span><br><span class="line">    client.subscribe(MQTT_TOPIC)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The callback for when a PUBLISH message is received from the server.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_message</span>(<span class="params">client, userdata, msg</span>):</span></span><br><span class="line">    <span class="keyword">global</span> frame</span><br><span class="line">    <span class="comment"># Decoding the message</span></span><br><span class="line">    img = base64.b64decode(msg.payload)</span><br><span class="line">    <span class="comment"># converting into numpy array from buffer</span></span><br><span class="line">    npimg = np.frombuffer(img, dtype=np.uint8)</span><br><span class="line">    <span class="comment"># Decode to Original Frame</span></span><br><span class="line">    frame = cv.imdecode(npimg, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client = mqtt.Client()</span><br><span class="line">client.on_connect = on_connect</span><br><span class="line">client.on_message = on_message</span><br><span class="line"></span><br><span class="line">client.connect(MQTT_BROKER)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting thread which will receive the frames</span></span><br><span class="line">client.loop_start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    cv.imshow(<span class="string">&quot;Stream&quot;</span>, frame)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stop the Thread</span></span><br><span class="line">client.loop_stop()</span><br></pre></td></tr></table></figure>
<p>但正如<a href="https://stackoverflow.com/questions/53385855/python-mqtt-improving-publish-speed-for-image-numpy-array">这里所说</a>，MQTT适用于低带宽和不可靠的连接，而像实时视频传输这种需求，MQTT可能就不适合了，这种需求对应的解决方案有：RTSP、FFMPEG、GStreamer、ZeroZMQ，但是下面我们讲介绍的是ImageZMQ。</p>
<p>ImageZMQ和MQTT名字中都包含MQ，两者的含义都是一样的，都是Message Queuing的意思，都是采用客户端和中间代理服务器的工作模式。ImageZMQ是下图右侧人物在下图左侧人物创办的PyImageSearch Gurus学习课程中开发出来的，并在<a href="https://www.pyimageconf.com/">PyImageConf</a> 2018上演讲介绍，ppt slide在<a href="https://www.pyimageconf.com/static/talks/jeff_bass.pdf">这里</a>。ZeroMQ是专门针对高吞吐量和低延迟开发的应用程序，而ImageZMQ库基于ZeroMQ被设计用于通过网络高效地传输视频流。ImageZMQ是一个Python软件包，与OpenCV完美集成。</p>
<p>【<a href="https://github.com/jeffbass/imagezmq">ImageZMQ Github 地址</a>】<br>【<a href="https://www.pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/">ImageZMQ 介绍博客</a>】</p>
<center class="half">
    <img src="https://s1.ax1x.com/2020/10/19/0zaG5R.png" width="285"/>
    <img src="https://s1.ax1x.com/2020/10/19/0za0qe.png" width="280"/>
</center>

<p><img src=https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/imagezmq-opencv/imagezmq_demo.gif width=130%></p>
<h2 id="其他综合项目"><a href="#其他综合项目" class="headerlink" title="其他综合项目"></a>其他综合项目</h2><p>《<a href="https://dzone.com/articles/opencv-and-esp32-moving-a-servo-with-my-face">OpenCV and ESP32: Moving a Servo With My Face</a>》本博客介绍的项目是用ESP32和私服电机作为客户端，订阅电脑那边发出的‘/servo’主题，根据人脸位置转动私服电机。文中的MQTT代码是用C++写的，运行在ESP32上面。</p>
<p>《<a href="https://www.instructables.com/Smart-JPEG-Camera-for-Home-Security/">A Smart JPEG Camera for Home Security</a>》这篇博客介绍了在树莓派上使用Node-Red编程，使其作为MQTT客户端，通过wifi dongle（就是USB插口的无线网卡）发送摄像头和<a href="https://learn.adafruit.com/pir-passive-infrared-proximity-motion-sensor/how-pirs-work?view=all">人体热释红外传感器</a>的数据给电脑端的服务器。文章是2016年写的，所以文中的python-mosquitto就是今天的paho。</p>
<p>这里提一下Node-Red，这是一个基本流的可视化编程工具，运行在浏览器上，现在已经被默认搭载在各种物联网设备上（比如树莓派），它的工作原理<a href="https://nodered.org/about/">官网</a>是这么介绍的：</p>
<blockquote>
<p><strong>Runtime/Editor</strong><br><br>Node-RED consists of a Node.js based runtime that you point a web browser at to access the flow editor. Within the browser you create your application by dragging nodes from your palette into a workspace and start to wire them together. With a single click, the application is deployed back to the runtime where it is run.<br><br>The palette of nodes can be easily extended by installing new nodes created by the community and the flows you create can be easily shared as JSON files.</p>
</blockquote>
<p>一句话概括就是Node-Red会把你编写的流程转换成程序代码（json格式）交给Node.js环境运行。它强大的生命力有一点是因为已有大量的<a href="https://flows.nodered.org/">Node、Flow、Collection</a>供人下载。国外有个专门指导NodeRed教学的网站<a href="http://noderedguide.com/">noderedguide</a>可以看一下。</p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p>推荐一位国外MQTT领域的专家 - Steve 的博客，<a href="http://www.steves-internet-guide.com/">他的博客</a>基本就是专门讲MQTT这块的（如下图所示）</p>
<p><img src="https://pic.downk.cc/item/5f8b13631cd1bbb86b1e873d.jpg"></p>
<p>这里就只列举几篇比较进阶的文章：</p>
<p><a href="http://www.steves-internet-guide.com/multiple-client-connections-python-mqtt/">《Handling Multiple Client Connections-Python MQTT》</a></p>
<p><a href="http://www.steves-internet-guide.com/mqtt-websockets/">《Using MQTT Over WebSockets with Mosquitto》</a></p>
<p>另外再推荐一下EMQ网站上的一个<a href="https://www.emqx.io/cn/blog">博客集</a>，里面收集了很多博主编写的优质文章。</p>
]]></content>
      <categories>
        <category>物联网</category>
      </categories>
      <tags>
        <tag>IOT</tag>
        <tag>MQTT</tag>
        <tag>Raspberry Pi</tag>
      </tags>
  </entry>
  <entry>
    <title>图像处理算法</title>
    <url>/2020/10/05/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h1><p>SIFT(Scale-Invariant Feature Transform，尺度不变特征转换)在目标识别、图像配准领域具有广泛的应用。</p>
<p>先对原始图像进行扩大一倍的处理（用双线性插值）</p>
<p>在SIFT算法中，采用的是高斯差分金字塔（DOG），具体为什么是高斯差分金字塔不知道，只知道大牛们通过实验得出：尺度归一化的高斯拉普拉斯算子能够得到最稳定的图像特征，但因为计算量太大，而高斯差分函数与高斯拉普拉斯算子很相似，所以通过高斯差分函数来近似的计算图像最稳定的特征。</p>
<p>下面看一个简单的高斯差分<a href="https://blog.csdn.net/qq_35239859/article/details/105160095">例子</a>：</p>
<p>第一步，先求出不同$\sigma$下的高斯滤波输出，并求出它们的DOG图像,如下图所示</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">original - $\sigma_1$</th>
<th style="text-align:center">$\sigma_1$-$\sigma_2$</th>
<th style="text-align:center">$\sigma_2$$\sigma_3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://pic.downk.cc/item/5f7b2ecb160a154a67ba72ef.jpg"></td>
<td style="text-align:center"><img src="https://pic.downk.cc/item/5f7b2eed160a154a67ba7d52.jpg"></td>
<td style="text-align:center"><img src="https://pic.downk.cc/item/5f7b2f13160a154a67ba87c9.jpg"></td>
</tr>
</tbody>
</table>
</div>
<p>第二步，根据DOG,求角点。</p>
<p>三维图中的最大值和最小值点是角点。如图所示，标记当前像素点为红色，领域像素点标记为黄，可以看出，领域的黄色像素点一共有3×9-1=26个。如果它是所有邻接像素点的最大值或最小值点，则该红色像素点被标记为特征点，如此依次进行，则可以完成图像的特征点提取。</p>
<p><img src="https://pic.downk.cc/item/5f7b2f89160a154a67baa70a.jpg"></p>
<p>于是我们计算出三个DOG图中是极值的点，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f7b304b160a154a67bae034.jpg"></p>
<p>黑色为极小值，白色为极大值</p>
<p>因此，原始图像上显示的DOG角点检测结果，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f7b3076160a154a67baeec1.jpg"></p>
<p>在SIFT算法中，</p>
<p><img src=https://www.researchgate.net/profile/Liang-Bi_Chen/publication/325264930/figure/fig2/AS:655137697697799@1533208407234/Difference-of-Gaussian-DOG-Difference-of-Gaussian-DOG.png></p>
<p><a href="https://www.researchgate.net/figure/Difference-of-Gaussian-DOG-Difference-of-Gaussian-DOG_fig2_325264930">[图片来源]</a></p>
<p><strong>DOG的定义</strong></p>
<p>高斯差（英语：Difference of Gaussians，简称“DOG”）就是将两个不同高斯模糊程度的图像相减的算法。我们知道高斯模糊用的卷积核里面的数值满足正态分布/高斯分布，其二维形式的公式为：</p>
<script type="math/tex; mode=display">G(x, y, \sigma)=\frac{1}{2 \pi \sigma^{2}} e^{-\left(x^{2}+y^{2}\right) / 2 \sigma^{2}}</script><p>那么高斯差二维形式的公式为：</p>
<script type="math/tex; mode=display">G(x, y, \sigma)=\frac{1}{2 \pi \sigma^{2}} \exp ^{-\left(x^{2}+y^{2}\right) /\left(2 \sigma^{2}\right)}-\frac{1}{2 \pi K^{2} \sigma^{2}} \exp ^{-\left(x^{2}+y^{2}\right) /\left(2 K^{2} \sigma^{2}\right)}</script><p><img src=https://www.olympus-lifescience.com.cn/data/olympusmicro/primer/java/digitalimaging/processing/diffgaussians/diffgaussiansfigure1.jpg></p>
<p><a href="https://www.olympus-lifescience.com.cn/zh/microscope-resource/primer/java/digitalimaging/processing/diffgaussians/">[图片来源]</a></p>
<p>由于高斯差是两个不同的低通（low-pass）滤波图像之间的差异，DoG实际上是一个带通（band-pass）滤波器，它去除了代表噪声的高频分量，也去除了代表图像中均匀区域的一些低频分量。我们认为通带中的频率分量与图像中的边缘相关联。</p>
<p><strong>DOG的应用</strong></p>
<p>作为一个增强算法，DOG可以被用来增加边缘和其他细节的可见性，大部分的边缘锐化算子使用增强高频信号的方法，但是因为随机噪声也是高频信号，很多锐化算子也增强了噪声。DOG算法去除的高频信号中通常包含了随机噪声，所以这种方法是最适合处理那些有高频噪声的图像。这个算法的一个主要缺点就是在调整图像对比度的过程中信息量会减少。</p>
<p>当它被用于图像增强时，DOG算法中两个高斯核的半径之比通常为4:1或5:1。当设为1.6时，即为高斯拉普拉斯算子的近似。高斯拉普拉斯算子在多尺度多分辨率像片。用于近似高斯拉普拉斯算子两个高斯核的确切大小决定了两个高斯模糊后的影像间的尺度。</p>
<p>DOG也被用于尺度不变特征变换中的斑点检测。事实上，DOG算法作为两个多元正态分布的差通常总额为零，把它和一个恒定信号进行卷积没有意义。当K约等于1.6时它很好的近似了高斯拉普拉斯变换，当K约等于5时又很好的近似了视网膜上神经节细胞的视野。</p>
<p><a href="http://www.360doc.com/content/19/0709/12/32196507_847629007.shtml">http://www.360doc.com/content/19/0709/12/32196507_847629007.shtml</a><br><a href="https://www.cnblogs.com/ronny/p/4028776.html">https://www.cnblogs.com/ronny/p/4028776.html</a><br><a href="https://blog.csdn.net/zddblog/article/details/7521424">https://blog.csdn.net/zddblog/article/details/7521424</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
  </entry>
  <entry>
    <title>YOLOV4</title>
    <url>/2020/10/04/YOLOV4/</url>
    <content><![CDATA[<p>推荐阅读江大白的《<a href="https://zhuanlan.zhihu.com/p/143747206">深入浅出Yolo系列之Yolov3&amp;Yolov4核心基础知识完整讲解</a>》，我是在写了本篇博文很长时间后才阅读到他这篇宝藏博客的，由此引发了我的一个感想：我感觉我们不需要关注很多技术博主公众号，只需要关注几个优质的博主就行了，一般他们的作品都会比较优质。</p>
<h1 id="darknet在本地电脑的安装"><a href="#darknet在本地电脑的安装" class="headerlink" title="darknet在本地电脑的安装"></a>darknet在本地电脑的安装</h1><p>详细的过程请参照The AI Guy的<a href="https://www.youtube.com/watch?v=saDipJR14Lc&amp;t=992s">教学视频</a>，我这里只写一些重点的步骤。</p>
<p>yolo默认是使用darknet框架，darknet是一个类似tensorflow的框架，不过它的安装方式跟tensorflow不太一样。</p>
<p>linux版比较简单,可参考<a href="https://blog.csdn.net/Eric_Fisher/article/details/89884108">这篇博客</a>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;AlexeyAB&#x2F;darknet.git</span><br><span class="line">&#x2F;&#x2F;或者使用更快的镜像：https:&#x2F;&#x2F;gitee.com&#x2F;wwdok&#x2F;darknet.git</span><br><span class="line">cd darknet</span><br></pre></td></tr></table></figure>
<p>修改Makefile，添加对GPU，CUDNN，OpenCV等的支持。<br>如果你的CUDA没有使用默认的路径，请进行修改：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi Makefile</span><br></pre></td></tr></table></figure>
<p><img src=https://images3.pianshen.com/576/00/00e1bc11525ead2915af54e786d28c60.png></p>
<p>接下来,编译</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure>
<p>如果都已正确编译，请尝试运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;darknet</span><br></pre></td></tr></table></figure>
<p>应该得到输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">usage: .&#x2F;darknet &lt;function&gt;</span><br></pre></td></tr></table></figure>
<p>windows上的安装比较复杂一点：</p>
<p>首先用记事本打开darknet\build\darknet\darknet.vcxproj，确保里面的CUDA版本对应你电脑CUDA的版本，一共有两处需要修改，如图：</p>
<p><img src="https://pic.downk.cc/item/5f79b659160a154a6762986b.jpg"><br><img src="https://pic.downk.cc/item/5f79b671160a154a67629d51.jpg"></p>
<p>我还看到darknet.vcxproj里有 $(OPENCV_DIR) 和 $(CUDA_PATH) ，所以我还自己去设置了这两个环境变量，以防后面出什么差错，说明一下，opencv430的文件夹是从Windows下载，不是从Sources下载：</p>
<p><img src="https://pic.downk.cc/item/5f79b6eb160a154a6762bb31.jpg"><br><br><br><img src="https://pic.downk.cc/item/5f79b87a160a154a6763221f.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f79d0d6160a154a67694ebf.jpg"></p>
<p>接下来打开CMake，像下图配置两个路径，点击<strong>Configure</strong>后会报错显红，我按照视频的教程来配置了下图黄框两处的变量：</p>
<p><img src="https://pic.downk.cc/item/5f799a45160a154a6755d70a.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f799b36160a154a67560ac9.jpg" ></p>
<p>但配置好后再点击<strong>Configure</strong>e之后仍然报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CMake Error at C:&#x2F;Program Files&#x2F;CMake&#x2F;share&#x2F;cmake-3.17&#x2F;Modules&#x2F;CMakeDetermineCompilerId.cmake:367 (message):</span><br><span class="line">  No CUDA toolset found.</span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  C:&#x2F;Program Files&#x2F;CMake&#x2F;share&#x2F;cmake-3.17&#x2F;Modules&#x2F;CMakeDetermineCompilerId.cmake:32 (CMAKE_DETERMINE_COMPILER_ID_BUILD)</span><br><span class="line">  C:&#x2F;Program Files&#x2F;CMake&#x2F;share&#x2F;cmake-3.17&#x2F;Modules&#x2F;CMakeDetermineCUDACompiler.cmake:72 (CMAKE_DETERMINE_COMPILER_ID)</span><br><span class="line">  CMakeLists.txt:66 (enable_language)</span><br></pre></td></tr></table></figure>
<p>解决办法是将<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\visual_studio_integration\MSBuildExtensions</code>里面的4个文件全部拷贝到<code>C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Microsoft\VC\v160\BuildCustomizations</code>。</p>
<p><img src="https://pic.downk.cc/item/5f79ac97160a154a675fc9b0.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f79ad3d160a154a676062ea.jpg"></p>
<p>再次点击<strong>Configure</strong>，已经没有上面那个报错了，但CMake仍然一片红。这时候再点开ENABLE，取消ENABLE_CUDNN_HALF的勾选，再点击configure后CMake就变成一片白色了，成功：<br><img src="https://pic.downk.cc/item/5f79b2ea160a154a6761c47e.jpg"></p>
<p>接下来点击<strong>Generate</strong>，很快CMake底部输出栏就输出了“Generating done”。这时我们通过Open Project打开build文件夹，可以看到生成了很多文件（夹）：</p>
<p><img src="https://pic.downk.cc/item/5f79b3ff160a154a67620664.jpg"></p>
<p>用 Visual Studio 打开上图中的Darknet.sln。选择“Release”+“x64”，右键点击“ALL_BUILD”，在右键菜单中，选择“生成”，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f79c2df160a154a6765fcaf.jpg"></p>
<p>接着再右键点击“INSTALL”，在右键菜单中，选择“生成”，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f79c351160a154a67661818.jpg"></p>
<p>当我想要运行darknet和yolo看看时，又遇到报错：“由于找不到pthreadVC2.dll,无法继续执行代码,重新安装程序可能会解决此问题”。</p>
<p>解决办法是：去网上下载pthreadvc2.dll，然后同时复制粘贴到<code>C:\Windows\System32</code>和<code>C:\Windows\SysWOW64</code>目录下。</p>
<p>最后一步，就是运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">C:\MachineLearning\CV\darknet&gt;darknet.exe detect cfg&#x2F;yolov4.cfg weights&#x2F;yolov4.weights data&#x2F;dog.jpg</span><br></pre></td></tr></table></figure>
<p>我们还可以在命令行后面加<code>-thresh 0.25</code> 、<code>-i 0</code>、 <code>-ext_output dog.jpg</code>、<code>-out_filename results/yolov4.mp4</code>个性化输出结果，其中，i代表GPU指数，-ext_output代表输出坐标，-out_filename 代表保存成图片或视频。</p>
<p>yolov4的效果比yolov3还好，连盆栽都检测出来了：</p>
<p><img src="https://pic.downk.cc/item/5f79cbd8160a154a67681d63.jpg"></p>
<p>为了以后能在其他文件夹下面运行darknet，我把darknet.exe的路径加入了环境变量：</p>
<p><img src="https://pic.downk.cc/item/5f79cd35160a154a676877ff.jpg" width=80% style="zoom: 80%;" ></p>
<h1 id="使用YOLO"><a href="#使用YOLO" class="headerlink" title="使用YOLO"></a>使用YOLO</h1><p>除了上面说的检测图片，我们还可以这么玩yolo：</p>
<p><strong>1.检测本地电脑上的视频</strong></p>
<p>修改video_yolov4.sh里面的内容如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./darknet detector demo ./cfg/coco.data ./cfg/yolov4.cfg ./weights/yolov4.weights data/cars.mp4 -out_filename results/yolov4.mp4</span><br><span class="line"></span><br><span class="line"><span class="comment"># ./darknet detector demo ./cfg/coco.data ./cfg/yolov4-tiny.cfg ./weights/yolov4-tiny.weights data/cars.mp4 -out_filename results/yolov4-tiny.mp4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./darknet detector demo ./cfg/coco.data ./weights/Yolo-Fastest/COCO/yolo-fastest.cfg ./weights/Yolo-Fastest/COCO/yolo-fastest.weights -out_filename results/yolov4-fastest.mp4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>打出<code>sh video_yolov4.sh</code>即可运行。保存成sh的好处是后面要跑的时候不需要再敲一遍长长的命令行。</p>
<p><strong>2. 检测电脑摄像头画面</strong></p>
<p>如果你想检测电脑摄像头画面里的物体，则运行：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">darknet.exe detector demo cfg\coco.data cfg\yolov4.cfg weights&#x2F;yolov4.weights -c 0</span><br></pre></td></tr></table></figure>
<p><strong>3. 检测网络摄像头画面</strong></p>
<p>我这里要将手机拍摄的画面传给电脑，然后用电脑对手机传输过来的画面检测。<br>先下载一个IP摄像头App，然后点击“开启服务器”</p>
<p><img src="https://pic.downk.cc/item/5f7be000160a154a67dc583a.jpg" width=35%></p>
<p>你就可以看到手机画面进入了摄像头预览界面，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f7be086160a154a67dc75ff.jpg"></p>
<p>然后记住屏幕上的IP地址，比如我这里是192.168.1.4:8080,那么我就运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">darknet.exe detector demo cfg&#x2F;coco.data cfg&#x2F;yolov4.cfg weights&#x2F;yolov4.weights http:&#x2F;&#x2F;192.168.1.4:8080&#x2F;video?dummy&#x3D;param.mjpg</span><br></pre></td></tr></table></figure>
<p><strong>4. 使用YOLO Fastesst</strong></p>
<p>有大佬开源了号称最快版的YOLO版本，把这个<a href="https://github.com/dog-qiuqiu/Yolo-Fastest">仓库</a>下载下来后我们只需要取它的cfg文件和weights文件即可</p>
<p><img src="https://pic.downk.cc/item/5f7be8a2160a154a67de213e.jpg"></p>
<p>修改命令行后运行<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">darknet.exe detector demo cfg&#x2F;coco.data weights&#x2F;Yolo-Fastest&#x2F;COCO&#x2F;yolo-fastest.cfg weights&#x2F;Yolo-Fastest&#x2F;COCO&#x2F;yolo-fastest.weights -c 0</span><br></pre></td></tr></table></figure></p>
<p><strong>5. 用OpenCV的DNN跑YOLO</strong></p>
<p>python版：【<a href="https://github.com/hpc203/Yolo-Fastest-opencv-dnn">Github Repo 1</a>】【<a href="https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO">Github Repo 2</a>】<br>java版：</p>
<p><strong>6. 在colab上跑YOLO和训练自己的数据集</strong></p>
<p>相关教程视频请见The AI GUy的<a href="https://www.youtube.com/playlist?list=PLKHYJbyeQ1a3tMm-Wm6YLRzfW1UmwdUIN">YOLOv4 Object Detection系列</a></p>
<p>打开<a href="https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg?usp=sharing">该colab笔记本</a>。</p>
<p><strong>7. 将yolo移植到tensorflow上面</strong></p>
<p>相关教程视频请见The AI GUy的<a href="https://www.youtube.com/playlist?list=PLKHYJbyeQ1a3tMm-Wm6YLRzfW1UmwdUIN">YOLOv4 Object Detection系列</a></p>
<p>这里简单复述一下重点，如果使用自定义类别和数据集，请查看<a href="https://github.com/theAIGuysCode/tensorflow-yolov4-tflite">repo</a>：</p>
<p>使用以下命令行将.weights 格式转换为.pb格式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Convert yolov4 darknet weights to tensorflow</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;yolov4.weights --output .&#x2F;checkpoints&#x2F;yolov4-416 --input_size 416 --model yolov4 </span><br><span class="line"></span><br><span class="line"># Convert yolov4-tiny darknet weights to tensorflow</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;yolov4-tiny.weights --output .&#x2F;checkpoints&#x2F;yolov4-tiny-416 --input_size 416 --model yolov4 --tiny</span><br><span class="line"></span><br><span class="line"># Convert custom yolov4 darknet weights to tensorflow</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;custom.weights --output .&#x2F;checkpoints&#x2F;custom-416 --input_size 416 --model yolov4 </span><br></pre></td></tr></table></figure>
<p>我自己电脑上的命令行是：<code>python save_model.py --weights ../../darknet/weights/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4</code><br>最后生成的文件长这样：</p>
<p><img src="https://pic.downk.cc/item/5f7bf98f160a154a67e22532.jpg"></p>
<p>运行yolo的tensorflow pb格式模型</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Run yolov4 tensorflow model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg</span><br><span class="line"></span><br><span class="line"># Run yolov4-tiny tensorflow model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-tiny-416 --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg --tiny</span><br><span class="line"></span><br><span class="line"># Run custom yolov4 tensorflow model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;custom-416 --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;car.jpg</span><br><span class="line"></span><br><span class="line"># Run yolov4 on video</span><br><span class="line">python detect_video.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --size 416 --model yolov4 --video .&#x2F;data&#x2F;video&#x2F;road.mp4 --output .&#x2F;detections&#x2F;results.avi</span><br><span class="line"></span><br><span class="line"># Run custom yolov4 model on video</span><br><span class="line">python detect_video.py --weights .&#x2F;checkpoints&#x2F;custom-416 --size 416 --model yolov4 --video .&#x2F;data&#x2F;video&#x2F;cars.mp4 --output .&#x2F;detections&#x2F;results.avi</span><br><span class="line"></span><br><span class="line"># Run yolov4 on webcam</span><br><span class="line">python detect_video.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --size 416 --model yolov4 --video 0 --output .&#x2F;detections&#x2F;results.avi</span><br></pre></td></tr></table></figure>
<p>上面生成的模型是pb格式，接下来是生成tflite格式模型：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Save tf model for tflite converting</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;yolov4.weights --output .&#x2F;checkpoints&#x2F;yolov4-416 --input_size 416 --model yolov4 --framework tflite</span><br><span class="line"></span><br><span class="line"># Save custom yolov4 tf model for tflite converting</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;custom.weights --output .&#x2F;checkpoints&#x2F;custom-416 --input_size 416 --model yolov4 --framework tflite</span><br></pre></td></tr></table></figure>
<p>我自己电脑上运行的命令行是<code>python save_model.py --weights ../../darknet/weights/yolov4.weights --output ./checkpoints/yolov4-416-tflite --input_size 416 --model yolov4 --framework tflite</code>。注意，如果要生成tflite，不能直接用前面那个pb文件，得在命令行后面加个<code>--framework tflite</code>重新生成pb文件，然后再用这个pb文件转换成tflite文件。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># yolov4</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --output .&#x2F;checkpoints&#x2F;yolov4-416.tflite</span><br><span class="line"></span><br><span class="line"># convert custom yolov4 tflite model</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;custom-416 --output .&#x2F;checkpoints&#x2F;custom-416.tflite</span><br><span class="line"></span><br><span class="line"># yolov4 quantize float16</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --output .&#x2F;checkpoints&#x2F;yolov4-416-fp16.tflite --quantize_mode float16</span><br><span class="line"></span><br><span class="line"># yolov4 quantize int8</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --output .&#x2F;checkpoints&#x2F;yolov4-416-int8.tflite --quantize_mode int8 --dataset .&#x2F;data&#x2F;dataset&#x2F;val2017.txt</span><br></pre></td></tr></table></figure>
<p>我自己电脑上运行的命令行是：<code>python convert_tflite.py --weights ./checkpoints/yolov4-416-tflite --output ./checkpoints/yolov4-416-int8.tflite --quantize_mode int8 --dataset ./data/dataset/val2017.txt</code></p>
<p>最后测试一下yolov4 tflite的模型性能如何<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Run tflite model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-416.tflite --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg --framework tflite</span><br><span class="line"></span><br><span class="line"># Run tflite model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-416-int8.tflite --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg --framework tflite</span><br><span class="line"></span><br><span class="line"># Run custom tflite model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;custom-416.tflite --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;car.jpg --framework tflite</span><br></pre></td></tr></table></figure></p>
<p>不过这个hunglc007的tensorflow-yolov4-tflite仓库的tflite版有问题，首先是它的pb格式模型虽然生成出来了，但我用一个视频测试了一下，视频中的汽车一个都没检测出来，然后要把pb模型转换成tflite模型时遇到了报错：“RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor input_1<br>Empty min/max for tensor input_1”。然后我在这个<a href="https://github.com/hunglc007/tensorflow-yolov4-tflite/issues/207">issue</a>里看到有人说用这个<a href="https://github.com/hhk7734/tensorflow-yolov4">repo</a>就没问题，我试了一下，生成的tflite模型大小有251M，准确度跟yolov4.weights差不多。</p>
<p><strong>8. 训练自己的yolo检测器</strong></p>
<p>要训练自己的YOLO检测器，首先要有数据集，The AI Guy介绍了两种制作数据集的方法：</p>
<p>第一种：从Google的 <a href="https://storage.googleapis.com/openimages/web/index.html">Open Images Dataset</a> 下载图片，相关的脚本在这个<a href="https://github.com/theAIGuysCode/OIDv4_ToolKit">repo</a>或<a href="https://www.youtube.com/watch?v=_4A9inxGqRM">视频</a>。简单来说，就是先运行<code>python main.py downloader --classes Apple Orange --type_csv train --limit 1000 --multiclasses 1</code>下载需要的图片，然后在运行<code>python convert_annotations.py</code>之前，我们需要在它的同级目录下创建一个classes.txt文件，内容就是一个类别一行，convert_annotations.py会利用它把Labels文件夹的txt文件转换成yolo格式的txt文件，并且生成的txt文件和图片位于同一文件夹里面。</p>
<p>第二种：从Google的图片搜索里下载图片，相关的脚本在这个<a href="https://github.com/theAIGuysCode/Download-Google-Images">repo</a>或<a href="https://www.youtube.com/watch?v=EGQyDla8JNU">视频</a>。简单来说，它是生成一个包含一堆图片链接的urls.txt，然后用<code>python download_images.py --urls urls.txt --output images</code>把这些图片下载下来，并用opencv测试打开这些图片，如果打不开，则说明格式可能不符合（比如webp），那么就将这些图片删除掉。得到图片后再用<a href="https://github.com/tzutalin/labelImg">labelImg</a>标注图片，这个软件可以选保存为YOLO格式。</p>
<p>以此类推，再下载20%的验证集和测试集</p>
<p>完成上面的两者之一的步骤后，你应该拥有了两个文件夹，每个文件夹里面有很多图片和同名的txt文件，打包成obj.zip和test.zip之后上传到google drive。接下来，我们就是要处理custom .cfg, obj.data, obj.names, train.txt 和 test.txt 这些文件了，具体步骤看<a href="https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg?usp=sharing#scrollTo=A9mYUoKOWWlR">YOLOv4_Training_Tutorial.ipynb</a>。<br>注意，如果你要用这个笔记本训练自己的数据集，请先拷贝一份到自己的谷歌云端硬盘。</p>
<p><strong>9. 结合deepsort实现目标跟踪</strong></p>
<p>来自The AI Guy开源的<a href="https://github.com/theAIGuysCode/yolov4-deepsort">repo</a>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Run yolov4 deep sort object tracker on video</span><br><span class="line">python object_tracker.py --weights ..&#x2F;..&#x2F;Object_Detection&#x2F;tensorflow-yolov4-tflite&#x2F;checkpoints&#x2F;yolov4-tiny-416 --video .&#x2F;data&#x2F;video&#x2F;test.mp4 --output .&#x2F;outputs&#x2F;yolov4-tiny-tf-track-result.avi --model yolov4</span><br><span class="line"></span><br><span class="line"># Run yolov4 deep sort object tracker on webcam (set video flag to 0)</span><br><span class="line">python object_tracker.py --video 0 --output .&#x2F;outputs&#x2F;webcam.avi --model yolov4</span><br></pre></td></tr></table></figure>
<p>实测结果是：<br>行人视频：<br>用CPU<br>yolov4-tf：1.6 FPS<br>yolov4-tiny-tf：1.8 FPS</p>
<p>汽车视频（cars.mp4）:<br>GPU（GTX 1660 Ti）<br>yolov4 : 17.4 FPS(输出视频) 18.8 FPS （不输出视频）<br>yolov4-tiny : 19.5 FPS(输出视频) 27.2 FPS（不输出视频）<br>yolo-Fastest ； 20.1 FPS (输出视频) 28.2FPS（不输出视频）</p>
<p>这个帧率跟你的命令行也有关系，比如你的命令行说要输出视频，那么帧率就会低一点。</p>
<p>The AI Guy还做了一版用colab跑的<a href="https://colab.research.google.com/drive/1zmeSTP3J5zu2d5fHgsQC06DyYEYJFXq1?usp=sharing">ipynb</a>和<a href="https://www.youtube.com/watch?v=_zrNUzDS8Zc&amp;list=PLKHYJbyeQ1a3tMm-Wm6YLRzfW1UmwdUIN&amp;index=10">视频</a>，FPS能达到10FPS，不过我们也看到，用另一个车辆视频测试时，FPS提高到了15FPS，所以可以得出结论，跟踪的物体越多，FPS越小。</p>
<p>关于如何过滤显示的类别，具体请查看它的仓库。</p>
<p><strong>10.用YOLOV4检测结果导出为txt文件作为预标注</strong></p>
<p>如果我们的数据集很多图片是未标注的，如果都是人工去标注，是很枯燥费时的，我们可以利用命令行让YOLOV4把检测结果导出成它训练时用的txt那种格式的文件，也是labelimg导出的txt文件，导出来后打开labelimg，把图片和标注txt的文件夹设置好，labelimg自动会对应上同名的图片和标注txt文件，自动把矩形框显示在界面上，这时候你再微调一下就可以了。</p>
<p>这个<a href="https://github.com/AlexeyAB/darknet/issues/6240">issue</a>讨论了这个问题。<br>有一位网友biparnakroy还创建了一个相关项目 : <a href="https://github.com/biparnakroy/pseudoLabelGeneratorForYOLO，">https://github.com/biparnakroy/pseudoLabelGeneratorForYOLO，</a></p>
<p>并提供了一个转换成VOC格式的仓库：<br><a href="https://github.com/hai-h-nguyen/Yolo2Pascal-annotation-conversion，">https://github.com/hai-h-nguyen/Yolo2Pascal-annotation-conversion，</a></p>
<p>评论里还有人说有类别重映射的仓库：<a href="https://github.com/sa7ina/YOLOv2-5_Class_Remap">https://github.com/sa7ina/YOLOv2-5_Class_Remap</a></p>
<p>运行的命令行是：<code>darknet.exe detector test cfg/coco.data cfg/yolov4.cfg weights/yolov4.weights -thresh 0.25 -dont_show -save_labels &lt; data/new_train.txt</code></p>
<p>关键在于<code>-save_labels &lt; data/new_train.txt</code>, 注意，符号<code>&lt;</code>不能少了，那new_train.txt里面是什么呢，里面是带检测图片的路径:</p>
<p><img src="https://pic.downk.cc/item/5fa16a541cd1bbb86bdfa678.jpg"></p>
<p>我是用以下脚本生成这个new_train.txt的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">txt_file = <span class="string">r&quot;C:\MachineLearning\CV\darknet\data\new_train.txt&quot;</span></span><br><span class="line">reference_folder = <span class="string">r&quot;D:\MachineLearning\DataSet\iGuardDataset\IMAGE\temp&quot;</span></span><br><span class="line"><span class="built_in">print</span>(os.listdir(reference_folder))</span><br><span class="line">prefix = <span class="string">r&#x27;D:/MachineLearning/DataSet/iGuardDataset/IMAGE/temp/img&#x27;</span></span><br><span class="line">extension = <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">start_num = <span class="number">384</span></span><br><span class="line">num_files = <span class="built_in">len</span>([name <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(reference_folder) <span class="keyword">if</span> os.path.isfile(os.path.join(reference_folder, name))])  <span class="comment"># 排除将文件夹也计数</span></span><br><span class="line"></span><br><span class="line">f = <span class="built_in">open</span>(txt_file, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(start_num, start_num+num_files):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s%d%s&#x27;</span> % (prefix, index, extension), file=f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;写入一行：&#123;&#125;&#123;&#125;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(prefix, index, extension))</span><br></pre></td></tr></table></figure>
<p>运行正常结束后，就会在图片的同一个文件夹下面生成同名的txt：</p>
<p><img src="https://pic.downk.cc/item/5fa16b7c1cd1bbb86bdfe1d2.jpg"></p>
<p>注意看，图中一个文件是classes.txt，这个东西的用处是labelimg软件根据标注框txt里的类别索引整数索引到这个classes.txt里的真实类别名称，这个文件不是代码生成的，因为我是用coco类别检测的，所以这个classes.txt是我从data/coco.names拷贝过来重命名的。</p>
<p>看，这就是YOLOV4帮你标注好的矩形框：</p>
<p><img src="https://pic.downk.cc/item/5fa16d671cd1bbb86be041a2.jpg"></p>
<p>如果觉得误检的比较多，那么你就调高threshold。又或者你觉得不要给我检测那么多类出来，还要我一个一个去删，解决思路是自己写个python脚本，对txt文件里你不想要的类别id所在的行删除掉。</p>
<h1 id="YOLOV4模型可视化"><a href="#YOLOV4模型可视化" class="headerlink" title="YOLOV4模型可视化"></a>YOLOV4模型可视化</h1><p>使用netron就可以查看yolov4的模型结构，通过打开cfg文件，没错，cfg文件你既可以用VS code打开用文本形式查看，也可以用netron打开，用图形形式查看。</p>
<h1 id="YOLO-cfg配置参数讲解"><a href="#YOLO-cfg配置参数讲解" class="headerlink" title="YOLO cfg配置参数讲解"></a>YOLO cfg配置参数讲解</h1><p>详细的解释请查看darknet Github Wiki：</p>
<ul>
<li><a href="https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-%5Bnet%5D-section">CFG Parameters in the [net] section</a></li>
<li><a href="https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-different-layers">CFG Parameters in the different layers  </a></li>
<li><a href="https://blog.csdn.net/hrsstudy/article/details/65447947">中文博客解释</a>  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[net]</span><br><span class="line">batch&#x3D;64</span><br><span class="line">subdivisions&#x3D;16  # 如果内存不够大，将batch分割为subdivisions个子batch,batch固定的前提下，subdivision越小，训练出来的模型精度越高</span><br><span class="line">momentum&#x3D;0.949</span><br><span class="line">decay&#x3D;0.0005  # 权重衰减正则项，防止过拟合</span><br><span class="line">angle&#x3D;0  # 通过旋转角度来生成更多训练样本</span><br><span class="line">saturation &#x3D; 1.5</span><br><span class="line">exposure &#x3D; 1.5</span><br><span class="line">hue&#x3D;.1</span><br><span class="line"></span><br><span class="line">learning_rate&#x3D;0.001</span><br><span class="line">burn_in&#x3D;1000  # current_learning rate &#x3D; learning_rate * pow(steps &#x2F; burn_in, power) ，，其中，learning_rate&#x3D;0.001，power&#x3D;4，所以 burn_in &#x3D; 0.001 * pow(steps&#x2F;1000, 4) ，从公式可以看出，当steps小于1000时，steps&#x2F;1000小于1，这样的数再做4次方，那就更小了，所以第1到第1000个steps时，学习率远远小于0.001，1000个steps之后，学习率才等于0.001,有点像yolov5的warm up。</span><br><span class="line"></span><br><span class="line">max_batches &#x3D; 6000  # 训练达到max_batches后停止学习</span><br><span class="line">policy&#x3D;steps  # 调整学习率的policy，有如下policy：CONSTANT, STEP, EXP, POLY, STEPS, SIG, RANDOM</span><br><span class="line">steps&#x3D;4800,5400  #  这两个数值代表着steps进行到这两个时间点时，学习率会乘以下面对应的两个scales</span><br><span class="line">scales&#x3D;.1,.1  # 以此类推，如果steps&#x3D;8000,9000,12000, scales&#x3D;.1,.1,.1 且当前迭代步数为10000 ，那么当前学习率为 &#x3D; learning_rate * scales[0] * scales[1] &#x3D; 0.001 * 0.1 * 0.1 &#x3D; 0.00001</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask &#x3D; 6,7,8</span><br><span class="line">anchors &#x3D; 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401</span><br><span class="line">classes&#x3D;2</span><br><span class="line">num&#x3D;9  # anchors或者说mask的数量</span><br><span class="line">jitter&#x3D;.3  # 随意更改图像大小和宽高比从 x(1 - 2*jitter) 到 x(1 + 2*jitter)</span><br><span class="line">ignore_thresh &#x3D; .7</span><br><span class="line">truth_thresh &#x3D; 1</span><br><span class="line">random&#x3D;1  # random为1时会启用Multi-Scale Training，随机使用不同尺寸的图片进行训练。</span><br><span class="line">scale_x_y &#x3D; 1.05</span><br><span class="line">iou_thresh&#x3D;0.213</span><br><span class="line">cls_normalizer&#x3D;1.0</span><br><span class="line">iou_normalizer&#x3D;0.07</span><br><span class="line">iou_loss&#x3D;ciou</span><br><span class="line">nms_kind&#x3D;greedynms</span><br><span class="line">beta_nms&#x3D;0.6</span><br><span class="line">max_delta&#x3D;5</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="YOLO-V4的tricks"><a href="#YOLO-V4的tricks" class="headerlink" title="YOLO V4的tricks"></a>YOLO V4的tricks</h1><p>相信我们已经被上面yolo v4的检测效果所惊喜，那yolo v4到底用了什么技巧达到如此准确的检测呢？</p>
<p><a href="https://zhuanlan.zhihu.com/p/103070923">https://zhuanlan.zhihu.com/p/103070923</a></p>
<p><a href="https://aistudio.baidu.com/aistudio/education/group/info/1617">百度目标检测7日打卡营</a></p>
<h1 id="如何训练YOLOV4来检测自定义目标"><a href="#如何训练YOLOV4来检测自定义目标" class="headerlink" title="如何训练YOLOV4来检测自定义目标"></a>如何训练YOLOV4来检测自定义目标</h1><ol>
<li><p>要训练<code>cfg/yolov4-custom.cfg</code>，请下载预训练的权重文件（162 MB）：<a href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137">yolov4.conv.137</a>。</p>
</li>
<li><p>创建文件yolo-obj.cfg，其内容与yolov4-custom.cfg中的内容相同（或将yolov4-custom.cfg复制到yolo-obj.cfg），并：</p>
<p>将batcch更改为<code>batch= 64</code></p>
<p>将subdivisions更改为<code>subdivisions=16</code></p>
<p>将max_batches更改为class * 2000（但不少于训练图像的数量，但不少于训练图像的数量且不少于6000），f.e。 如果您训练3个类别，<code>max_batches = 6000</code></p>
<p>将steps更改为max_batches的80％和90％，例如 <code>steps=4800,5400</code></p>
<p>设置网络大小width = 416 height = 416或任何32的值的倍数</p>
<p>在3个<code>[yolo]</code>层中将<code>classes=80</code>更改为实际类别数</p>
<p>在3个<code>[yolo]</code>层之前的 <code>[convolutional]</code>中将<code>[filters = 255]</code>更改为<code>[filters =（classs + 5）x3]</code>，请记住，只需要改每个[yolo]层之前的那个<code>[convolutional]</code> 。因此，如果classes = 1，则应该是filter = 18。 如果class = 2，则filter = 21。详细的公式为：<code>filters=(classes + coords + 1)*&lt;number of mask&gt;</code></p>
<p>使用<code>[Gaussian_yolo]</code>层时，请在每个<code>[Gaussian_yolo]</code>层之前的3个<code>[convolutional]</code>中更改<code>[filters = 57]</code> 为 <code>filter =（classs + 9）x3</code></p>
</li>
<li><p>在目录build \ darknet \ x64 \ data \中创建文件obj.names，内容是一个目标名称一个行。</p>
</li>
<li><p>在目录build \ darknet \ x64 \ data \中创建文件obj.data，内容是：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">classes &#x3D; 2</span><br><span class="line">train  &#x3D; data&#x2F;train.txt</span><br><span class="line">valid  &#x3D; data&#x2F;test.txt</span><br><span class="line">names &#x3D; data&#x2F;obj.names</span><br><span class="line">backup &#x3D; backup&#x2F;</span><br></pre></td></tr></table></figure>
<ol>
<li>给数据集标注好矩形框，标签文件是格式<code>&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code>的txt文件。其中：</li>
</ol>
<ul>
<li><code>&lt;object-class&gt;</code> - 代表目标类别的整数</li>
<li><code>&lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> - 相对于图像宽度高度的浮点值，位于 (0.0 to 1.0]区间。比如: <code>&lt;x&gt; = &lt;absolute_x&gt; / &lt;image_width&gt;</code> &amp; <code>&lt;height&gt; = * &lt;absolute_height&gt; / &lt;image_height&gt;</code></li>
<li>注意 <x_center> <y_center> - 是矩形框的中心，不是左上角。</li>
</ul>
<ol>
<li>Create file <code>train.txt</code> in directory <code>build\darknet\x64\data\</code>, with filenames of your images, each filename in new line, with path relative to <code>darknet.exe</code>, for example containing:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data&#x2F;obj&#x2F;img1.jpg</span><br><span class="line">data&#x2F;obj&#x2F;img2.jpg</span><br><span class="line">data&#x2F;obj&#x2F;img3.jpg</span><br></pre></td></tr></table></figure>
<ol>
<li>Download pre-trained weights for the convolutional layers and put to the directory <code>build\darknet\x64</code></li>
</ol>
<ul>
<li>for <code>yolov4.cfg</code>, <code>yolov4-custom.cfg</code> (162 MB): <a href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137">yolov4.conv.137</a> (Google drive mirror <a href="https://drive.google.com/open?id=1JKF-bdIklxOOVy-2Cr5qdvjgGpmGfcbp">yolov4.conv.137</a> )</li>
<li>for <code>yolov4-tiny.cfg</code>, <code>yolov4-tiny-3l.cfg</code>, <code>yolov4-tiny-custom.cfg</code> (19 MB): <a href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29">yolov4-tiny.conv.29</a></li>
<li>for <code>csresnext50-panet-spp.cfg</code> (133 MB): <a href="https://drive.google.com/file/d/16yMYCLQTY_oDlCIZPfn_sab6KD3zgzGq/view?usp=sharing">csresnext50-panet-spp.conv.112</a></li>
<li>for <code>yolov3.cfg, yolov3-spp.cfg</code> (154 MB): <a href="https://pjreddie.com/media/files/darknet53.conv.74">darknet53.conv.74</a></li>
<li>for <code>yolov3-tiny-prn.cfg , yolov3-tiny.cfg</code> (6 MB): <a href="https://drive.google.com/file/d/18v36esoXCh-PsOKwyP2GWrpYDptDY8Zf/view?usp=sharing">yolov3-tiny.conv.11</a></li>
<li>for <code>enet-coco.cfg (EfficientNetB0-Yolov3)</code> (14 MB): <a href="https://drive.google.com/file/d/1uhh3D6RSn0ekgmsaTcl-ZW53WBaUDo6j/view?usp=sharing">enetb0-coco.conv.132</a></li>
</ul>
<ol>
<li>Start training by using the command line: <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137</code></li>
</ol>
<p>To train on Linux use command: <code>./darknet detector train data/obj.data yolo-obj.cfg yolov4.conv.137</code> (just use <code>./darknet</code> instead of <code>darknet.exe</code>)</p>
<ul>
<li>(file <code>yolo-obj_last.weights</code> will be saved to the <code>build\darknet\x64\backup\</code> for each 100 iterations)</li>
<li>(file <code>yolo-obj_xxxx.weights</code> will be saved to the <code>build\darknet\x64\backup\</code> for each 1000 iterations)</li>
<li>(to disable Loss-Window use <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -dont_show</code>, if you train on computer without monitor like a cloud Amazon EC2)</li>
<li>(to see the mAP &amp; Loss-chart during training on remote server without GUI, use command <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map</code> then open URL <code>http://ip-address:8090</code> in Chrome/Firefox browser)</li>
</ul>
<p>8.1. For training with mAP (mean average precisions) calculation for each 4 Epochs (set <code>valid=valid.txt</code> or <code>train.txt</code> in <code>obj.data</code> file) and run: <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -map</code></p>
<ol>
<li>After training is complete - get result <code>yolo-obj_final.weights</code> from path <code>build\darknet\x64\backup\</code></li>
</ol>
<ul>
<li><p>After each 100 iterations you can stop and later start training from this point. For example, after 2000 iterations you can stop training, and later just start training using: <code>darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights</code></p>
<p>(in the original repository <a href="https://github.com/pjreddie/darknet">https://github.com/pjreddie/darknet</a> the weights-file is saved only once every 10 000 iterations <code>if(iterations &gt; 1000)</code>)</p>
</li>
</ul>
<p><strong>Note:</strong> If during training you see <code>nan</code> values for <code>avg</code> (loss) field - then training goes wrong, but if <code>nan</code> is in some other lines - then training goes well.</p>
<p><strong>Note:</strong> If you changed width= or height= in your cfg-file, then new width and height must be divisible by 32.</p>
<p><strong>Note:</strong> After training use such command for detection: <code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p>
<p><strong>Note:</strong> if error <code>Out of memory</code> occurs then in <code>.cfg</code>-file you should increase <code>subdivisions=16</code>, 32 or 64。</p>
<h1 id="如何训练YOLOV4-tiny来检测自定义目标"><a href="#如何训练YOLOV4-tiny来检测自定义目标" class="headerlink" title="如何训练YOLOV4 - tiny来检测自定义目标"></a>如何训练YOLOV4 - tiny来检测自定义目标</h1><p>Do all the same steps as for the full yolo model as described above. With the exception of:</p>
<ul>
<li>Download file with the first 29-convolutional layers of yolov4-tiny: <a href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29">https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29</a> (Or get this file from yolov4-tiny.weights file by using command: <code>darknet.exe partial cfg/yolov4-tiny-custom.cfg yolov4-tiny.weights yolov4-tiny.conv.29 29</code></li>
<li>Make your custom model <code>yolov4-tiny-obj.cfg</code> based on <code>cfg/yolov4-tiny-custom.cfg</code> instead of <code>yolov4.cfg</code></li>
<li>Start training: <code>darknet.exe detector train data/obj.data yolov4-tiny-obj.cfg yolov4-tiny.conv.29</code></li>
</ul>
<p>For training Yolo based on other models (<a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/densenet201_yolo.cfg">DenseNet201-Yolo</a> or <a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/resnet50_yolo.cfg">ResNet50-Yolo</a>), you can download and get pre-trained weights as showed in this file: <a href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd">https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd</a> If you made you custom model that isn’t based on other models, then you can train it without pre-trained weights, then will be used random initial weights.</p>
<h1 id="怎样改善检测结果"><a href="#怎样改善检测结果" class="headerlink" title="怎样改善检测结果"></a>怎样改善检测结果</h1><p><a href="https://github.com/AlexeyAB/darknet#how-to-improve-object-detection">翻译</a></p>
<p><strong>训练前</strong>：</p>
<ul>
<li><p>在您的.cfg文件中设置flag <code>random = 1</code>, 这样它会通过训练Yolo不同的分辨率来提高精度：<a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L788">链接</a></p>
</li>
<li><p>增加.cfg文件中的网络分辨率（高度= 608，宽度= 608或任何32的倍数）-这将提高精度</p>
</li>
<li><p>检查您要检测的每个目标是否在数据集中都有被标记以及是否被正确标记，如果您想检查目标是否别正确标记，可以在训练的命令行后面加上<code>-show_imgs</code>。</p>
</li>
<li><p>对于您要检测的目标，应该有它：不同比例，不同角度(间隔30度），不同照明，不同背景的图像， 每个类别最好拥有2000张不同的图像，并且您应训练2000 *class以上的迭代。从神经网络的内部角度来看，这些都是不同的对象。因此，要检测的对象越不同，应使用越复杂的网络模型。</p>
</li>
<li><p>希望您的训练数据集包含没有检测目标的图像（对应空的.txt文件）-使用与带有对象的图像一样多的负样本图像</p>
</li>
<li><p>针对图像中有大量对象的情况，训练前请在cfg文件的最后一个[yolo]层或[region]层中添加参数max = 200或更高的值（YoloV3可以检测到的最多目标数量是0,0615234375 <em>（width </em> height），其中width和height是cfg文件中[net]部分的参数）。</p>
</li>
<li><p>要训练小目标（将图像调整为416x416后小于16x16的目标称为小目标），设置<code>layers = 23</code>,而不是<code>layers = 54</code>(<a href="https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L895">https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L895</a>)</p>
<p>设置<code>stride = 4</code>而不是<code>stride = 2</code>(<a href="https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L892">https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L892</a>)</p>
<p>设置<code>stride = 4</code>而不是<code>stride = 2</code>(<a href="https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L989">https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L989</a>)</p>
</li>
<li><p>对于既要训练大目标，又要训练小目标，请使用修改后的模型：</p>
<p>Full-model：5个yolo层：<a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg</a></p>
<p>Tiny-model：3个yolo层：<a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny_3l.cfg">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny_3l.cfg</a></p>
<p>YOLOv4：3个yolo层：<a href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-custom.cfg">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-custom.cfg</a>  (比yolov4.cfg多了stopbackward=800，这句代码AlexeyAB原话是这么解释的：put stopbackward=800 before some layer meaning that all the layers’ weights before current layer will not be updated only for the first 800 iterations. )</p>
</li>
<li><p>如果您训练的模型将左对象和右对象区分为单独的类（左/右手，左/右转道路标志，…），则禁用翻转数据增强<code>flip = 0</code>：<a href="https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17">https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17</a></p>
</li>
<li><p>训练集和测试集中都要有相对图像大小差不多的目标</p>
</li>
<li><p>加快训练速度（伴随降低检测精度）在cfg文件中为第136层设置参数<code>stopbackward = 1</code></p>
</li>
<li><p>为了使检测到的边界框更准确，您可以向每个[yolo]层添加3个参数<code>ignore_thresh = .9</code> <code>iou_normalizer = 0.5</code> <code>iou_loss = giou</code>并进行训练，它将增加mAP@0.9，但减小mAP@0.5。</p>
</li>
<li><p>仅当您是神经检测网络专家时再这么做 ：在cfg文件中重新计算数据集的锚框的宽度和高度：<code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code>然后设置相同的9个锚点在cfg文件的3个[yolo]图层中的每个图层中。但是您应该为每个[yolo]层更改anchor的遮罩 <code>masks</code>，因此对于YOLOv4，第一层[yolo]层的锚点小于30x30，第二层小于60x60，剩下的第3层，以此类推。同样，您应该在每个[yolo]层之前更改<code>filter =（classs + 5）* &lt;mask的数量&gt;</code>。如果许多计算出的锚不适合在适当的图层下, 那就使用默认锚框即可。</p>
</li>
</ul>
<p><strong>训练后，用于检测：</strong></p>
<ul>
<li>通过在.cfg文件中设置（<code>height= 608</code> 和<code>width= 608</code>）或（高度= 832和宽度= 832）或（任何32的倍数）来提高网络分辨率-这可以提高精度，并可以检测到小物件：<a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L8-L9">链接</a>。而且若要升高分辨率的话，无需再次从头训练网络，只需使用已经针对416x416分辨率进行训练的.weights文件。如果发生错误，内存不足，则在.cfg文件中，您应该增加<code>subdivisions= 16</code>、32或64：<a href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L4">链接</a></li>
</ul>
<h1 id="YOLO-V4的issues-amp-comments"><a href="#YOLO-V4的issues-amp-comments" class="headerlink" title="YOLO V4的issues&amp;comments"></a>YOLO V4的issues&amp;comments</h1><p><strong>Issue 1</strong>. Can Yolo3 take different width-height-ratio images as training input? #800</p>
<p><a href="https://github.com/pjreddie/darknet/issues/800#issuecomment-390352607">comment 1</a> ，<a href="https://github.com/pjreddie/darknet/issues/800#issuecomment-609566695">comment 2</a></p>
<p>概括：可以使用不同宽高比的图像进行训练，但这会使得模型的检测效果变糟糕，在yolov4.cfg里，可以使用<code>jitter</code>和<code>random</code>两个超参数来数据增强，减少这个影响。同时，YOLOv4会把训练图像保持宽高比缩放到长边等于416，短边用黑边填充到416。</p>
<p>这带给我们一个启示：“数据集图片尺寸不要跟网络的输入尺寸差距很大”。举个例子，咱们用手机相机拍摄的照片一般都很清晰，我的是4000多×2000多，但是<strong>越大越高分辨率的图片对神经网络来说是越不清晰</strong>！因为网络在预处理是会把图片尺寸除以原始图片的长边，比如我这里是除以4000多像素，那假如我的目标是100×200，转换成608×608的图片后，它的大小变成了15×30，也就是15×30的像素要表达我的那个目标，可想而知这丢失了很多目标信息，这样模型的小目标检测效果就会特别差！所以我建议把数据集的图片裁剪变小，推荐一个<a href="https://watermarkly.com/crop-photo/">裁剪工具</a>。</p>
<h1 id="智能标注工具"><a href="#智能标注工具" class="headerlink" title="智能标注工具"></a>智能标注工具</h1><p>训练前的数据标注是一个简单重复又意义重要的工作，现在就诞生了几个不错的数据标注工具，它们会帮你比如OPENCV/openvinotoolkit出品的CVAT（Computer Vision Annotation Tool），这里我用它的<a href="https://cvat.org/">在线版</a>演示一下单张图片单个类别的目标检测标注：</p>
<p>左侧工具栏有个魔术棒图标，我们可也利用它自己生成标注框：</p>
<p><img src="https://pic.downk.cc/item/5f7d93fd1cd1bbb86b6ac487.jpg"></p>
<p>标注结束后记得点一下save，最后就是点menu -&gt; Export as a dataset导出你想要的格式的数据集了：</p>
<p><img src="https://pic.downk.cc/item/5f7db1f41cd1bbb86b7255c1.jpg"></p>
<p>下载下来的数据集如下图所示，我导出的就是YOLO格式：</p>
<p><img src="https://pic.downk.cc/item/5f7d98a21cd1bbb86b6bbcdc.jpg"></p>
<p>这款工具还可能标注实例分割，数据源是视频等，功能还是比较强大的，但是缺点是因为是网页版，所以上传大的数据集容易网络断开。</p>
<p>具有半自动标注的工具了还有：</p>
<ul>
<li><p><a href="https://github.com/virajmavani/semi-auto-image-annotation-tool">semi-auto-image-annotation-tool</a>，不过似乎维护得比较不频繁了，但我觉得他们当时的创新之举还是值得一提的。</p>
</li>
<li><p><a href="https://github.com/microsoft/VoTT">VOTT</a>，在学位帽图标这里设置，Model Provider让你设置用什么模型预测，是SSD还是自定义模型，Predict让你设置预测出来框是否还给你带上标签Tag，Auto Detect让你设置点击下一张图片后是否自动检测<br><img src="https://pic.downk.cc/item/5f9d724a1cd1bbb86bc764e1.jpg" width=90%></p>
</li>
</ul>
<p>如果不自动检测的话，就得自己手动点下面这个同样学位帽的图标：<br> <img src="https://pic.downk.cc/item/5f9d72741cd1bbb86bc775db.jpg" ></p>
<p> 目前（2020/10/31）VOTT还不支持导出YOLO格式的功能，但这个<a href="https://github.com/microsoft/VoTT/issues/994">issue</a>里官方给的建议是用他们的<a href="https://roboflow.com/convert/vott-json-to-yolo-darknet-txt">Roboflow</a>转换工具把VoTT Json格式转换成YOLO txt格式。</p>
<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><p><strong>Tiny YOLOv4</strong> V.S. <strong>MobileNet SSD</strong></p>
<p><a href="https://www.youtube.com/watch?v=IAL480andOQ&amp;list=PL_Nji0JOuXg2E6QVMwCrLOzzTmr36fFcH&amp;index=2&amp;ab_channel=AugmentedStartups">https://www.youtube.com/watch?v=IAL480andOQ&amp;list=PL_Nji0JOuXg2E6QVMwCrLOzzTmr36fFcH&amp;index=2&amp;ab_channel=AugmentedStartups</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>电机类型与FOC控制</title>
    <url>/2020/09/29/%E7%94%B5%E6%9C%BA%E7%B1%BB%E5%9E%8B%E4%B8%8EFOC%E6%8E%A7%E5%88%B6/</url>
    <content><![CDATA[<p>RMF（Rotating Magnetic Field）</p>
<p>感应电机工作原理：<a href="https://www.youtube.com/watch?v=AQqyGNOP_3o">YouTube视频</a></p>
<p>永磁同步电机工作原理：<a href="https://www.youtube.com/watch?v=Vk2jDXxZIhs">YouTube视频</a></p>
<p>同步磁阻电动机：<a href="https://www.youtube.com/watch?v=vvw6k4ppUZU">YouTube视频</a></p>
<p>B站UP主稚晖君做过的一期<a href="https://www.bilibili.com/video/BV11V41127pq"><strong>视频</strong></a>里演示了FOC算法控制直流无刷电机的效果，对应地，他也写了一篇<a href="http://pengzhihui.xyz/2020/07/02/foc/#more"><strong>博客</strong></a>，博客里更多的是介绍FOC控制电机的原理。</p>
<p>视频中他提到他设计的Ctrl FOC驱动器的板子是参考了VESC和ODrive,VESC名字里的V是这块板子的创造者Benjamin Vedder的姓氏首字母是V，ESC代表Electronic Speed Controller。<br><img src="https://s1.ax1x.com/2020/09/29/0ejdSS.png" alt="0ejdSS.png" border="0" /></p>
]]></content>
      <tags>
        <tag>电机/马达</tag>
        <tag>FOC</tag>
      </tags>
  </entry>
  <entry>
    <title>损失函数</title>
    <url>/2020/09/26/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>目标检测任务的损失函数由Classificition Loss和Bounding Box Regeression Loss两部分构成。本文介绍目标检测任务中近几年来Bounding Box Regression Loss Function的演进过程，其演进路线是Smooth L1 Loss → IoU Loss → GIoU Loss → DIoU Loss → CIoU Loss，本文按照此路线进行讲解。</p>
<h2 id="1-Smooth-L1-Loss"><a href="#1-Smooth-L1-Loss" class="headerlink" title="1. Smooth L1 Loss"></a>1. <strong>Smooth L1 Loss</strong></h2><p>在Faster R-CNN以及SSD中对边框的回归使用的损失函数都是Smooth L1 作为损失函数，其定义为：</p>
<script type="math/tex; mode=display">SmoothL_1(x) = 
\begin{cases}
    0.5x^2 \qquad if|x|<1\\
    |x| - 0.5   \quad otherwise
\end{cases}</script><p>其中，$x=f(x_i)−y_i$ 为预测值和真实值的差值。</p>
<p>实际目标检测框回归任务中的损失loss为</p>
<script type="math/tex; mode=display">L(g,p) = \sum_{i\in{(x,y,w,h)}}smooth_{L1}(g_i - p_i)</script><p>其中 $g=(g_x,g_y,g_w,g_h)$ 表示Ground Truth的框坐标，  $p=(p_x,p_y,p_w,p_h)$ 表示Prediction的框坐标，即分别求4个点的loss，然后相加作为Bounding Box Regression Loss。</p>
<p><strong>对比 L1 Loss 和 L2 Loss</strong></p>
<p>L1 loss可以看做是平均绝对误差（Mean Absolute Error,MAE) 是指模型预测值f(x)和真实值y之间距离的平均值，其公式如下：</p>
<script type="math/tex; mode=display">M A E=\frac{\sum_{n=1}^{n}\left|f\left(x_{i}\right)-y_{i}\right|}{n}</script><p>其中，$y_i$和$f(x_i)$分别表示第i个样本的真实值及其对应的预测值，n为样本的个数。</p>
<p>忽略下标 i ，设n=1，以f(x)−y为横轴，MAE的值为纵轴，得到函数的图形如下：</p>
<p><img src="https://pic.downk.cc/item/5f905bbd1cd1bbb86b8f3fec.png"></p>
<p>相比于MSE，MAE有个优点就是，对于离群点不那么敏感。</p>
<p><img src="https://pic.downk.cc/item/5f905bd71cd1bbb86b8f479a.png"></p>
<p>L2 loss可以看作是均方误差（Mean Square Error,MSE）是模型预测值f(x) 与真实样本值y 之间差值平方的平均值，其公式如下;</p>
<script type="math/tex; mode=display">M S E=\frac{\sum_{i=1}^{n}\left(f_{x_{i}}-y_{i}\right)^{2}}{n}</script><p>忽略下标 i ，设n=1，以f(x)−y为横轴，MSE的值为纵轴，得到函数的图形如下：</p>
<p><img src="https://pic.downk.cc/item/5f905fd11cd1bbb86b90638f.png"></p>
<p>如果样本中存在离群点，MSE会给离群点更高的权重，这就会牺牲其他正常点数据的预测效果，最终降低整体的模型性能。 如下图：</p>
<p><img src="https://pic.downk.cc/item/5f905bfe1cd1bbb86b8f5299.png"></p>
<p>可见，使用 MSE 损失函数，受离群点的影响较大，虽然样本中只有 5 个离群点，但是拟合的直线还是比较偏向于离群点。</p>
<p>假设$x=f(x_i)−y_i$ ，则上面的公式可以概括为：</p>
<script type="math/tex; mode=display">L_1(x) = ± x</script><script type="math/tex; mode=display">L_2(x) = x^2</script><script type="math/tex; mode=display">SmoothL_1(x) = 
\begin{cases}
    0.5x^2 \qquad if|x|<1\\
    |x| - 0.5   \quad otherwise
\end{cases}</script><p>上面损失函数对$x$的导数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L_{1}(x)}{\partial x} &=\left\{\begin{array}{cl}
1 & \text { if } x \geq 0 \\
-1 & \text { otherwise }
\end{array}\right.\\

\frac{\partial L_{2}(x)}{\partial x} &=2 x \\

\frac{\partial \operatorname{smooth}_{L_{1}}(x)}{\partial x} &=\left\{\begin{array}{ll}
x & \text { if }|x|<1 \\
\pm 1 & \text { otherwise }
\end{array}\right.
\end{aligned}</script><p>上面导数可以看出：</p>
<p>对于L1导数,L1对x的导数为常数，在<strong>训练后期</strong>，预测值与ground truth差异很小时，L1的导数的绝对值仍然为1，而 learning rate 如果不变，损失函数将在稳定值附近波动，难以继续收敛以达到更高精度。</p>
<p>对于L2导数，当x增大时，L2的损失也增大。 这就导致在<strong>训练初期</strong>，预测值与 groud truth 差异过于大时，损失函数对预测值的梯度十分大，训练不稳定。</p>
<p>对于Smooth L1导数，Smotth L1在x较小时，对x的梯度也会变小。 而当x较大时，对x的梯度的上限为1，也不会太大以至于破坏网络参数。Smooth L1完美的避开了L1和L2作为损失函数的缺陷。</p>
<p><strong>L1 Loss ,L2 Loss以及SmoothL1 放在一起的函数曲线对比</strong></p>
<p><img src="https://pic.downk.cc/item/5f905c111cd1bbb86b8f5910.png"></p>
<p>从上面可以看出，该函数实际上就是一个分段函数，在[-1,1]之间实际上就是L2损失，这样解决了L1的不光滑问题，在[-1,1]区间外，实际上就是L1损失，这样就解决了离群点梯度爆炸的问题。</p>
<h2 id="IOU-Loss"><a href="#IOU-Loss" class="headerlink" title="IOU_Loss"></a><strong>IOU_Loss</strong></h2><p><img src=https://pic3.zhimg.com/80/v2-c812620791de642ccb7edcde9e1bd742_720w.jpg></p>
<p>可以看到IOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题:</p>
<p><img src=https://picb.zhimg.com/80/v2-e3d9a882dec6bb5847be80899bb98ea3_720w.jpg></p>
<p>问题1：即上图中状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。</p>
<p>问题2：即上图中状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。</p>
<p>因此2019年出现了GIOU_Loss来进行改进。</p>
<h2 id="GIOU-Loss-Generalized-IOU"><a href="#GIOU-Loss-Generalized-IOU" class="headerlink" title="GIOU_Loss(Generalized IOU)"></a><strong>GIOU_Loss(Generalized IOU)</strong></h2><p><img src=https://pic1.zhimg.com/80/v2-443123f1aa540f7dfdc84b233edcdc67_720w.jpg></p>
<p>GIOU的创新之处在于考虑了<strong>最小外接矩形</strong>和<strong>差集</strong>这两个指标，用差集除以最小外接矩形就能区分开IOU问题里的状态2和状态3了，状态3的差集除以最小外接矩形的值更小，所以状态3的GIOU更大，GIOU_LOSS更小。</p>
<p>但是GIOU还存在一种不足：</p>
<p><img src=https://pic1.zhimg.com/80/v2-49024c2ded9faafe7639c5207e575ed6_720w.jpg></p>
<p>问题：状态1、2、3都是 <strong>预测框在目标框内部</strong> 且 <strong>预测框大小一致</strong> 的情况，这时预测框和目标框的差集和最小外接矩形都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。<br>基于这个问题，2020年的AAAI出现了DIOU_Loss。</p>
<h2 id="DIOU-Loss-Distance-IoU"><a href="#DIOU-Loss-Distance-IoU" class="headerlink" title="DIOU_Loss(Distance-IoU)"></a><strong>DIOU_Loss(Distance-IoU)</strong></h2><p><img src=https://pic4.zhimg.com/80/v2-029f094658e87f441bf30c80cb8d07d0_720w.jpg></p>
<p>DIOU的定义和GIOU很像，相当于把 “差集面积/最小外接矩形面积” 替换成 “中心点距离平方/最小外接矩形对角线距离平方”，公式长这个样子：</p>
<script type="math/tex; mode=display">D I o U=I o U-\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}</script><p>但是DIOU仍然有缺点，那就是没有考虑到长宽比。</p>
<p><img src=https://pic4.zhimg.com/80/v2-22bf2e9c8a2fbbbb877e0f1ede69009f_720w.jpg></p>
<p>比如上面三种情况，目标框包裹预测框，预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的，但是这3种状态中，看起来应该是状态1最好吧。那应该怎么区别这3种状态的好坏呢？</p>
<p>针对这个问题，又提出了CIOU_Loss，不得不说，好的解决方案都是前人用时间一点一点积累起来的！</p>
<h2 id="CIOU-Loss-Complete-IoU"><a href="#CIOU-Loss-Complete-IoU" class="headerlink" title="CIOU_Loss(Complete-IoU)"></a><strong>CIOU_Loss(Complete-IoU)</strong></h2><p>CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子$\alpha v$，将预测框和目标框的长宽比都考虑了进去。</p>
<p>其中$v$是衡量长宽比一致性的参数，定义为：</p>
<script type="math/tex; mode=display">v=\frac{4}{\pi^{2}}\left(\arctan \frac{w^{g t}}{h^{g t}}-\arctan \frac{w}{h}\right)^{2}</script><p>$\alpha$ 是用于做trade-off的参数:</p>
<script type="math/tex; mode=display">\alpha=\frac{v}{(1-I o U)+v}</script><p>完整的定义为$L_{C I o U}=1-I o U+\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}+\alpha v$，</p>
<p>或者在我们这里是：$\mathrm{ClOU}_{-} \mathrm{Loss}=1 - \mathrm{CIOU}=1-\left(\mathrm{I} 0 \mathrm{U}-\frac{\text { Distance } 2^{2}}{\text { Distance }-\mathrm{C}^{2}}-\frac{v^{2}}{(1-\mathrm{IOU})+v}\right)$</p>
<p>这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。</p>
<p>再来综合的看下各个Loss函数的不同点：</p>
<p>IOU_Loss：主要考虑检测框和目标框重叠面积。</p>
<p>GIOU_Loss：在IOU的基础上，考虑了非包围情况下，两个矩形框形成的几何面积的信息。</p>
<p>DIOU_Loss：在IOU和GIOU的基础上，考虑包围情况下，两个矩形框中心点距离的信息。</p>
<p>CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的信息。</p>
<p>Yolov4中采用了CIOU_Loss的回归方式，使得预测框回归的精度更高一些。</p>
<p>上面的各种IOU不仅可以用来基于先验框的预测框的回归训练，还可以用在NMS上。常用的目标检测算法中，一般采用普通的NMS来进行预测框的筛选，Yolov4则借鉴上面D/CIOU来进行预测框的筛选，论文：<a href="https://arxiv.org/pdf/1911.08287.pdf">https://arxiv.org/pdf/1911.08287.pdf</a></p>
<p>将其中计算IOU的部分替换成DIOU的方式：</p>
<p>来看下实际的案例</p>
<p><img src=https://pic4.zhimg.com/80/v2-ddb336d26adb2a2e37415b6266c88ec6_720w.jpg></p>
<p>在上图重叠的摩托车检测中，黄色箭头所指的中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。</p>
<p>因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。</p>
<p>注意：有读者会有疑问，这里为什么不用CIOU_nms，而用DIOU_nms?</p>
<p>答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。</p>
<p>但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。</p>
<h2 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a><strong>Focal Loss</strong></h2><p>Focal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。<br>我们知道，anchor-based算法会产生大量的背景简单负样本 、少量的目标简单正样本、中量的背景目标过渡区域困难正、负样本。现在考虑数量最多的简单背景负样本，这些简单背景负样本很容易被分类为背景，所以它们的损失很小，但毕竟负样本数量太多，模型还是不得不考虑这些样本，所以最终的结果是，模型优化了大量背景负样本的损失，而忽视了优化那些目标样本的损失，也就是说优化重点跑偏了。同时因为背景负样本很容易分类，它们的损失已经低到不能再低了，所以还会导致模型的loss后期很难继续收敛。</p>
<p><img src="https://s1.ax1x.com/2020/10/02/0QAJG8.png" alt="0QAJG8.png" border="0" /></p>
<p>OHEM（online hard example mining）是后来针对正负样本不均衡提出的一种筛选example的方法，OHEM的主要思想可以用原文的一句话概括：In OHEM each example is scored by its loss, non-maximum suppression (NMS) is then applied, and a minibatch is constructed with the highest-loss examples。OHEM算法虽然增加了困难样本的权重，但是却忽略了容易分类的样本，结果它把所有的easy example都去除掉，造成无法用easy positive example进一步提升训练的精度。</p>
<p>于是后来2017年何凯明等人提出了Focal loss，他提出只要通过将原先训练回归任务常用的 交叉熵误差CE (Cross Entropy) 改为 FL (Focal Loss) 即可，首先回顾二分类交叉上损失：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{Cross Entropy}(y,y')=-\mathrm{ylog} y^{\prime}-(1-y) \log \left(1-y^{\prime}\right)=\left\{\begin{array}{ll}
-\log y^{\prime} \quad & y=1 \\
-\log \left(1-y^{\prime}\right) & y=0
\end{array}\right.</script><p>$y’$是样本属于正样本的概率值。可见普通的交叉熵对于正样本(y=1)而言，$y’$越大损失越小。对于负样本(y=0)而言，$y’$越小则损失越小。</p>
<p>下面我们来看一下Focal Loss的公式：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{FocalLoss}(y,y')=\left\{\begin{array}{ll}
-\alpha\left(1-y^{\prime}\right)^{\gamma} \log y^{\prime} & y=1 \\
-(1-\alpha) y^{\prime \gamma} \log \left(1-y^{\prime}\right) & y=0
\end{array}\right.</script><p>在公式中，α主要负责调节正负样本的失衡，γ主要负责调节难易样本的失衡。</p>
<p>先不管α，先思考一下γ的作用逻辑。对于正样本（y=1），其易分类的样本的$y’$趋近于1，所以$(1-y’)^\gamma$很小；对于负样本（y=0），其易分类的样本$y’$趋近于0，所以$y’^\gamma$也很小。合起来说就是，对于易分类的（well-classified）样本的权值都被调低了。</p>
<p>文章也对于 $\gamma,\alpha$ 的取值做了一番实验：</p>
<p><img src="https://s1.ax1x.com/2020/10/02/01EBm4.png" alt="01EBm4.png" border="0" /></p>
<p>在实验中，发现当γ=0的时候，α=0.75的效果最好，但γ不为0时，则是 $\gamma = 2, \alpha = 0.25$ 的取值组合效果最好，这一点是有点反直觉，因为正样本竟然被分配了一个更小的权重，我的理解是可能$y’^\gamma$对负样本的权重影响更大，$y’^\gamma$使得简单负样本的权重仍然很小。</p>
<p>为此，FAIR团队还专门写了一个简单的one-stage detector来验证focal loss的强大。并将该网络结构起名RetinaNet：</p>
<p><img src=https://pic1.zhimg.com/v2-f3ef4c6e92c2f6d9e2782e7e32361042_1440w.jpg></p>
<p>最后赋个具体的值来感受一下这个公式的作用：</p>
<p>假设我们有1000000个 $p_t = 0.99$ 的简单负样本（根据下图可以算出这个负样本的正样本概率为p=0.01，awesome）和 10个 $p_t=0.01$ 的困难正样本（根据下图可以算出这个正样本的正样本的概率只有p=0.01，awful）。alpha取0.25，则负样本的$\alpha_t$=1-α=0.75，正样本的$\alpha_t$=α=0.25。γ取2。负样本的$\alpha_{t}\left(1-p_{t}\right)^{\gamma}=0.000075$，正样本的$\alpha_{t}\left(1-p_{t}\right)^{\gamma}=0.245025$。</p>
<p><img src="https://s1.ax1x.com/2020/10/03/01umfs.png" alt="01umfs.png" border="0" width=70%/><br><img src="https://s1.ax1x.com/2020/10/03/01KpEF.png" alt="01KpEF.png" border="0" width=40%/></p>
<p>在使用普通交叉熵损失的情况下，来自负样本的损失为 1000000$\cdot$log(0.99)=1000000×0.0043648054=4364 正面例子的损失是 10$\cdot$log(0.01)=10×2=20。正例的损失贡献为20/(4364+20)=0.0046。几乎可以忽略不计。</p>
<p>在使用焦点损失的情况下，来自负面示例的损失为1000000×0.0043648054×0.000075=0.3274 正面例子的损失是 10×2×0.245025=4.901。正例的损失贡献为4.901/(4.901+0.3274)=0.9374！现在正例主宰了总损失！</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><blockquote>
<p><a href="https://www.cnblogs.com/wangguchangqing/p/12021638.html">《回归损失函数1：L1 loss, L2 loss以及Smooth L1 Loss的对比》</a><br><br><a href="https://zhuanlan.zhihu.com/p/143747206">《深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5核心基础知识完整讲解》</a><br><br><a href="https://blog.csdn.net/JNingWei/article/details/80038594">《论文阅读: RetinaNet》</a><br><br><a href="https://arxiv.org/pdf/1708.02002.pdf">《Focal Loss for Dense Object Detection》</a><br><br><a href="https://leimao.github.io/blog/Focal-Loss-Explained/">《Use Focal Loss To Train Model Using Imbalanced Dataset》</a></p>
</blockquote>
]]></content>
      <categories>
        <category>CV</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title>掌握期权交易</title>
    <url>/2020/09/12/%E6%8E%8C%E6%8F%A1%E6%9C%9F%E6%9D%83%E4%BA%A4%E6%98%93/</url>
    <content><![CDATA[<p>想要交易期权的门槛比较高，有几条要求是存款大于50万，通过期权知识测试，提供过往期权模拟交易证明等。现在我暂时交易不了，就当先储备知识。</p>
<p>推荐视频：《<a href="https://www.youtube.com/watch?v=IunTigepa1I&amp;list=PLN5R8P-UTWKOZqWpONYRTrjVc1FySLW03">秒懂期权系列-看完就赚！</a>》</p>
<h2 id="期权基本概念"><a href="#期权基本概念" class="headerlink" title="期权基本概念"></a>期权基本概念</h2><p><HR><br><strong>期权（Option）</strong>，是一种选择权，指是一种能在未来某特定时间（<strong>到期日：expiration data</strong>）以特定价格（<strong>行权价/敲定价：strike price</strong>）买入或卖出一定数量的某种特定商品（<strong>标的资产：underlying asset</strong>）的权利。期权的分类</p>
<p><strong>1、按期权的权利划分，有看涨期权和看跌期权两种类型。</strong></p>
<p>　　看涨期权（Call Options）是指期权的买方向期权的卖方支付一定数额的权利金后，即拥有在期权合约的有效期内，按事先约定的价格<strong>向期权卖方买入</strong>一定数量的期权合约规定的特定商品的权利，但不负有必须买进的义务。而<strong>期权卖方</strong>有义务在期权规定的有效期内，应期权买方的要求，以期权合约事先规定的价格<strong>卖出</strong>期权合约规定的特定商品。</p>
<p>　　看跌期权（Put Options）是指期权的买方向期权的卖方支付一定数额的权利金后，即拥有在期权合约的有效期内，按事先约定的价格<strong>向期权卖方卖出</strong>一定数量的期权合约规定的特定商品的权利，但不负有必须卖出的义务。而<strong>期权卖方</strong>有义务在期权规定的有效期内，应期权买方的要求，以期权合约事先规定的价格<strong>买入</strong>期权合约规定的特定商品。</p>
<p><strong>2、按期权的交割时间划分，有美式期权、欧式期权和百慕大期权三种类型。</strong></p>
<p>美式期权是指在期权合约规定的有效期内任何时候都可以行使权利。欧式期权是指在期权合约规定的到期日方可行使权利，期权的买方在合约到期日之前不能行使权利，过了期限，合约则自动作废。百慕大期权（Bermuda option）是一种可以在到期日前所规定的一系列时间段行权的期权，百慕大期权可以被视为美式期权与欧式期权的混合体，如同百慕大群岛混合了美国文化和英国文化一样。目前中国新兴的外汇期权业务，类似于欧式期权，但又有所不同。</p>
<p><strong>3、按期权合约上的标的划分，有股票期权、股指期权、利率期权、商品期权以及外汇期权等种类。</strong></p>
<h2 id="期权的构成要素"><a href="#期权的构成要素" class="headerlink" title="期权的构成要素"></a>期权的构成要素</h2><p>　　(1)执行价格，又称履约价格（strike price）：期权的买方行使权利时事先规定的标的物买卖价格。</p>
<p>　　(2)权利金（premium）： 期权的买方支付的期权价格，即买方为获得期权而付给期权卖方的费用。</p>
<p>　　(3)保证金：期权卖方必须存入交易所用于履约的财力担保。</p>
<p>　　(4)看涨期权（call option）和看跌期权（put option）：看涨期权，是指在期权合约有效期内按执行价格买进一定数量标的物的权利；看跌期权，是指卖出标的物的权利。</p>
<p>当期权买方预期<strong>标的物</strong>价格会<strong>超出执行价格</strong>时，他就会<strong>买进看涨期权</strong>，这样他就可以<strong>从期权卖方低价买入，再高价卖出到市场</strong>；</p>
<p>当期权买方预期<strong>标的物</strong>价格会<strong>低于执行价格</strong>时，他就会<strong>买进看跌期权</strong>，这样他就可以<strong>从市场低价买入，再高价卖出给期权卖方</strong>。</p>
<h2 id="期权的价格-Market-Price-of-Option"><a href="#期权的价格-Market-Price-of-Option" class="headerlink" title="期权的价格 (Market Price of Option)"></a>期权的价格 (Market Price of Option)</h2><p>期权的价格称为“权利金”或者“期权费”。权利金是期权合约中的唯一变量，期权合约上的其他要素，如：执行价格、合约到期日、交易品种、交易金额、交易时间、交易地点等要素都是在合约中事先规定好的，是标准化的，而期权的价格是是由交易者在交易所里竞价得出的。</p>
<p><strong>期权价格主要由内涵价值、时间价值两部分组成。</strong></p>
<p>　　1、内涵价值：（Intrinsic Value)</p>
<p>　　内涵价值指<strong>买方</strong>立即履行合约时可获取的总利润。具体来说，可以分为实值期权、虚值期权和两平期权。</p>
<p>　　（1）实值期权</p>
<p>　　当看涨期权的执行价格低于当时的实际价格时，或者当看跌期权的执行价格高于当时的实际价格时，该期权为实值期权。</p>
<p>　　（2）虚值期权</p>
<p>　　当看涨期权的执行价格高于当时的实际价格时，或者当看跌期权的执行价格低于当时的实际价格时，该期权为虚值期权。当期权为虚值期权时，内涵价值为零。</p>
<p>　　（3）两平期权</p>
<p>　　当看涨期权的执行价格等于当时的实际价格时，或者当看跌期权的执行价格等于当时的实际价格时，该期权为两平期权。当期权为两平期权时，内涵价值为零。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">看涨期权</th>
<th style="text-align:center">看跌期权</th>
<th style="text-align:center">结论</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">期权执行价格&lt;实际价格</td>
<td style="text-align:center">期权执行价格&gt;实际价格</td>
<td style="text-align:center">实值期权</td>
</tr>
<tr>
<td style="text-align:center">期权执行价格&gt;实际价格</td>
<td style="text-align:center">期权执行价格&lt;实际价格</td>
<td style="text-align:center">虚值期权</td>
</tr>
<tr>
<td style="text-align:center">期权执行价格=实际价格</td>
<td style="text-align:center">期权执行价格=实际价格</td>
<td style="text-align:center">两平期权</td>
</tr>
</tbody>
</table>
</div>
<p>2、时间价值(Time Value)</p>
<p>　　期权距到期日时间越长，大幅度价格变动的可能性越大，期权买方执行期权获利的机会也越大。与较短期的期权相比，期权买方对较长时间的期权的应付出更高的权利金。</p>
<p><img src=https://wiki.mbalib.com/w/images/5/53/%E6%9C%9F%E6%9D%83%E7%9A%84%E6%97%B6%E9%97%B4%E4%BB%B7%E5%80%BC1.jpg></p>
<p>　　期权的时间价值随着到期日的临近而减少，期权到期日的时间价值为零。</p>
<p>　　期权的时间价值反映了期权交易期间时间风险和价格波动风险，当合约0%或100%履约时，期权的时间价值为零。</p>
<p>综上所述，三种期权的价格表为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">看涨期权</th>
<th style="text-align:center">期权价格（未到期）</th>
<th style="text-align:center">期权价格（到期）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">实值期权</td>
<td style="text-align:center">内涵价值+时间价值</td>
<td style="text-align:center">内涵价值</td>
</tr>
<tr>
<td style="text-align:center">虚值期权</td>
<td style="text-align:center">时间价值</td>
<td style="text-align:center">零</td>
</tr>
<tr>
<td style="text-align:center">两平期权</td>
<td style="text-align:center">时间价值</td>
<td style="text-align:center">零</td>
</tr>
</tbody>
</table>
</div>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>　中国银行期权宝7月11日报价</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">标的汇率</th>
<th>期权类别</th>
<th>协定价格</th>
<th>到期日</th>
<th>期权费率（%）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">澳元/美元</td>
<td>看跌</td>
<td>0.6820</td>
<td>2003-7-23</td>
<td>4.34</td>
</tr>
<tr>
<td style="text-align:center">澳元/美元</td>
<td>看涨</td>
<td>0.6820</td>
<td>2003-7-23</td>
<td>0.30</td>
</tr>
</tbody>
</table>
</div>
<p>以上是澳元从7月5日0.6845大幅下跌至0.6486后的中国银行“期权宝”7月11日的报价。由于澳元出现了大幅下挫，7月11日执行价格为 0.6820、到期日为7月23日的澳元/美元看跌期权，由于离到期日只有12天，而期权实值非常大，因而其期权费报价达到4.34%的高价；而同样执行价格和到期日的看涨期权，由于其内涵价值为虚值，期权费报价却只有0.30%。从另一个角度来看，澳元/美元在大幅下跌后，市场普遍认为在短期内澳元/美元回到0.6820的可能性非常低，因此到期日非常近、执行价格较高的看跌期权的期权费自然高，而看涨期权的期权费则较低。</p>
<h2 id="期权基本交易方式"><a href="#期权基本交易方式" class="headerlink" title="期权基本交易方式"></a>期权基本交易方式</h2><p>从前面我们知道，期权可以分为看涨期权和看跌期权两种类型，而期权交易者又可有买入期权或者卖出期权两种操作，所以期权交易有四种基本策略：买进看涨期权、卖出看涨期权、买进看跌期权、卖出看跌期权。</p>
<p>　　1、买入看涨期权</p>
<p>　　若交易者买进看涨期权，之后市场价格果然上涨，且升至执行价格之上，则交易者可执行期权从而获利。从理论上说，价格可以无限上涨，所以买入看涨期权的盈利理论上是无限大。若到期一直未升到执行价格之上，则交易者可放弃期权，其最大损失为期权费（下图中的左上图）。</p>
<p>　　2、卖出看涨期权</p>
<p>　　若交易者卖出看涨期权，在到期日之前没能升至执行价格之上，则作为看涨期权的买方将会放弃期权，而看涨期权的卖方就会取得期权费的收入。反之，看涨期权的买方将会要求执行期权，期权的卖方的损失为：（市场价格-执行价格）-期权费（下图中的右上图）。</p>
<p>　　3、买入看跌期权</p>
<p>　　若交易者买进看跌期权，之后市场价格果然下跌，且跌至执行价格之下，则交易者可执行期权从而获利，由于价格不可能跌到负数，所以买入看跌期权的最大盈利为执行价格减去标的价格再减去期权费。若到期一直涨到执行价格之上，则交易者可放弃期权，其最大损失为期权费。投资者如果买了一只预感会下跌的股票，则可以同时买看跌期权对冲。</p>
<p>　　4、卖出看跌期权</p>
<p>　　若交易者卖出看跌期权，在到期日之前没能跌至执行价格之下，则作为看跌期权的买方将会放弃期权，而看跌期权的卖方就会取得期权费的收入。反之，看跌期权的买方将会要求执行期权，期权的卖方将损失执行价格减去市场价格和期权费的差。</p>
<p>期权交易盈亏图</p>
<p><img src="https://pic.downk.cc/item/5f7f2ce41cd1bbb86bd28cde.jpg"></p>
<p>总结来说：作为期权买入方，最大亏损为期权费。作为期权卖出方，最大盈利为期权费。<br>所以期权有点像保险产品，你买一份保险最大亏损为保险费，万一你出什么事了，就可以获取数倍理赔的收益。保险公司在大家没集体出事时，就是赚取众人交的保险费。</p>
<p>具体交易案例可见<a href="http://baijiahao.baidu.com/s?id=1661565680231378352&amp;wfr=spider&amp;for=pc">这篇文章</a>。</p>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><blockquote>
<p><a href="https://wiki.mbalib.com/wiki/%E6%9C%9F%E6%9D%83">《期权-智库百科》</a><br><br><a href="https://www.youtube.com/watch?v=FXFz3zJGLvg">《期权交易 | 应付下跌+把握上涨，同时抓！颠覆你对于投资的认知！》</a><br><br><a href="https://www.youtube.com/watch?v=xgFAKDdW1po">《股市太高，机会所剩无几？会赚钱的人，都在玩期权！4步骤，搞定期权交易！》</a><br><br><a href="https://www.youtube.com/watch?v=tozJLdS7FdA">《散户必看期权策略，教你正确抵抗下跌风险！Covered call &amp; Beta Hedging》</a></p>
</blockquote>
]]></content>
      <categories>
        <category>理财</category>
      </categories>
      <tags>
        <tag>期权</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习模型的压缩与加速</title>
    <url>/2020/08/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8E%8B%E7%BC%A9%E4%B8%8E%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<p>要在算力有限的移动端设备部署深度学习模型，必须对模型进行压缩和加速，不然那极慢的推理速度会让模型失去了它原本的作用和意义。同时，你在移动设备上运行深度学习模型，还会面临发热严重、耗电快的痛点。</p>
<p>从定义上来说，模型压缩就是让模型大小降低，比如一个tflite模型大小从16M降到4M，模型加速就是让模型推理速度更快，比如一个tflite模型在手机上目标检测一帧需要50mm快到只需要25ms。</p>
<h2 id="权重共享-参数共享"><a href="#权重共享-参数共享" class="headerlink" title="权重共享/参数共享"></a><strong>权重共享/参数共享</strong></h2><p>参数共享包括权重（weight）共享和偏置(bias)共享。</p>
<p>在我学习深度神经网络时，看到的已经时下面动图所示的原理，并想当然地认为深度神经网络一开始就是这么设计的</p>
<p><img src=https://pic2.zhimg.com/v2-54fbd436effac6d20d75afbeae1604e7_b.webp></p>
<p>简单点，就是下图这个样子：</p>
<p><img src=https://pic1.zhimg.com/50/v2-a35a29688a764b9ec5e438d53bc1d35a_hd.webp></p>
<p>其实一开始的深度神经网络是下图这样子的：</p>
<p><img src=https://pic4.zhimg.com/80/v2-5b79eb2d4ca654d514af834517889c28_720w.jpg></p>
<p>可以看到，图片上不同片区使用了不同的卷积核。</p>
<p>权重共享/参数共享的含义就是一张图片上使用同一个卷积核去滑动扫描。这意味着一个卷积核专门用来检测某一个特征，这些特征位于图片的不同位置。比如有一组weights和bias是某个局部感知域学到的用来识别一个垂直的边。那么预测的时候不管这条边在图片上哪个位置，它都会被这个局部感知域检测到。更具体一点，卷积网络能很好的适应目标在图片中的位置变化：把图片中的狗稍微移动一下位置，它仍然知道这是一只狗。</p>
<p>因为这个原因，我们有时把输入层到隐藏层的映射叫做特征映射(feature map)。我们把定义特征映射的权重叫做共享的权重(shared weights)，bias叫做共享的bias(shared bais)。这组weights和bias定义了一个kernel或者filter。</p>
<p>但是，这种一张图像上不同位置使用不同卷积核的思想不是永远没用或不行的。Facebook创建的的DeepFace网络使用了另一种类型的层来加快他们的训练并获得惊人的结果，该层称为权重未共享的本地连接层。</p>
<table><tr>
<td><img src=https://wx1.sbimg.cn/2020/08/29/61eSK.png border=0></td>
<td><img src=https://wx1.sbimg.cn/2020/08/29/61Whl.png border=0></td>
</tr></table>


<h2 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a><strong>分组卷积</strong></h2><p>AlexNet的Group Convolution，对输入进行分组，卷积核数量不变，但channel数减少，<a href="https://blog.csdn.net/qq_41554005/article/details/102722895">更多详细说明</a>：</p>
<p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL1pob3V6aXF1blpaUVpaUS9waWNCZWQvbWFzdGVyLzIwMTkxMDIzMTkyODU2LnBuZw?x-oss-process=image/format,png" alt="img"></p>
<h2 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a><strong>矩阵分解</strong></h2><p>矩阵分解本质上就是一个低秩因式分解的操作，将M x N的矩阵分解为M x K + K x N，只要让K &lt;&lt; M 且 K &lt;&lt; N，就可以大大降低模型体积。</p>
<p>在最初的BERT中，以Base为例，Embedding层的维度与隐藏层的维度一样都是768，但是我们知道，对于词的分布式表示，往往并不需要这么高的维度，比如在Word2Vec时代就多采用50或300这样的维度。那么一个很简单的思想就是，通过将Embedding部分分解来达到降低参数量的作用。</p>
<p><img src="https://wx1.sbimg.cn/2020/08/29/6usFJ.png" alt="6usFJ.png" border="0" /></p>
<p>V：词汇表(<strong>V</strong>ocabulary)大小 ；H：隐藏层(<strong>H</strong>idden)维度 ；E：词向量维度</p>
<p>上图中，Base中的隐藏层大小H=768， 词汇表大小V=3w，此时的参数量为：768 <em> 3w = 23040000。 后来出现的ALBERT就应用了矩阵分解的方法，它将 Embedding 的维度改为 128，那么此时Embedding层的参数量为：128 </em> 3w + 128 * 768 = 3938304。二者的参数量相差为19101696，大约为19M，Embedding参数量从BERT的23M变为了ALBERT的4M。</p>
<p><strong>使用小卷积核代替大卷积核</strong></p>
<p>AlexNet中用到了一些非常大的卷积核，比如11×11、7×7卷积核，之前人们的观念是，卷积核越大，receptive field（感受野）越大，看到的图片信息越多，因此获得的特征越好。虽说如此，但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在VGG（最早使用）、Inception网络中，利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳(如下图所示)，同时参数量（3×3×2+1 VS 5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。</p>
<ul>
<li><p>用两个3 <em> 3卷积核可以代替一个5 </em> 5卷积核</p>
</li>
<li><p>用三个3 <em> 3卷积核可以代替一个7 </em> 7卷积核</p>
</li>
</ul>
<p><img src="https://pic.downk.cc/item/5f4bcb42160a154a678a99c9.jpg" width=80%></p>
<h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a><strong>深度可分离卷积</strong></h2><p><img src=https://img-blog.csdnimg.cn/20190706183157843.png></p>
<p><img src=https://img-blog.csdnimg.cn/20190706184127821.png></p>
<p>卷积核参数量和计算量的对比：</p>
<p><img src=https://img-blog.csdnimg.cn/20190706184232915.png></p>
<p><img src=https://img-blog.csdnimg.cn/20190706184703985.png></p>
<p><img src=https://img-blog.csdnimg.cn/20190706185010762.png></p>
<p>可见参数量和乘加操作的运算量均下降为原来的</p>
<p>$1/N + 1/(D^2_k)$</p>
<p>我们通常所使用的是3×3的卷积核，也就是会下降到原来的九分之一到八分之一。</p>
<p>[<a href="https://blog.csdn.net/u010712012/article/details/94888053">图片来源</a>]</p>
<h2 id="全局平均值"><a href="#全局平均值" class="headerlink" title="全局平均值"></a><strong>全局平均值</strong></h2><p>以下图的MNIST神经网络为例，这是一个识别数字的简单网络<br><img src="https://wx1.sbimg.cn/2020/08/29/6u9wG.png" alt="6u9wG.png" border="0" /><br>我们看最后的全连接层，它就是用宽度为12、高度为12、通道数为20的卷积核重复操作100次卷积得到结果。对于输入的每一张图，用了一个和图像一样大小的核卷积，各个通道卷积之后得到一个数，将这20个数相加得到一个小球的结果，重复100次。这样就能把一张图高度浓缩成一个数了。</p>
<p>但是全连接的参数实在是太多了，你想这张图里就有12x12x20x100个参数。所以现在的趋势是尽量避免全连接，目前主流的一个方法是全局平均值。也就是最后那一层的feature map（最后一层卷积的输出结果），直接求平均值。有多少种分类就训练出多少层。</p>
<p>AlexNet和VGGNet中，全连接层几乎占据了90%的参数量。inceptionV1创造性的使用全局平均池化来代替最后的全连接层，使得其在网络结构更深的情况下（22层，AlexNet仅8层），参数量只有500万，仅为AlexNet的1/12。</p>
<h2 id="伪量化"><a href="#伪量化" class="headerlink" title="伪量化"></a><strong>伪量化</strong></h2><p>伪量化是指在保存模型每一层时，利用低精度（比如8bit）来保存每一个网络参数，同时也保存拉伸比例scale和零值对应的浮点数zero_point。推理阶段，利用如下公式来网络参数还原为32bit浮点：</p>
<p><img src="https://pic.downk.cc/item/5f4bcde0160a154a678b7470.jpg" width=80%></p>
<p>这个过程被称为伪量化。 伪量化之所以得名，是因为存储时使用了低精度进行量化，但推理时会还原为正常高精度。为什么推理时不仍然使用低精度呢？这是因为一方面框架层有些算子只支持浮点运算，需要专门实现算子定点化才行。另一方面，高精度推理准确率相对高一些。伪量化可以实现模型压缩，但对模型加速没有多大效果。</p>
<h2 id="定点化"><a href="#定点化" class="headerlink" title="定点化"></a><strong>定点化</strong></h2><p>与伪量化不同的是，定点化在推理时，不需要还原为浮点数。这需要框架实现算子的定点化运算支持。</p>
<h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a><strong>剪枝</strong></h2><p><img src="https://pic.downk.cc/item/5f4bd1d6160a154a678cca88.jpg" width=50%></p>
<p><strong>非结构化剪枝：</strong></p>
<p>非结构化修剪：对网络上任意位置的单个权重置零，这意味着删除神经元之间的单个连接（在密集层中）或删除卷积过滤器的单个权重（在卷积层中）。<br>请注意，生成的权重张量虽然稀疏了，但仍然保持其原始形状，这导致无法帮助模型降低推理成本，因为CPU、GPU并未针对稀疏矩阵乘法进行优化。</p>
<p><img src="https://pic.downk.cc/item/5f4bd29f160a154a678d145e.jpg"></p>
<p><strong>结构化剪枝：</strong></p>
<p>结构化修剪：通过删除张量的整行/列来减小重量张量的尺寸。这转化为去除神经元及其所有传入和传出连接（在密集层中）或整个卷积过滤器（在卷积层中）。</p>
<p><img src="https://pic.downk.cc/item/5f4bcf81160a154a678c0387.jpg"></p>
<p><strong>相关项目：</strong></p>
<p><a href="https://github.com/tanluren/yolov3-channel-and-layer-pruning/blob/master/train.py（相关博客：https://blog.csdn.net/weixin_41397123/article/details/103828931）">https://github.com/tanluren/yolov3-channel-and-layer-pruning/blob/master/train.py（相关博客：https://blog.csdn.net/weixin_41397123/article/details/103828931）</a></p>
<p><a href="https://github.com/Lam1360/YOLOv3-model-pruning">https://github.com/Lam1360/YOLOv3-model-pruning</a></p>
<p><a href="https://github.com/coldlarry/YOLOv3-complete-pruning">https://github.com/coldlarry/YOLOv3-complete-pruning</a>  （相关博客：<a href="https://zhuanlan.zhihu.com/p/153496637）">https://zhuanlan.zhihu.com/p/153496637）</a></p>
<p><a href="https://github.com/PengyiZhang/SlimYOLOv3">https://github.com/PengyiZhang/SlimYOLOv3</a></p>
<h2 id="论文研读"><a href="#论文研读" class="headerlink" title="论文研读"></a><strong>论文研读</strong></h2><h3 id="《Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks》"><a href="#《Learning-both-Weights-and-Connections-for-Efficient-Neural-Networks》" class="headerlink" title="《Learning both Weights and Connections for Efficient Neural Networks》"></a><strong>《Learning both Weights and Connections for Efficient Neural Networks》</strong></h3><p>【<a href="https://arxiv.org/abs/1506.02626v3">英文原文</a>】<br>【<a href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/">中文解读</a>】</p>
<h3 id="《Deep-Compression：Compressing-Deep-Neural-Networks-with-Pruning-Trained-Quantization-and-Huffman-coding》"><a href="#《Deep-Compression：Compressing-Deep-Neural-Networks-with-Pruning-Trained-Quantization-and-Huffman-coding》" class="headerlink" title="《Deep Compression：Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman coding》"></a><strong>《Deep Compression：Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman coding》</strong></h3><p>【<a href="https://arxiv.org/abs/1510.00149">英文原文</a>】<br>【<a href="https://blog.csdn.net/u011995719/article/details/79126531">中文解读</a>】</p>
<h2 id="《Learning-Structured-Sparsity-in-Deep-Neural-Networks》"><a href="#《Learning-Structured-Sparsity-in-Deep-Neural-Networks》" class="headerlink" title="《Learning Structured Sparsity in Deep Neural Networks》"></a><strong>《Learning Structured Sparsity in Deep Neural Networks》</strong></h2><p>【<a href="https://arxiv.org/abs/1608.03665">英文原文</a>】<br>【<a href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/">中文解读</a>】</p>
<h3 id="《Non-Structured-DNN-Weight-Pruning-–-Is-It-Beneficial-in-Any-Platform-》"><a href="#《Non-Structured-DNN-Weight-Pruning-–-Is-It-Beneficial-in-Any-Platform-》" class="headerlink" title="《Non-Structured DNN Weight Pruning – Is It Beneficial in Any Platform?》"></a><strong>《Non-Structured DNN Weight Pruning – Is It Beneficial in Any Platform?》</strong></h3><p>【<a href="https://arxiv.org/abs/1907.02124">英文原文</a>】</p>
<h1 id="轻量模型项目"><a href="#轻量模型项目" class="headerlink" title="轻量模型项目"></a>轻量模型项目</h1><ul>
<li><h2 id="Nanodet"><a href="#Nanodet" class="headerlink" title="Nanodet"></a>Nanodet</h2></li>
</ul>
<p><a href="https://github.com/RangiLyu/nanodet">Github项目地址</a></p>
<h3 id="训练Nanodet"><a href="#训练Nanodet" class="headerlink" title="训练Nanodet"></a>训练Nanodet</h3><p>目前nanodet的数据集格式是coco，我这边标好的数据集是yolo格式的，其他人可能是voc格式，所以需要转换。我找到两处yolo转coco的脚本<a href="https://www.programmersought.com/article/43131872583/">【1】</a><a href="https://github.com/RangiLyu/nanodet/issues/10#issuecomment-732726417">【2】</a>，但最终我用的是<a href="https://roboflow.com/convert/yolo-darknet-txt-to-coco-json">roboflow</a>，不过对于担心数据泄漏的问题，还请自己用脚本生成。</p>
<p>它的数据增强配置位于<code>config/nanodet-m.yml</code>：</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201124225411.png" alt="image-20201124225411418"></p>
<p>因为代码基于pytorch框架写的，所以用的数据增强也是基于torchvision的transforms实现的，更多图像增强说明请见：</p>
<p><a href="https://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py">torchvision transforms 源码</a></p>
<p><a href="https://pytorch.org/docs/stable/torchvision/transforms.html">torchvision transforms文档</a></p>
<p><a href="http://noahsnail.com/2020/06/12/2020-06-12-%E7%8E%A9%E8%BD%ACpytorch%E4%B8%AD%E7%9A%84torchvision.transforms/">博客1</a> <a href="https://www.it610.com/article/1275734689881014272.htm">博客2</a></p>
<p>perspective的效果就像下图（<a href="https://www.researchgate.net/publication/338184137_Perspective_Transformation_Data_Augmentation_for_Object_Detection">来源</a>）这样：</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201124234944.png" alt="image-20201124234943933" style="zoom:67%;" /></p>
<p>里面的normalize引起了我的兴趣。<a href="https://medium.com/@dibyadas/visualizing-different-normalization-techniques-84ea5cc8c378">《Visualizing Different Normalization Techniques》</a>这篇文章试验了3种图像归一化技巧</p>
<p>我写了个<a href="https://colab.research.google.com/drive/1CcHcE0qiwZ_HEyTSRJLS6NrbVMt5ko99">colab脚本</a>测试看看这个normalize会把输入图像处理成什么样子，结果是下面这个样子：</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201125143221.png" alt="image-20201125143221549"></p>
<p>有人做了一个可以体验Image Normalization效果的<a href="http://ipolcore.ipol.im/demo/clientApp/demo.html?id=77777000040">网页</a>，用它提供的几个样本图片效果比较明显，不过我用自己的图片测试基本看不出什么变化。</p>
<p>有位日本大哥维护着一个<a href="https://github.com/PINTO0309/PINTO_model_zoo">PINTO_model_zoo</a>，nanodet也成为了他的入选目标之一，它的仓库了提供了现成的coco类别的nanodet tflite model，不过我有个问题，就是如果我们自己训练了pytorch模型，要怎么转换成tflite格式，这位日本大哥在这个<a href="https://github.com/PINTO0309/PINTO_model_zoo/issues/55">issue</a>里耐心地回答了我，感谢！对于从pytorch格式转到tflite格式，网上应该也有其他办法，有没有更简短地转换路线有待研究，这位<a href="https://github.com/zldrobit">中国大哥</a>的一些repo就是关于tflite模型转换的。</p>
<h2 id="参考阅读"><a href="#参考阅读" class="headerlink" title="参考阅读"></a><strong>参考阅读</strong></h2><p><a href="https://zhuanlan.zhihu.com/p/179945324">《一文深入 - 深度学习模型压缩和加速》</a></p>
<p><a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/">《论文总结 - 模型剪枝 Model Pruning》</a></p>
<p><a href="http://www.easemob.com/press/535">《环信人工智能专家李理：详解卷积神经网络》</a></p>
<p><a href="https://amitness.com/2020/02/albert-visual-summary/">《Visual Paper Sumgailunwenmary: ALBERT (A Lite BERT)》</a></p>
<p><a href="https://prateekvjoshi.com/2016/04/12/understanding-locally-connected-layers-in-convolutional-neural-networks/">《Understanding Locally Connected Layers In Convolutional Neural Networks》</a></p>
<p><a href="https://www.zhihu.com/column/zhangxiaolongOptimization">《AI移动端优化》</a></p>
]]></content>
      <tags>
        <tag>CV</tag>
      </tags>
  </entry>
  <entry>
    <title>C++学习笔记</title>
    <url>/2020/08/23/C++%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h2 id="1-为什么源文件-cpp中要引用它自己的头文件-h-hpp？"><a href="#1-为什么源文件-cpp中要引用它自己的头文件-h-hpp？" class="headerlink" title="1.为什么源文件.cpp中要引用它自己的头文件.h/.hpp？"></a><strong>1.为什么源文件.cpp中要引用它自己的头文件.h/.hpp？</strong></h2><p><a href="https://stackoverflow.com/questions/30817570/whats-the-benefit-for-a-c-source-file-include-its-own-header-file">回答出处</a></p>
<p>假设我们的工程中有下面3个文件，<strong>main.c</strong>里有 <strong>#include “foo.h”</strong> 好理解，那为什么 <strong>foo.c</strong> 里也需要有 <strong>#include “foo.h”</strong> 呢？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* foo.h *&#x2F;</span><br><span class="line">#ifndef FOO_H</span><br><span class="line">#define FOO_H</span><br><span class="line">double foo( int x );</span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* foo.c *&#x2F;</span><br><span class="line">int foo( int x )</span><br><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;* main.c *&#x2F;</span><br><span class="line">#include &quot;foo.h&quot;</span><br><span class="line"></span><br><span class="line">int main( void )</span><br><span class="line">&#123;</span><br><span class="line">  double x &#x3D; foo( 1 );</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意，<code>foo.h</code>中的函数声明与<code>foo.c</code> 中的函数定义不匹配 : 返回类型不同 ! <code>foo.h</code> 里的foo函数声明返回的数据类型是<strong>double</strong>，而 <code>foo.c</code> 中foo函数定义返回的数据类型是 <strong>int</strong> ！</p>
<p>如下图所示，<code>foo.c</code> 和 <code>main.c</code> 彼此分开编译。由于main.c调用已在 <code>foo.h</code> 中声明的foo函数，因此 <code>main.o</code> 能编译成功。类似的，由于 <code>foo.c</code> 里没有 include  <code>foo.h</code>，编译器不知道声明和定义的函数类型不匹配，所以 <code>foo.o</code> (相当于下图的 <code>add.o</code> )也能编译成功。。。</p>
<p><img src=https://www.learncpp.com/images/CppTutorial/Section1/IncludeHeader.png></p>
<p>但当最后你要将两个目标文件（.o/.obj）链接在一起时，问题就来了，两处的函数返回类型不一致终将导致报错。</p>
<p>而如果你有把 <code>foo.h</code> 包含在 <code>foo.c</code> 里，那么在你运行程序之前，编译器就能发现这个不匹配的问题，让你及时debug掉这个问题。</p>
<p>还有，如果 <code>foo.h</code> 定义了 <code>foo.c</code> 所使用的任何类型或常量，那么这时候<code>foo.c</code>肯定需要包含 <code>foo.h</code> 。</p>
]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>在Android中使用OpenCV</title>
    <url>/2020/08/17/%E5%9C%A8Android%E4%B8%AD%E4%BD%BF%E7%94%A8OpenCV/</url>
    <content><![CDATA[<h1 id="在Android中使用OpenCV-C"><a href="#在Android中使用OpenCV-C" class="headerlink" title="在Android中使用OpenCV C++"></a>在Android中使用OpenCV C++</h1><blockquote>
<p>本部分内容大部分翻译自<a href="https://www.thecodingnotebook.com/2020/04/image-processing-with-opencv-in-android.html">《OpenCV in Android native using C++》</a></p>
</blockquote>
<p>在本文中，我们将介绍如何在Android项目上使用C++版的OpenCV，以及如何在 Java / Kotlin 和 c++ 代码之间传递图像。</p>
<p><img src=https://1.bp.blogspot.com/-p8Ms-R-fCQc/XoOr-o8PLgI/AAAAAAAAA64/_kXEzvP0txMaZxMMiRqvWfMjZVlbAFc_wCNcBGAsYHQ/s1600/androidopencv.jpg width=80%></p>
<p><strong><a href="https://github.com/ValYouW/AndroidOpenCVDemo">GitHub上的源代码</a></strong> </p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Sn3YhfY5jqg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<span id="more"></span>
<h2 id="1-下载OpenCV-Android-SDK"><a href="#1-下载OpenCV-Android-SDK" class="headerlink" title="1. 下载OpenCV Android SDK"></a><strong>1. 下载OpenCV Android SDK</strong></h2><p>前往 <a href="https://opencv.org/releases/">https://opencv.org/releases/</a> 下载OpenCV Android SDK：<br><img src="https://s1.ax1x.com/2020/08/17/dnPmon.png" alt="dnPmon.png" border="0" /><br>下载好后是这样的，我的路径是：<code>E:\OpenCV-4.4.0-android-sdk</code><br><img src="https://pic.downk.cc/item/5f426c31160a154a676d6802.jpg"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">so文件所在: sdk -&gt; native -&gt; libs</span><br><span class="line">c++的代码 : sdk -&gt; native -&gt; jni -&gt; include -&gt; opencv2 </span><br></pre></td></tr></table></figure>
<h2 id="2-设置全局环境变量"><a href="#2-设置全局环境变量" class="headerlink" title="2. 设置全局环境变量"></a><strong>2. 设置全局环境变量</strong></h2><p>定义一个<strong>OPENCV_ANDROID</strong>指向opencv android sdk（即<code>E:\OpenCV-4.4.0-android-sdk</code>）的根文件夹的全局环境变量。<br><img src="https://pic.downk.cc/item/5f426d49160a154a676dd676.jpg"></p>
<h2 id="3-创建原生安卓工程"><a href="#3-创建原生安卓工程" class="headerlink" title="3.创建原生安卓工程"></a><strong>3.创建原生安卓工程</strong></h2><p><img src="https://pic.downk.cc/item/5f4273d5160a154a6770835b.jpg" width=80%></p>
<p>C++ standard 选择默认的 “Toolchain Default”</p>
<p><img src="https://pic.downk.cc/item/5f42740c160a154a677098ae.jpg" width=80%></p>
<h2 id="4-添加一些代码"><a href="#4-添加一些代码" class="headerlink" title="4.添加一些代码"></a><strong>4.添加一些代码</strong></h2><p>使用“ Android”视图查看项目树，您会注意到我们有一个“cpp”文件夹，里面有Android Studio自己生成的2个文件：</p>
<ul>
<li>CMakeLists.txt -原生代码的构建指令</li>
<li>native-lib.cpp -将包含Kotlin / Java 和 c++ 环境之间的“桥”（JNI）代码。</li>
</ul>
<p>尽管将所有代码都写在<code>native-lib.cpp</code>内部是完全“合规”的，但我们将使该文件仅包含与jni相关的方法，而我们将在另一个文件上进行真正的C++图像处理。</p>
<ul>
<li>右键单击该cpp文件夹，然后选择New —&gt; C/C++ Header file，将其命名opencv-utils.h</li>
<li>右键单击该cpp文件夹，然后选择New —&gt; C/C++ Source file，将其命名opencv-utils.cpp</li>
</ul>
<p>项目树应如下所示：</p>
<p><img src="https://pic.downk.cc/item/5f427a5a160a154a6773ebf9.jpg"></p>
<h2 id="5-配置CMakeLists"><a href="#5-配置CMakeLists" class="headerlink" title="5.配置CMakeLists"></a><strong>5.配置CMakeLists</strong></h2><p>打开<code>CMakeLists.txt</code>并在“cmake_minimum_required()”后面添加以下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># opencv</span><br><span class="line">set(OpenCV_STATIC ON)</span><br><span class="line">set(OpenCV_DIR $ENV&#123;OPENCV_ANDROID&#125;&#x2F;sdk&#x2F;native&#x2F;jni)</span><br><span class="line">find_package(OpenCV REQUIRED)</span><br></pre></td></tr></table></figure>
<p>请注意这一行<code>set(OpenCV_DIR $ENV&#123;OPENCV_ANDROID&#125;/sdk/native/jni)</code> 使用了上面定义的环境变量指向opencv sdk，因此请仔细检查它是否指向正确的位置。 </p>
<p>find_package的作用是在电脑上找到后缀名为.cmake的文件，以此来获取其中的引用路径(.h文件的路径)和库的路径(例如“ /usr/lib/x86_64-linux-gnu/libopencv_video.so”)，所以<code>find_package(OpenCV REQUIRED)</code>会在电脑上中找到<strong>OpenCVConfig.cmake</strong> ： <code>&quot;E:\OpenCV-4.4.0-android-sdk\sdk\native\jni\OpenCVConfig.cmake&quot;</code>，该文件定义了<strong>OpenCV_INCLUDE_DIRS</strong>和<strong>OpenCV_LIBS</strong>等变量，从而后面的include_directories和target_link_libraries就可以使用/访问这两个变量。<br><code>find_package(OpenCV REQUIRED)</code>也可以说是用来添加OPENCV库的，如果要指定OpenCV版本，则代码变成<code>find_package(OpenCV 4.3 REQUIRED)</code>。<br><img src="https://pic.downk.cc/item/5f429f65160a154a67844f37.jpg"></p>
<p>接下来，我们应该添加前面创建的源文件<code>opencv-utils.cpp</code>，以便对其进行编译。在<code>CMakeLists.txt</code>里向下滚动找到native-lib.cpp，同样都是cpp源文件，<code>native-lib.cpp</code>要编译的话，<code>opencv-utils.cpp</code>也要编译，所以两个cpp都放进add_library()里，即：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add_library( # Sets the name of the library.</span><br><span class="line">        native-lib</span><br><span class="line"></span><br><span class="line">        # Sets the library as a shared library.</span><br><span class="line">        SHARED</span><br><span class="line"></span><br><span class="line">        # Provides a relative path to your source file(s).</span><br><span class="line">        opencv-utils.cpp</span><br><span class="line">        native-lib.cpp)</span><br></pre></td></tr></table></figure>
<p>这个add_library的作用就是定义一个叫native-lib的库，以及该库包含哪些源文件。另外从它里面的SHARED可以知道，这些源文件最后会被编译成shared object（so）。</p>
<p>接着滚轮往下滚，添加下面的代码，这个意思是在NDK中查找jnigraphics库，并取它的路径变量名为jnigraphics-lib, jnigraphics包含图形操作的库:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># jnigraphics lib from NDK is used for Bitmap manipulation in native code</span><br><span class="line">find_library(jnigraphics-lib jnigraphics)</span><br></pre></td></tr></table></figure>
<p>Android NDK一般位于<code>C:\Users\[username]\AppData\Local\Android\Sdk\ndk-bundle.</code>感兴趣的朋友可以找一下有没有这个jnigraphics。</p>
<p>正如find_library这个词汇的词性所表征，它只能找一个库，不能同时找多个库，虽然官网有示例说可以加入option实现，但似乎有坑难以成功。</p>
<p>最后，我们必须在CMake链接过程中告诉它目标链接库有OpenCV库和jnigraphics库，所以更改target_link_libraries为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">target_link_libraries( # Specifies the target library.</span><br><span class="line">        native-lib</span><br><span class="line"></span><br><span class="line">        $&#123;OpenCV_LIBS&#125;</span><br><span class="line">        $&#123;jnigraphics-lib&#125;</span><br><span class="line">        # Links the target library to the log library</span><br><span class="line">        # included in the NDK.</span><br><span class="line">        $&#123;log-lib&#125;)</span><br></pre></td></tr></table></figure>
<p>jnigraphics-lib就是前面find_library定义的一个路径变量名，然后这时候要加上<code>OpenCV_LIBS</code>不要漏了。</p>
<h2 id="6-同步与构建"><a href="#6-同步与构建" class="headerlink" title="6.同步与构建"></a><strong>6.同步与构建</strong></h2><p>配置好上面的CMakeLists，那就把项目按这个配置同步一下，并重新构建项目：</p>
<ul>
<li>File —&gt; Sync Project with Gradle Files</li>
<li>Build —&gt; Make Project</li>
</ul>
<p>如果构建成功，那就太好了！如果您遇到<code>Error computing CMake server result</code>的错误的话，对我有用的办法是删除app/build.gradle中定义的cmake版本：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">externalNativeBuild &#123;</span><br><span class="line">    cmake &#123;</span><br><span class="line">        path &quot;src&#x2F;main&#x2F;cpp&#x2F;CMakeLists.txt&quot;</span><br><span class="line">        version &quot;3.10.2&quot; # &lt;&lt;-- REMOVE THIS LINE</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-开始使用OpenCV"><a href="#7-开始使用OpenCV" class="headerlink" title="7.开始使用OpenCV"></a><strong>7.开始使用OpenCV</strong></h2><p><strong>翻转和模糊</strong></p>
<p>出于演示目的，我们的应用程序将使用OpenCV翻转和模糊图像，将以下内容添加到<code>opencv-utils.h</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#pragma once</span><br><span class="line"></span><br><span class="line">#include &lt;opencv2&#x2F;core.hpp&gt;</span><br><span class="line"></span><br><span class="line">using namespace cv;</span><br><span class="line"></span><br><span class="line">void myFlip(Mat src);</span><br><span class="line">void myBlur(Mat src, float sigma);</span><br></pre></td></tr></table></figure>
<p>如果遇到Android Studio将这些include opencv2内容标记为红色，没事，构建项目后就好了。</p>
<p>接着，<code>opencv-utils.cpp</code>里面的实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#include &quot;opencv-utils.h&quot;</span><br><span class="line">#include &lt;opencv2&#x2F;imgproc.hpp&gt;</span><br><span class="line"></span><br><span class="line">void myFlip(Mat src) &#123;</span><br><span class="line">    flip(src, src, 0);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void myBlur(Mat src, float sigma) &#123;</span><br><span class="line">    GaussianBlur(src, src, Size(), sigma);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>将C++代码暴露给kotlin/java代码</strong></p>
<p>接下来，我们需要将flip 和 blur方法暴露给“托管”代码，这将发生在native-lib.cpp内部，我们将不讨论有关如何公开方法的jni标准和规则，我们将复制预定义的stringFromJNI方法以用作模板，因此，在我的情况下，Android Studio创建了以下方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">extern &quot;C&quot; JNIEXPORT jstring JNICALL</span><br><span class="line">Java_com_vyw_opencv_1demo_MainActivity_stringFromJNI(...)&#123;...&#125;</span><br></pre></td></tr></table></figure>
<p>注意,方法名称由Java + activity名称空间 + 方法名 组成。</p>
<p>因此我们的方法是下面的样子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">extern &quot;C&quot; JNIEXPORT void JNICALL</span><br><span class="line">Java_com_vyw_opencv_1demo_MainActivity_flip(JNIEnv* env, jobject p_this, jobject bitmapIn, jobject bitmapOut) &#123;</span><br><span class="line">    Mat src;</span><br><span class="line">    bitmapToMat(env, bitmapIn, src, false);</span><br><span class="line">    &#x2F;&#x2F; NOTE bitmapToMat returns Mat in RGBA format, if needed convert to BGRA using cvtColor</span><br><span class="line"></span><br><span class="line">    myFlip(src);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; NOTE matToBitmap expects Mat in GRAY&#x2F;RGB(A) format, if needed convert using cvtColor</span><br><span class="line">    matToBitmap(env, src, bitmapOut, false);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">extern &quot;C&quot; JNIEXPORT void JNICALL</span><br><span class="line">Java_com_vyw_opencv_1demo_MainActivity_blur(JNIEnv* env, jobject p_this, jobject bitmapIn, jobject bitmapOut, jfloat sigma) &#123;</span><br><span class="line">    Mat src;</span><br><span class="line">    bitmapToMat(env, bitmapIn, src, false);</span><br><span class="line">    myBlur(src, sigma);</span><br><span class="line">    matToBitmap(env, src, bitmapOut, false);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>代码几乎是不言自明的，bitmapToMat和matToBitmap，顾名思义，这两种方法在Android Bitmap和OpenCV Mat类之间进行转换，这些方法取自OpenCV源<a href="https://github.com/opencv/opencv/blob/master/modules/java/generator/src/cpp/utils.cpp">opencv / modules / java / generator / src / cpp / utils.cpp</a>，并进行了一些细微调整。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bitmapToMat</span><span class="params">(JNIEnv *env, jobject bitmap, Mat&amp; dst, jboolean needUnPremultiplyAlpha)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    AndroidBitmapInfo  info;</span><br><span class="line">    <span class="keyword">void</span>*              pixels = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        CV_Assert( AndroidBitmap_getInfo(env, bitmap, &amp;info) &gt;= <span class="number">0</span> );</span><br><span class="line">        CV_Assert( info.format == ANDROID_BITMAP_FORMAT_RGBA_8888 ||</span><br><span class="line">                   info.format == ANDROID_BITMAP_FORMAT_RGB_565 );</span><br><span class="line">        CV_Assert( AndroidBitmap_lockPixels(env, bitmap, &amp;pixels) &gt;= <span class="number">0</span> );</span><br><span class="line">        CV_Assert( pixels );</span><br><span class="line">        dst.create(info.height, info.width, CV_8UC4);</span><br><span class="line">        <span class="keyword">if</span>( info.format == ANDROID_BITMAP_FORMAT_RGBA_8888 )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="function">Mat <span class="title">tmp</span><span class="params">(info.height, info.width, CV_8UC4, pixels)</span></span>;</span><br><span class="line">            <span class="keyword">if</span>(needUnPremultiplyAlpha) cvtColor(tmp, dst, COLOR_mRGBA2RGBA);</span><br><span class="line">            <span class="keyword">else</span> tmp.copyTo(dst);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// info.format == ANDROID_BITMAP_FORMAT_RGB_565</span></span><br><span class="line">            <span class="function">Mat <span class="title">tmp</span><span class="params">(info.height, info.width, CV_8UC2, pixels)</span></span>;</span><br><span class="line">            cvtColor(tmp, dst, COLOR_BGR5652RGBA);</span><br><span class="line">        &#125;</span><br><span class="line">        AndroidBitmap_unlockPixels(env, bitmap);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span>(<span class="keyword">const</span> cv::Exception&amp; e) &#123;</span><br><span class="line">        AndroidBitmap_unlockPixels(env, bitmap);</span><br><span class="line">        jclass je = env-&gt;FindClass(<span class="string">&quot;java/lang/Exception&quot;</span>);</span><br><span class="line">        env-&gt;ThrowNew(je, e.what());</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (...) &#123;</span><br><span class="line">        AndroidBitmap_unlockPixels(env, bitmap);</span><br><span class="line">        jclass je = env-&gt;FindClass(<span class="string">&quot;java/lang/Exception&quot;</span>);</span><br><span class="line">        env-&gt;ThrowNew(je, <span class="string">&quot;Unknown exception in JNI code &#123;nBitmapToMat&#125;&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">matToBitmap</span><span class="params">(JNIEnv* env, Mat src, jobject bitmap, jboolean needPremultiplyAlpha)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    AndroidBitmapInfo  info;</span><br><span class="line">    <span class="keyword">void</span>*              pixels = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        CV_Assert( AndroidBitmap_getInfo(env, bitmap, &amp;info) &gt;= <span class="number">0</span> );</span><br><span class="line">        CV_Assert( info.format == ANDROID_BITMAP_FORMAT_RGBA_8888 ||</span><br><span class="line">                   info.format == ANDROID_BITMAP_FORMAT_RGB_565 );</span><br><span class="line">        CV_Assert( src.dims == <span class="number">2</span> &amp;&amp; info.height == (uint32_t)src.rows &amp;&amp; info.width == (uint32_t)src.cols );</span><br><span class="line">        CV_Assert( src.type() == CV_8UC1 || src.type() == CV_8UC3 || src.type() == CV_8UC4 );</span><br><span class="line">        CV_Assert( AndroidBitmap_lockPixels(env, bitmap, &amp;pixels) &gt;= <span class="number">0</span> );</span><br><span class="line">        CV_Assert( pixels );</span><br><span class="line">        <span class="keyword">if</span>( info.format == ANDROID_BITMAP_FORMAT_RGBA_8888 )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="function">Mat <span class="title">tmp</span><span class="params">(info.height, info.width, CV_8UC4, pixels)</span></span>;</span><br><span class="line">            <span class="keyword">if</span>(src.type() == CV_8UC1)</span><br><span class="line">            &#123;</span><br><span class="line">                cvtColor(src, tmp, COLOR_GRAY2RGBA);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(src.type() == CV_8UC3)&#123;</span><br><span class="line">                cvtColor(src, tmp, COLOR_RGB2RGBA);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(src.type() == CV_8UC4)&#123;</span><br><span class="line">                <span class="keyword">if</span>(needPremultiplyAlpha) cvtColor(src, tmp, COLOR_RGBA2mRGBA);</span><br><span class="line">                <span class="keyword">else</span> src.copyTo(tmp);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// info.format == ANDROID_BITMAP_FORMAT_RGB_565</span></span><br><span class="line">            <span class="function">Mat <span class="title">tmp</span><span class="params">(info.height, info.width, CV_8UC2, pixels)</span></span>;</span><br><span class="line">            <span class="keyword">if</span>(src.type() == CV_8UC1)</span><br><span class="line">            &#123;</span><br><span class="line">                cvtColor(src, tmp, COLOR_GRAY2BGR565);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(src.type() == CV_8UC3)&#123;</span><br><span class="line">                cvtColor(src, tmp, COLOR_RGB2BGR565);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span>(src.type() == CV_8UC4)&#123;</span><br><span class="line">                cvtColor(src, tmp, COLOR_RGBA2BGR565);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        AndroidBitmap_unlockPixels(env, bitmap);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span>(<span class="keyword">const</span> cv::Exception&amp; e) &#123;</span><br><span class="line">        AndroidBitmap_unlockPixels(env, bitmap);</span><br><span class="line">        jclass je = env-&gt;FindClass(<span class="string">&quot;java/lang/Exception&quot;</span>);</span><br><span class="line">        env-&gt;ThrowNew(je, e.what());</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (...) &#123;</span><br><span class="line">        AndroidBitmap_unlockPixels(env, bitmap);</span><br><span class="line">        jclass je = env-&gt;FindClass(<span class="string">&quot;java/lang/Exception&quot;</span>);</span><br><span class="line">        env-&gt;ThrowNew(je, <span class="string">&quot;Unknown exception in JNI code &#123;nMatToBitmap&#125;&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="8-从kotlin-java里调用C"><a href="#8-从kotlin-java里调用C" class="headerlink" title="8.从kotlin/java里调用C++"></a><strong>8.从kotlin/java里调用C++</strong></h2><p>我们到达了演示的最后部分 : 从MainActivity.kt里调用C++方法。</p>
<p>首先，我在res/drawable-nodpi文件夹中添加了一张示例图像mountain.jpg（您可能需要自己创建它），我选择了这种no dpi风格，因为我不想让Android放大图像，我使用了一个相对较小的图像（640x427），从而可以实时地地进行模糊处理。</p>
<p>然后我用以下命令设置MainActivity的UI视图：</p>
<ul>
<li>ImageView-预加载的测试图像为 app:srcCompat=”@drawable/mountain”</li>
<li>Button-将用于翻转图像</li>
<li>SeekBar-将用于控制模糊西格玛，从0到100（以后的代码将转换为在1-10时浮动）</li>
</ul>
<p><strong>声明JNI方法</strong></p>
<p>为了使用<code>native-lib.cpp</code>中的方法，我们需要在activity中将它们声明为external函数，并且需要加载我们的native-lib库（libnative-lib.so），如果您是使用“ Native C ++”模板创建了项目，那么它理应就写好了。请滚动至最底部MainActivity.kt，添加blur和flip，如下所示：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">external fun <span class="title">stringFromJNI</span><span class="params">()</span>: String</span></span><br><span class="line"><span class="function">external fun <span class="title">blur</span><span class="params">(bitmapIn: Bitmap, bitmapOut: Bitmap, sigma: Float)</span></span></span><br><span class="line"><span class="function">external fun <span class="title">flip</span><span class="params">(bitmapIn: Bitmap, bitmapOut: Bitmap)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">companion object </span>&#123;</span><br><span class="line">    <span class="comment">// Used to load the &#x27;native-lib&#x27; library on application startup.</span></span><br><span class="line">    init &#123;</span><br><span class="line">        System.loadLibrary(<span class="string">&quot;native-lib&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在java中，则是这样：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * A native method that is implemented by the &#x27;native-lib&#x27; native library,</span></span><br><span class="line"><span class="comment">     * which is packaged with this application.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">native</span> String <span class="title">stringFromJNI</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>处理Android位图</strong></p>
<p>现在，我们将翻转并模糊ImageView位图，首先让我们创建2个位图，第一个将保存原始图像（srcBitmap），另一个将用作目标位图（dstBitmap），它可在屏幕上被看到。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">class MainActivity : AppCompatActivity(), SeekBar.OnSeekBarChangeListener &#123;</span><br><span class="line">    <span class="keyword">var</span> srcBitmap: Bitmap? = <span class="keyword">null</span></span><br><span class="line">    <span class="keyword">var</span> dstBitmap: Bitmap? = <span class="keyword">null</span></span><br><span class="line"></span><br><span class="line">    <span class="function">override fun <span class="title">onCreate</span><span class="params">(savedInstanceState: Bundle?)</span> </span>&#123;</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Load the original image</span></span><br><span class="line">        srcBitmap = BitmapFactory.decodeResource(<span class="keyword">this</span>.resources, R.drawable.mountain)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Create and display dstBitmap in image view, we will keep updating</span></span><br><span class="line">        <span class="comment">// dstBitmap and the changes will be displayed on screen</span></span><br><span class="line">        dstBitmap = srcBitmap!!.copy(srcBitmap!!.config, <span class="keyword">true</span>)</span><br><span class="line">        imageView.setImageBitmap(dstBitmap)</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>每当用户移动Seakbar时，我们将使用Seakbar值作为sigma来模糊图像：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// SeekBar event handler</span></span><br><span class="line"><span class="function">override fun <span class="title">onProgressChanged</span><span class="params">(seekBar: SeekBar?, progress: Int, fromUser: Boolean)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.doBlur()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">fun <span class="title">doBlur</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// The SeekBar range is 0-100 convert it to 0.1-10</span></span><br><span class="line">    val sigma = max(<span class="number">0.1F</span>, sldSigma.progress / <span class="number">10F</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This is the actual call to the blur method inside native-lib.cpp</span></span><br><span class="line">    <span class="keyword">this</span>.blur(srcBitmap!!, dstBitmap!!, sigma)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后，我们有了翻转按钮的事件处理程序：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">fun <span class="title">btnFlip_click</span><span class="params">(view: View)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// This is the actual call to the blur method inside native-lib.cpp</span></span><br><span class="line">    <span class="comment">// note we flip srcBitmap (which is not displayed) and then call doBlur which will</span></span><br><span class="line">    <span class="comment">// eventually update dstBitmap (and which is displayed)</span></span><br><span class="line">    <span class="keyword">this</span>.flip(srcBitmap!!, srcBitmap!!)</span><br><span class="line">    <span class="keyword">this</span>.doBlur()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>参考阅读：<br><br><a href="https://www.jianshu.com/p/90d5ae8ae6c0">《拥抱 C/C++ : Android JNI 的使用》</a><br><br><a href="https://developer.android.com/ndk/guides/stable_apis?hl=zh-cn">《Android NDK 原生 API》</a></p>
</blockquote>
<h1 id="在Android中使用OpenCV-java"><a href="#在Android中使用OpenCV-java" class="headerlink" title="在Android中使用OpenCV java"></a>在Android中使用OpenCV java</h1><p>如果是opencv应用在java代码中，引入opencv的方式略有不同，早期版本的opencv Android sdk引入opencv的方式就像这篇<a href="https://zhuanlan.zhihu.com/p/83240808">博客</a>说的这样，但新版的opencv Android sdk引入opencv的方式如这个StackOverflow的<a href="https://stackoverflow.com/questions/62750486/showing-no-opencv-module-in-dependancies-in-android-studio">答主所说</a>，有所不同，需要更改build.gradle里的两个地方：</p>
<ol>
<li>把 apply plugin: ‘com.android.library’ 改成 apply plugin: ‘com.android.library’</li>
<li>注释掉下面的“applicationId “org.opencv””</li>
</ol>
<p>如果导入opencv java库后遇到 “import import android.hardware.camera2.<em>” 错误的话，很可能是你的<strong>opencvlibrary的build.gradle里的版本</strong>跟<em>*app的build.gradle里的版本</em></em>不相同导致的，如果是的话就改成一样：<br><img src="https://pic.downk.cc/item/5f99753b1cd1bbb86b950fb7.jpg"><br><img src="https://pic.downk.cc/item/5f9975901cd1bbb86b9528cc.jpg"></p>
]]></content>
      <categories>
        <category>CV</category>
        <category>Android</category>
      </categories>
      <tags>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>基金涨跌幅每日邮件提醒</title>
    <url>/2020/07/27/%E5%9F%BA%E9%87%91%E6%B6%A8%E8%B7%8C%E5%B9%85%E6%AF%8F%E6%97%A5%E9%82%AE%E4%BB%B6%E6%8F%90%E9%86%92/</url>
    <content><![CDATA[<p>本文是基于这篇文章<a href="https://www.yuque.com/aicv/lab/mumndo">《Python自动发邮件》</a>实践得到的笔记和经验。</p>
<p><strong>购买服务器</strong></p>
<p>还没有云服务器的可以购买阿里云的ECS学生专享套餐，我买了一个月，花了10块钱。<br>写这篇文章的时候，我还花了99元买了一年的腾讯标准型SA2云服务器。</p>
<p>后面我们要让云服务器通知邮箱服务器发邮件，就需要开启云服务器的25号端口，但两个云服务器默认都是关闭25号端口，我尝试用465端口代替，但最后又遇到了其他问题，最后索性都提交了工单。</p>
<p>这里我要评价一下这两个服务器。首先结论是腾讯云的售后服务比阿里云好很多。先讲阿里云，我要在阿里云申请解封25号端口，要填一些信息，但填完信息后报出“非本人资产”的错误，阿里云也没说这个错误是怎么意思，怎么解决，感觉就是故意刁难你申请解封，最后我被折腾到想提工单，阿里云就扔给你一个智能机器人，几经周转终于召唤出提工单入口，提完阿里云的工单后，我又去腾讯云提工单，腾讯云的工单入口一眼就能找到，1分钟就完事了。到了隔天，腾讯云的售后又是留言回复，又是打电话，又是加qq，最后告诉试试解封25号端口，腾讯云的解封25号端口也比阿里云方便，不会报什么错，而且不用审核，马上解封生效，代码马上能发邮件。而过了一天，阿里云的售后聊了几句后就没回应了，问题也还没解决。难道是阿里云太忙？反正我用起来腾讯云的售后体验更好！</p>
<p><img src="https://s1.ax1x.com/2020/08/13/dpBgpj.png" alt="dpBgpj.png"></p>
<p><strong>安装python3</strong></p>
<p>我刚买的centos只安装了python2.7，为了更好的兼容现在流行的代码，我们需要再安装python3，命令很简单：<code>yum install python3</code>，安装成功的样子：<br><img src="https://s1.ax1x.com/2020/08/12/aX60sK.png" alt="aX60sK.png"></p>
<span id="more"></span>
<p><strong>修改python软链接（可选）</strong></p>
<p>买来的服务器可能同时装有python2 和 python3，现在基本都用python3了，但如果你在命令行里输入python，它关联的还是python2，所以我们要对为python建立新的软连接：</p>
<p>在服务器根目录下，敲入以下代码：</p>
<p>第一步: 移除已有软连接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rf &#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line">rm -rf &#x2F;usr&#x2F;bin&#x2F;python</span><br></pre></td></tr></table></figure>
<p>第二步:创建新的软连接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln -s &#x2F;usr&#x2F;bin&#x2F;python3.6 &#x2F;usr&#x2F;bin&#x2F;python3</span><br><span class="line">ln -s &#x2F;usr&#x2F;bin&#x2F;python3.6 &#x2F;usr&#x2F;bin&#x2F;python</span><br></pre></td></tr></table></figure>
<p>这里我的python3.6的安装路径是/usr/bin/python3.6，需要根据实际的路径填写， 可以通过<code>whereis python(2/3)</code>来查看python路径</p>
<p>这时，你再在命令行输入python3或者python都可以用3.6版本的python了。</p>
<p><strong>使用FileZilla将python脚本上传到云服务器</strong></p>
<p>点开文件，有个站点管理器，创建完新站点再配置好相关信息后连接服务器。端口号不需要我们输入。有可能你的服务器支持的是SFTP，而不是FTP，那么配置时协议注意要选对，否则会报错：“无法和 SFTP 服务器建立 FTP 连接，请选择合适的协议。”。</p>
<p><img src="https://s1.ax1x.com/2020/07/31/aMaUKJ.png" alt="aMaUKJ.png" border="0" /></p>
<p>最后把左侧的文件拖到右侧文件区就能上传文件到服务器了</p>
<p><img src="https://s1.ax1x.com/2020/07/27/aCqv6S.png" alt="aCqv6S.png" border="0" /></p>
<p><strong>安装python第三方包</strong></p>
<p>bs4和requests这两个第三方包/模块是我们代码要用到的，一般服务器没有预装好，不信可以通过pip list输出所有库看看，没有的话就安装它们</p>
<p>pip3 install bs4<br>pip3 install requests</p>
<p>注意是pip3而不是pip，否则模块会被安装到/usr/lib/python2.7/site-packages。即使你用pip3 install 安装好bs4和requests，你还是会遇到no module name xxx的报错，我们还需要多一步：</p>
<p><code>export PYTHONPATH=/usr/local/lib/python3.6/site-packages</code></p>
<p>其中这个PYTHONPATH的路径可通过pip3 show bs4打印出来的Location查到：</p>
<p><a href="https://imgchr.com/i/aCqUJ0"><img src="https://s1.ax1x.com/2020/07/27/aCqUJ0.png" alt="aCqUJ0.png" border="0" width=80%/></a></p>
<p>注意是<code>python3 fund-monitor.py</code>,不是<code>python fund-monitor.py</code>，否则会因为版本不同的原因导致报错;”ValueError: chr() arg not in range(256)”，我们可以通过<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -V</span><br><span class="line">python3 -V</span><br></pre></td></tr></table></figure><br>来查看python的具体版本。</p>
<p><strong>核心python代码</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> smtplib</span><br><span class="line"><span class="keyword">import</span> email</span><br><span class="line"><span class="comment"># 负责构造文本</span></span><br><span class="line"><span class="keyword">from</span> email.mime.text <span class="keyword">import</span> MIMEText</span><br><span class="line"><span class="comment"># 负责构造图片</span></span><br><span class="line"><span class="keyword">from</span> email.mime.image <span class="keyword">import</span> MIMEImage</span><br><span class="line"><span class="comment"># 负责将多个对象集合起来</span></span><br><span class="line"><span class="keyword">from</span> email.mime.multipart <span class="keyword">import</span> MIMEMultipart</span><br><span class="line"><span class="keyword">from</span> email.header <span class="keyword">import</span> Header</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;Cookie&quot;</span>:</span><br><span class="line">    <span class="string">&quot;qgqp_b_id=f8b59df051caea02b176f6d76db75887; EMFUND1=null; EMFUND2=null; EMFUND3=null; EMFUND4=null; EMFUND5=null; EMFUND6=null; EMFUND7=null; st_si=92310565820236; st_asi=delete; searchbar_code=160119; EMFUND0=null; EMFUND8=07-14%2021%3A54%3A31@%23%24%u5357%u65B9%u4E2D%u8BC1500ETF@%23%24510500; EMFUND9=07-25 23:07:36@#$%u5357%u65B9%u4E2D%u8BC1500ETF%u8054%u63A5A@%23%24160119; ASP.NET_SessionId=5ljqqn1s20zpfryhuw5fx4jw; st_pvi=06954122844047; st_sp=2020-05-20%2007%3A32%3A46; st_inirUrl=https%3A%2F%2Fwww.google.com%2F; st_sn=2; st_psi=20200725230736417-0-5210348614&quot;</span>,</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>:</span><br><span class="line">    <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fund_info</span>(<span class="params">fund_code</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取基金涨跌幅信息：信息来源（天天基金网：http://fund.eastmoney.com/）</span></span><br><span class="line"><span class="string">    fund_code：为基金代码，若该基金不存在，返回 False，否则返回 元组(日期， 涨跌幅比例)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    url = <span class="string">&quot;http://fund.eastmoney.com/&#123;&#125;.html&quot;</span>.<span class="built_in">format</span>(fund_code)  </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = requests.get(url, headers)</span><br><span class="line">        soup = BeautifulSoup(r.text, <span class="string">&quot;html.parser&quot;</span>)</span><br><span class="line"><span class="comment">#         print(soup.text)</span></span><br><span class="line">        result = soup.findAll(<span class="string">&quot;dl&quot;</span>, attrs=&#123;<span class="string">&quot;dataItem01&quot;</span>&#125;)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(result[<span class="number">0</span>]))</span><br><span class="line">        pattern = <span class="string">r&quot;&lt;span id=\&quot;gz_gztime\&quot;&gt;\((\d&#123;2&#125;)-(\d&#123;2&#125;)-(\d&#123;2&#125;)&quot;</span></span><br><span class="line">        date = re.findall(pattern, <span class="built_in">str</span>(result[<span class="number">0</span>]))[<span class="number">0</span>]</span><br><span class="line">        date = <span class="string">&quot;-&quot;</span>.join(date)  <span class="comment"># 日期</span></span><br><span class="line">        </span><br><span class="line">        pattern = <span class="string">&#x27;\&quot; id=\&quot;gz_gszzl\&quot;&gt;(.&#123;1,5&#125;)%&lt;/span&gt;&#x27;</span></span><br><span class="line">        rateresult=re.findall(pattern, <span class="built_in">str</span>(result[<span class="number">0</span>]))[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># rate = float(rateresult.lstrip(&quot;-+&quot;)) #lstrip(&quot;-+&quot;)用来裁剪掉数字前面的正负号，但因为后面用了abs(rate)，所以可以不用裁剪</span></span><br><span class="line">        rate = <span class="built_in">float</span>(rateresult)</span><br><span class="line">        <span class="keyword">return</span> date, rate</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>, <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_mail</span>(<span class="params">fund_code, receiver_mail, thresh=<span class="number">1</span>,user=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    fund_code：基金代码</span></span><br><span class="line"><span class="string">    receiver_mail：收件人邮箱</span></span><br><span class="line"><span class="string">    thresh：阈值</span></span><br><span class="line"><span class="string">    user：用户名标志</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># SMTP服务器,这里使用qq邮箱</span></span><br><span class="line">    mail_host = <span class="string">&quot;smtp.qq.com&quot;</span></span><br><span class="line">    <span class="comment"># 发件人邮箱</span></span><br><span class="line">    mail_sender = <span class="string">&quot;**********@qq.com&quot;</span></span><br><span class="line">    <span class="comment"># 邮箱授权码，去邮箱设置里获取</span></span><br><span class="line">    mail_license = <span class="string">&quot;***************&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 收件人邮箱，可以为多个收件人</span></span><br><span class="line">    mail_receivers = [receiver_mail]  <span class="comment"># [&quot;******@qq.com&quot;,&quot;******@foxmail.com&quot;]</span></span><br><span class="line">    <span class="built_in">print</span>(mail_receivers)</span><br><span class="line">    <span class="comment"># 构建 MIMEMultipart 对象代表邮件本身，可以往里面添加文本、图片、附件等</span></span><br><span class="line">    mm = MIMEMultipart(<span class="string">&#x27;related&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 邮件主题</span></span><br><span class="line">    subject_content = <span class="string">&quot;&quot;&quot;基金&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 设置发送者,注意严格遵守格式,里面邮箱为发件人邮箱</span></span><br><span class="line">    mm[<span class="string">&quot;From&quot;</span>] = <span class="string">&quot;基金投资助手&lt;**********@qq.com&gt;&quot;</span></span><br><span class="line">    <span class="comment"># 设置接受者,注意严格遵守格式,里面邮箱为接受者邮箱</span></span><br><span class="line">    <span class="comment"># 多个接受者 &quot;receiver_1_name&lt;******@qq.com&gt;,receiver_2_name&lt;******@foxmail.com&gt;&quot;</span></span><br><span class="line">    mm[<span class="string">&quot;To&quot;</span>] = <span class="string">&quot;receiver_1_name&lt;&#123;&#125;&gt;&quot;</span>.<span class="built_in">format</span>(receiver_mail)</span><br><span class="line">    <span class="comment"># 设置邮件主题</span></span><br><span class="line">    mm[<span class="string">&quot;Subject&quot;</span>] = Header(subject_content, <span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    body_content = <span class="string">&quot;现在是 &#123;&#125; \n\n&quot;</span>.<span class="built_in">format</span>(time.strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>, time.localtime()))</span><br><span class="line">    <span class="keyword">for</span> code <span class="keyword">in</span> fund_code:</span><br><span class="line">        date, rate = get_fund_info(code)</span><br><span class="line"><span class="comment">#         print(date, rate)</span></span><br><span class="line">        <span class="comment"># 邮件正文内容</span></span><br><span class="line">        <span class="keyword">if</span> date <span class="keyword">and</span> rate:</span><br><span class="line">            <span class="comment">#####  根据基金代码获取基金信息</span></span><br><span class="line">            url = <span class="string">&quot;http://fund.eastmoney.com/js/fundcode_search.js&quot;</span></span><br><span class="line">            r = requests.get(url, headers)</span><br><span class="line">            info = re.findall(<span class="string">&quot;(\[.*?\])&quot;</span>, r.text[<span class="number">9</span>:-<span class="number">2</span>])</span><br><span class="line">            fund_info = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x.replace(<span class="string">&quot;\&quot;&quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot;[&quot;</span>, <span class="string">&quot;&quot;</span>).replace(<span class="string">&quot;]&quot;</span>, <span class="string">&quot;&quot;</span>).split(<span class="string">&quot;,&quot;</span>)[<span class="number">0</span>]==code, info))[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">abs</span>(rate) &gt; thresh:</span><br><span class="line">                temp = <span class="string">&quot;&quot;&quot;&#123;&#125; 基金涨幅为&#123;&#125;, \n \t \t  涨跌幅超过阈值！\n\n&quot;&quot;&quot;</span>.<span class="built_in">format</span>(fund_info, rate)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                temp = <span class="string">&quot;&quot;&quot;&#123;&#125; 基金涨幅为&#123;&#125; \n\n&quot;&quot;&quot;</span>.<span class="built_in">format</span>(fund_info, rate)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp = <span class="string">&quot;&quot;&quot;代码为 &#123;&#125; 的基金 \n 不存在！&quot;&quot;&quot;</span>.<span class="built_in">format</span>(code)</span><br><span class="line">        body_content += temp</span><br><span class="line">    <span class="comment"># 构造文本,参数1：正文内容，参数2：文本格式，参数3：编码方式</span></span><br><span class="line">    message_text = MIMEText(body_content, <span class="string">&quot;plain&quot;</span>, <span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">    <span class="comment"># 向MIMEMultipart对象中添加文本对象</span></span><br><span class="line">    mm.attach(message_text)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建SMTP对象</span></span><br><span class="line">    stp = smtplib.SMTP()</span><br><span class="line">    <span class="comment"># 设置发件人邮箱的域名和端口，端口地址为25</span></span><br><span class="line">    stp.connect(mail_host, <span class="number">25</span>)</span><br><span class="line">    <span class="comment"># set_debuglevel(1)可以打印出和SMTP服务器交互的所有信息</span></span><br><span class="line">    stp.set_debuglevel(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 登录邮箱，传递参数1：邮箱地址，参数2：邮箱授权码</span></span><br><span class="line">    stp.login(mail_sender, mail_license)</span><br><span class="line">    <span class="comment"># 发送邮件，传递参数1：发件人邮箱地址，参数2：收件人邮箱地址，参数3：把邮件内容格式改为str</span></span><br><span class="line">    stp.sendmail(mail_sender, mail_receivers, mm.as_string())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;邮件发送成功&quot;</span>)</span><br><span class="line">    <span class="comment"># 关闭SMTP对象</span></span><br><span class="line">    stp.quit()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 基金编码，收件邮箱，阈值，用户名（做标记）</span></span><br><span class="line">send_mail(fund_code=[<span class="string">&quot;040011&quot;</span>, <span class="string">&quot;001616&quot;</span>, <span class="string">&quot;000985&quot;</span>, <span class="string">&quot;001986&quot;</span>, <span class="string">&quot;001740&quot;</span>, <span class="string">&quot;001156&quot;</span>, <span class="string">&quot;519069&quot;</span>, <span class="string">&quot;004347&quot;</span>], receiver_mail=<span class="string">&quot;********@outlook.com&quot;</span>, thresh=<span class="number">2</span>, user=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输入<code>crontab -e</code>进入crontab编辑模式，这时候你会看到一些系统自己写好的一些脚本，敲击<code>i</code>进入编辑模式，在新的一行里输入<code>45 14 * * 1-5 bash /root/fund/fund_code.sh</code>。好了之后按ESC退出编辑模式，再按<code>:</code>进入末行模式，输入<code>wq</code>保存并退出vim编辑器。</p>
<p>在等着过会儿服务器自动给你发邮件之前，最好再检查一下：<code>python3 /fund/fund_monitor.py</code>,如果这句代码运行没问题，那待会儿成功的概率会大一点。</p>
<p>说一下我遇到的坑，主要就是我用谷歌浏览器看到的html代码跟python脚本提取出来的代码不一样：<br><img src="https://s1.ax1x.com/2020/08/10/aq4rTS.png" alt="aq4rTS.png"><br><img src="https://s1.ax1x.com/2020/08/10/aq4LlR.png" alt="aq4LlR.png"><br>从上图的红圈可以看出，日期在两个地方的格式以及日期的前后字符串都是不一样吗，所以我们的正则表达式要基于python代码提取出来的结果进行设计，否则代码运行会匹配不到报错。所以我用<code>print(str(result[0]))</code>把结果打印出来，然后再把这个结果粘贴到这个工具网站（<a href="https://regex101.com/）进行正则表达式的设计：">https://regex101.com/）进行正则表达式的设计：</a><br><img src="https://s1.ax1x.com/2020/08/10/aq5AXt.png" alt="aq5AXt.png"></p>
]]></content>
      <categories>
        <category>python</category>
        <category>理财</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>基金</tag>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title>SORT系列目标跟踪</title>
    <url>/2020/07/25/SORT%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>这篇文章<a href="https://blog.csdn.net/qq_42374559/article/details/96032672">《多目标跟踪入门篇(1)：SORT算法详解》</a>对SORT算法进行了一个很好的概括，字字珠玑。我觉得配上下面这张图(<a href="https://blog.csdn.net/HaoBBNuanMM/article/details/85555547">来源</a>)能更好的理解匈牙利算法的背景和作用：</p>
<h1 id="C-版SORT"><a href="#C-版SORT" class="headerlink" title="C++ 版SORT"></a>C++ 版SORT</h1><p><img src="https://s1.ax1x.com/2020/07/26/a9vvBF.png" alt="a9vvBF.png" border="0" /></p>
<p>匈牙利算法就是通过计算预测跟检测的BBOX的IOU值来完成前后两帧里的所有BBOX的匹配。</p>
<p>下面将介绍如何跑通这个SORT算法的C++版本（<a href="https://github.com/mcximing/sort-cpp）：">https://github.com/mcximing/sort-cpp）：</a></p>
<h2 id="1-下载安装opencv"><a href="#1-下载安装opencv" class="headerlink" title="1.下载安装opencv"></a>1.下载安装opencv</h2><p>因为这个SORT项目用到了opencv，所以需要先配置opencv的环境，去opencv官网：<a href="https://opencv.org/releases/，">https://opencv.org/releases/，</a> 选择Windows版下载。安装好后是这样子的：</p>
<p><img src="https://s1.ax1x.com/2020/07/25/aSvcYq.png" alt="aSvcYq.png"></p>
<h2 id="2-设置环境变量："><a href="#2-设置环境变量：" class="headerlink" title="2.设置环境变量："></a>2.设置环境变量：</h2><p>将<code>opencv\build\x64\vc15\bin</code>加入用户或者系统的环境变量path里面，如果你已经启动Visual studio，那么得重启才生效。不做该步骤的话后面会提示“找不到 opencv_world430.dll”<br><img src="https://s1.ax1x.com/2020/07/25/aSvo79.png" alt="aSvo79.png" border="0" width=70%/></p>
<p><img src="https://s1.ax1x.com/2020/07/25/aShrX6.png" alt="aShrX6.png"></p>
<h2 id="3-在VS打开项目-解决方案"><a href="#3-在VS打开项目-解决方案" class="headerlink" title="3.在VS打开项目/解决方案"></a>3.在VS打开项目/解决方案</h2><p>用VS打开下载下来的仓库的文件夹，打开后大概是这个样子：</p>
<p><img src="https://s1.ax1x.com/2020/07/25/aSxm7j.png" alt="aSxm7j.png" border="0" /></p>
<span id="more"></span>
<h2 id="4-配置管理器"><a href="#4-配置管理器" class="headerlink" title="4.配置管理器"></a>4.配置管理器</h2><p>在VS界面上方有下图所示的解决方案配置和解决方案平台，解决方案配置你选Release还是Debug取决于你，但后面那个解决方案平台如果默认不是x64的话，就得换成x64，因为从opencv\build下面的x64文件夹，我们推断opencv只支持x64。由于我这里默认是Win32，所以我需要添加新的活动解决方案平台，否则后面会报错：“模块计算机类型“x64”与目标计算机类型“X86”冲突”</p>
<p><img src="https://s1.ax1x.com/2020/07/26/aSx09x.png" alt="aSx09x.png" border="0" /></p>
<p><img src="https://s1.ax1x.com/2020/07/26/aSxcHH.png" alt="aSxcHH.png" border="0" /></p>
<p><img src="https://s1.ax1x.com/2020/07/26/aSxW4I.png" alt="aSxW4I.png" border="0" width=60%/></p>
<h2 id="5-配置属性"><a href="#5-配置属性" class="headerlink" title="5.配置属性"></a>5.配置属性</h2><p>在工具栏：项目-属性，调出项目属性框;或者右击解决方案管理器里的项目，列表最下方就是属性。</p>
<h3 id="5-1-切换平台工具集"><a href="#5-1-切换平台工具集" class="headerlink" title="5.1 切换平台工具集"></a>5.1 切换平台工具集</h3><p>因为这个项目默认的平台工具集是VS 2013，而我的电脑安装的是VS 2019，所以我需要把平台工具集换成Visual Studio 2019 （V142）</p>
<p><img src="https://s1.ax1x.com/2020/07/26/aSzgMT.png" alt="aSzgMT.png" border="0" /></p>
<h3 id="5-2-添加头文件包含路径"><a href="#5-2-添加头文件包含路径" class="headerlink" title="5.2 添加头文件包含路径"></a>5.2 添加头文件包含路径</h3><p>在VC++目录栏的包含目录中添加<code>opencv\build\include</code>路径。在库目录中添加<code>opencv\build\x64\vc15\lib</code>路径。</p>
<p><img src="https://s1.ax1x.com/2020/07/26/aSzqsO.png" alt="aSzqsO.png" border="0" /></p>
<p>如果不配置，则会出现“C1083    : “opencv2/video/tracking.hpp”: No such file or directory”的报错</p>
<h3 id="5-3-添加附加包含目录"><a href="#5-3-添加附加包含目录" class="headerlink" title="5.3 添加附加包含目录"></a>5.3 添加附加包含目录</h3><p>在C/C++ 里的附加包含目录也添加<code>opencv\build\include</code>路径：</p>
<p><img src="https://s1.ax1x.com/2020/07/26/apSPQf.png" alt="apSPQf.png" border="0" /></p>
<h3 id="5-4-添加链接库"><a href="#5-4-添加链接库" class="headerlink" title="5.4 添加链接库"></a>5.4 添加链接库</h3><p>首先在链接器的常规页面里添加lib文件夹，再在输入页面的附加依赖项中添加<code>opencv_world430d.lib</code>，这个文件就是<code>opencv\build\x64\vc15\lib</code>路径下的lib文件，不同版本的名字可能不同。Debug时添加末尾有“d”的lib，release版添加末尾没有“d”的lib。如果这一步配置有问题，后面会遇到“无法解析的外部符号…”的报错<br><img src="https://s1.ax1x.com/2020/07/26/apSdl6.png" alt="apSdl6.png" border="0" /></p>
<p><img src="https://s1.ax1x.com/2020/07/26/apS6kd.png" alt="apS6kd.png" border="0" /></p>
<h2 id="6-修改源代码"><a href="#6-修改源代码" class="headerlink" title="6.修改源代码"></a>6.修改源代码</h2><p>配置完各种环境，有几处源代码也需要修改一下：</p>
<ul>
<li><p>cvWaitKey(40);  <strong>—&gt;</strong>  waitKey(40);</p>
</li>
<li><p><em>(Mat_<float>(stateNum, stateNum)  <em>*—&gt;</em></em>  (Mat_<float>(stateNum, stateNum)</p>
</li>
</ul>
<p>右击资源管理器里的项目，选择 重新生成，没问题的话，会看到以下输出：</p>
<p><img src="https://s1.ax1x.com/2020/07/26/apSgfI.png" alt="apSgfI.png" border="0" /></p>
<h2 id="7-下载数据集"><a href="#7-下载数据集" class="headerlink" title="7.下载数据集"></a>7.下载数据集</h2><p>去 <a href="https://motchallenge.net/data/2D_MOT_2015/#download">https://motchallenge.net/data/2D_MOT_2015/#download</a> 下载数据集，注意，得是完整数据集，无法下载单个视频，另外我们也不能只下载单个视频，因为我们这里要用到的其实是一连串的视频截图。下载解压后，把源代码中的<strong>imgPath</strong>修改成你的路径</p>
<p><img src="https://s1.ax1x.com/2020/07/26/apf4HO.png" alt="apf4HO.png" border="0" /></p>
<h2 id="8-运行代码"><a href="#8-运行代码" class="headerlink" title="8.运行代码"></a>8.运行代码</h2><p>最后一步，看main.cpp的第76、77行，如果你选择<strong>TestSORT(seq, false);</strong> 如果是false，则不会跟踪以上任何一个视频，如果true，则会跟踪以上所有视频。如果你选择 <strong>TestSORT(“ADL-Rundle-8”, true);</strong> ，那么只对ADL-Rundle-8这个视频进行跟踪。最后点击<strong>本地Windows调试器</strong>，就能看到跟踪效果的视频啦！</p>
<p><img src="https://s1.ax1x.com/2020/07/26/apIi2d.png" alt="apIi2d.png" border="0" /></p>
<p>在这里说一个我发现的玄学，就是如果我按下键盘的空格键后，视频播放速度会加快，比较正常，不然视频播放速度是偏慢的。</p>
<h2 id="python-版SORT"><a href="#python-版SORT" class="headerlink" title="python 版SORT"></a>python 版SORT</h2><p>如果也想跑一下python版的SORT算法，可以看我fork的这个<a href="https://github.com/wwdok/sort">repo</a>。</p>
<h1 id="DeepSort"><a href="#DeepSort" class="headerlink" title="DeepSort"></a><strong>DeepSort</strong></h1><p>DeepSORT来自论文《<a href="https://arxiv.org/abs/1703.07402">Simple Online and Realtime Tracking with a Deep Association Metric</a>》，它在SORT的基础上集成了基于深度外观描述符（deep appearance descriptor）的外观信息（appearance information）。换句话说就是，引入了在行人重识别数据集（MARS：Motion Analysis and Re-identification Set)）上训练的深度学习模型（mars-small128），在实时目标追踪过程中，提取目标的表观特征进行最近邻匹配，可以改善有遮挡情况下的目标追踪效果。同时，也减少了目标ID跳变的问题。</p>
<p>【<a href="https://github.com/nwojke/deep_sort">DeepSort官方Github仓库</a>】<br>【<a href="https://github.com/bitzy/DeepSort">C++版DeepSort Github仓库</a>】</p>
<h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p><img src="https://pic.downk.cc/item/5f9ade781cd1bbb86beb23c5.jpg"></p>
<p>下载repo和两个detections、networks，放在如下所示新建的resources文件夹里：</p>
<p><img src="https://pic.downk.cc/item/5f9addf21cd1bbb86beb03bf.jpg"></p>
<h3 id="让模型跑起来"><a href="#让模型跑起来" class="headerlink" title="让模型跑起来"></a><strong>让模型跑起来</strong></h3><p>在让模型跑起来之前，先对deep_sort_app.py的命令行参数进行说明：</p>
<p>1.<strong>sequence_dir</strong>：SORT的跟踪数据集用的是2D_MOT_2015，而DeepSort用的是MOT16，所以要再去下载一下这个数据集（<a href="https://motchallenge.net/data/MOT16/">官网下载链接</a>）。我这里设置为：<code>r&#39;D:\MachineLearning\DataSet\MOT16\test\MOT16-06&#39;</code>。</p>
<p>2.<strong>detection_file</strong>：我这里设置为：<code>r&#39;./resources/detections/MOT16_POI_test/MOT16-06.npy&#39;</code>.npy文件是numpy专用的二进制文件，在DeepSORT里它跟在SORT里的txt文件一样，存储了预先检测好的矩形框位置信息。</p>
<p>3.<strong>output_file</strong>：默认保存在<code>&quot;/tmp/hypotheses.txt&quot;</code>,所以你要么新建一个tmp文件夹，要么删掉<code>/tmp</code>，否则保存会报错。</p>
<p>4.<strong>min_confidence</strong>：在一开始就把检测结果置信度较低的给忽略掉。</p>
<p>5.<strong>min_detection_height</strong>：同样在一开始就把检测结果矩形框高度太小的小目标给忽略掉。</p>
<p>6.<strong>nms_max_overlap</strong>：代码中默认设为1。因为不可能两个矩形框完全重合，所以作者的意思就是不对矩形框进行非极大值抑制。</p>
<p>7.<strong>max_cosine_distance</strong>：<br><img src=https://img-blog.csdnimg.cn/20190101201743717.jpg></p>
<p>[<a href="https://blog.csdn.net/HaoBBNuanMM/article/details/85555547">图片来源</a>]</p>
<p>8.<strong>nn_budget</strong>：神经网络预算</p>
<p>9.<strong>display</strong>：这个默认是True的，开启后运行代码时会跳出窗口显示跟踪效果。这不是必须的嘛，我不知道作者给大家这个选择的意义是什么，作者还写了一个bool_string()来规定输入的类型。</p>
<p>因为我懒得在命令行里敲入参数再运行，这样每次关掉命令行窗口后还要重新打一遍字，所以我直接在deep_sort_app.py里修改，修改5个地方，即：修改2处路径，删除两处<code>required=True</code>,修改<code>nn_budget</code>默认值为某个整数。</p>
<p><img src="https://pic.downk.cc/item/5f9842a01cd1bbb86b3d32e0.jpg"></p>
<p>修改完之后，在Pycharm里run一下就能看到跟踪效果的窗口显示出来了。按下空格键可以暂停，按下ESC键关闭窗口。</p>
<p>按下空格键可以暂停视频，我们会发现画面中的目标周边有两个矩形框，一个所有目标都是红色的矩形框，和一个随目标不同而不同颜色的矩形框。</p>
<p><img src="https://pic.downk.cc/item/5f9ae0721cd1bbb86bebb1b1.jpg"></p>
<p>前面开头说了mars-small128.pb是一个在Re-ID数据集上训练出来的人体检测模型，那它在这个项目里有什么用呢？其实它就是配合generate_detection.py使用：<code>python tools/generate_detections.py --model=resources/networks/mars-small128.pb --mot_dir=./MOT16/train --output_dir=./resources/detections/MOT16_train</code>,输出detection文件夹下那种目标检测结果的文件，用于你如果有其他非MOT16数据集里的视频，然后你想跟踪这个视频里的人体，那么就可以用mars-small128.pb先导出检测结果，然后再像常规那样运行deep_sort_app.py。所以这个东西对我来说意义不大，一是因为我实践中肯定是希望检测和跟踪是同时进行的，而不是断开来进行，否则就没有了应用的意义，另一个是这个模型只能检测人体，如果我想检测跟踪汽车，那这个模型就用不上了。</p>
<p>这个原始的repo还有个问题是，它的输入图像是一帧帧独立的图片，如果想输入视频呢，这个<a href="https://github.com/nwojke/deep_sort/issues/4">issue</a>里就讨论了这个问题，Ravisik 给出了代码片段，qidian123更是把deepsort跟yolov3结合了起来。</p>
<p>马氏距离：<a href="https://www.cnblogs.com/abella/p/11171364.html，https://www.biaodianfu.com/mahalanobis-distance.html">https://www.cnblogs.com/abella/p/11171364.html，https://www.biaodianfu.com/mahalanobis-distance.html</a></p>
<p>TheAIGuy开源了<a href="https://github.com/theAIGuysCode/yolov4-deepsort">yolov4-deepsort</a>，亲测好用。</p>
<p>其他跟踪方法：</p>
<p>光流跟踪</p>
<h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><p><a href="https://zhuanlan.zhihu.com/p/133670271">《搞不懂MOT数据集，会跑代码有啥用！》</a><br><br><a href="https://blog.csdn.net/zjc910997316/article/details/89447272">《SORT详解： 代码解读》</a><br><br><a href="https://blog.csdn.net/sgfmby1994/article/details/98517210">《Deep sort算法代码解读》</a><br><br><a href="https://www.bilibili.com/video/BV1zE411W7Fn?from=search&amp;seid=14150103438250024110">《Win10环境下OpenCV-4.2.0 Visual Studio C++开发环境设置》</a><br><br><a href="https://blog.csdn.net/HaoBBNuanMM/article/details/85555547">《【算法分析】SORT/Deep SORT 物体跟踪算法解析》</a></p>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>目标跟踪</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title>使用nodemailer发送邮件</title>
    <url>/2020/07/18/%E4%BD%BF%E7%94%A8nodemailer%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/</url>
    <content><![CDATA[<p>最近想做一个通过邮箱验证码来注册微信小程序账号的功能，这个可以通过nodemailer实现。</p>
<p><strong>Nodemailer：</strong></p>
<ul>
<li>支持Unicode编码</li>
<li>支持Window系统环境</li>
<li>支持HTML内容和普通文本内容</li>
<li>支持附件(传送大附件)</li>
<li>支持HTML内容中嵌入图片</li>
<li>支持SSL/STARTTLS安全的邮件发送</li>
<li>支持内置的transport方法和其他插件实现的transport方法</li>
<li>支持自定义插件处理消息</li>
<li>支持XOAUTH2登录验证</li>
</ul>
<p><strong>先决条件</strong>：<strong>已安装node.js</strong></p>
<h2 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a><strong>第一步：</strong></h2><p>我这里重新注册了一个163邮箱为例，注册完进入邮箱，点击设置，开启<strong>IMAP/SMTP服务</strong>，为什么要开启这个服务？因为正如页面下方所说的“<strong>POP3/SMTP/IMAP服务能让你在本地客户端上收发邮件</strong>”，我们运行脚本发送邮件就是在本地电脑上实现的。</p>
<p>开启后先是手机验证，验证成功会给你一串密钥，<strong>这串密码务必保存备份好</strong>，后面要写入代码中，没它不行。</p>
<p><img src="https://s1.ax1x.com/2020/07/18/U2K4mT.png" alt="U2K4mT.png" border="0" /></p>
<h2 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a><strong>第二步：</strong></h2><p>新建一个存放本程序的文件夹，比如“send-email”。然后进入该文件夹在当前路径下打开命令行窗口，敲写<code>npm init -y</code>, 这时候会在文件夹下面生成一个package.json文件，不用管它。</p>
<h2 id="第三步："><a href="#第三步：" class="headerlink" title="第三步："></a><strong>第三步：</strong></h2><p>安装nodemailer</p>
<p><code>npm install nodemailer -S</code></p>
<h2 id="第四步："><a href="#第四步：" class="headerlink" title="第四步："></a><strong>第四步：</strong></h2><p>敲写<code>touch send-email.js</code>创建js脚本，接下来我们要在里面写代码</p>
<h2 id="第五步："><a href="#第五步：" class="headerlink" title="第五步："></a><strong>第五步：</strong></h2><p>用VSCODE 打开send-email.js, 敲入下面的代码：</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> nodemailer = <span class="built_in">require</span>(<span class="string">&#x27;nodemailer&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> transporter = nodemailer.createTransport(&#123;</span><br><span class="line">  service: <span class="string">&#x27;163&#x27;</span>,</span><br><span class="line">  <span class="comment">//支持的邮箱类型 :https://nodemailer.com/smtp/well-known/</span></span><br><span class="line"><span class="comment">//   port: 465, // SMTP 端口</span></span><br><span class="line"><span class="comment">//   secureConnection: true, // 使用了 SSL</span></span><br><span class="line">  auth: &#123;</span><br><span class="line">    user: <span class="string">&#x27;abcdefg@163.com&#x27;</span>,<span class="comment">//发件箱地址</span></span><br><span class="line">    <span class="comment">// 下面填写163邮箱的smtp授权码，就是刚刚第一步说的密钥</span></span><br><span class="line">    pass: <span class="string">&#x27;UEOG*******EGOCR&#x27;</span>,</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> mailOptions  = &#123;</span><br><span class="line">  <span class="keyword">from</span>: <span class="string">&#x27;&quot;某某小程序&quot; &lt;abcdefg@163.com&gt;&#x27;</span>, <span class="comment">// 发送地址</span></span><br><span class="line">  to: <span class="string">&#x27;xxxx.xxxx@outlook.com&#x27;</span>, <span class="comment">// 接收地址</span></span><br><span class="line">  subject: <span class="string">&#x27;注册某某微信小程序&#x27;</span>, <span class="comment">// 邮件标题</span></span><br><span class="line">  <span class="comment">// 发送text或者html</span></span><br><span class="line">  <span class="comment">// text: &#x27;Hello World&#x27;,</span></span><br><span class="line">  html: <span class="string">&#x27;&lt;h1&gt;您的注册验证码为123456&lt;/h1&gt;&#x27;</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">transporter.sendMail(mailOptions, <span class="function">(<span class="params">err, data</span>) =&gt;</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (err) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">console</span>.log(<span class="string">&#x27;Error occurs&#x27;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">console</span>.log(<span class="string">&#x27;Email sent!!!&#x27;</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>在代码区右击Run Code或者<code>node send-email.js</code>，一切正常的话，就会出现下面的’Email sent!!!’</p>
<p><img src="https://s1.ax1x.com/2020/07/18/U2nvgx.png" alt="U2nvgx.png" border="0" /></p>
<p>如果发送的内容是比较复杂的HTML页面，那么可以通过Node的fs模块加载一个HTML文件来实现</p>
<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> mailOptions  = &#123;</span><br><span class="line">  <span class="keyword">from</span>: <span class="string">&#x27;&quot;某某小程序&quot; &lt;abcdefg@163.com&gt;&#x27;</span>, <span class="comment">// 发送地址</span></span><br><span class="line">  to: <span class="string">&#x27;xxxx.xxxx@outlook.com&#x27;</span>, <span class="comment">// 接收地址</span></span><br><span class="line">  subject: <span class="string">&#x27;注册某某微信小程序&#x27;</span>, <span class="comment">// 邮件标题</span></span><br><span class="line">  html: fs.createReadStream(path.resolve(__dirname, <span class="string">&#x27;email.html&#x27;</span>))</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="第六步："><a href="#第六步：" class="headerlink" title="第六步："></a><strong>第六步：</strong></h2><p>邮件内容里的验证码我的想法是通过加密算法实现对任意输入字符串输出一个固定长度（比如6位）的字符串，这时先不讲。</p>
<p><strong>拓展阅读</strong></p>
<ol>
<li><p>更多使用node.js处理邮件问题的教程请见这个仓库：<br><a href="https://github.com/accimeesterlin/nodemailer-examples">https://github.com/accimeesterlin/nodemailer-examples</a></p>
<p> <img src="https://s1.ax1x.com/2020/07/18/U2QWZT.png" alt="U2QWZT.png" border="0" width=60%/></p>
</li>
<li><p>短信验证码的教程请见这篇公众号文章《<a href="https://mp.weixin.qq.com/s/tZINGlt3TplBJfdGNRJQpw">10分钟实现短信验证码</a>》</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
  </entry>
  <entry>
    <title>理解batch normalization</title>
    <url>/2020/07/12/%E7%90%86%E8%A7%A3batch-normalization/</url>
    <content><![CDATA[<p>网上关于批量标准化/归一化的的介绍文章有不少，这里先贴上几个不错的教程：</p>
<p>周莫烦的这个视频<a href="https://www.youtube.com/watch?v=-5hESl-Lj-4">《为什么要批标准化（Batch Normalization）？》</a>会给你一个直观的感受，知道为什么要批量标准化。</p>
<p>吴恩达的这个视频<a href="https://www.youtube.com/watch?v=5qefnAek8OA&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&amp;index=30">《Batch Norm At Test Time (C2W3L07)》</a>告诉你测试时的批量归一化跟训练时有何不同</p>
<p>这篇文章<a href="https://cloud.tencent.com/developer/article/1591375">《【AlexeyAB DarkNet框架解析】十一，BN层代码详解(batchnorm_layer.c)》</a>给出了BN层的代码。</p>
<p>在配置文件（比如samples\configs\ssd_mobilenet_v2_quantized_300x300_coco.config”）里，box_predictor{}和feature_extractor{}里都有一个batch_norm:</p>
<pre><code>batch_norm &#123;
    train: true,
    scale: true,
    center: true,
    decay: 0.97,
    epsilon: 0.001,
&#125;
</code></pre><p>这里面的decay是什么意思呢？</p>
<p>在上面那篇文章的代码里，有这样一段代码：</p>
<pre><code>running_mean = momentum * running_mean + (1 - momentum) * sample_mean
running_var = momentum * running_var + (1 - momentum) * sample_var
</code></pre><p>这个公式里的momentum就是上面tensorflow config里说的decay。而running_mean和running_var就是给后面测试用的均值和方差的估计值。</p>
<p>从源代码里还可以知道sample_mean和sample_var就是训练时当下mini batch算出来的均值和方差。</p>
<ul>
<li><p>如果momentum/decay = 0：</p>
<pre><code>  running_mean = 1 * sample_mean
  running_var = 1  * sample_var
</code></pre><p>  则运行均值和运行方差始终是当下那个小批量的均值和方差，完全不考虑之前的批次得到的均值和方差，该种情况下训练性能和验证性能可能存在显着差异。</p>
</li>
<li><p>如果momentum/decay = 1：</p>
<pre><code>  running_mean = 1 * running_mean  
  running_var = 1 * running_var 
</code></pre><p>  则运行均值和运行方差始终是第一个小批量的均值和方差，完全不考虑之后的批次得到的均值和方差，毫无疑问，跟前面一种情况类似，只考虑某一个批次计算得到的均值和方差，只不过一个是总是变成最新的，一个总是保持一开始的均值和方差不变。</p>
</li>
</ul>
<p>由于我们希望对尽可能多的样本进行“平均”，并且过去小批次中的样本很重要，因此对代表着历史均值和历史方差的running_mean和running_var应赋予较大的权重（0.9以上），而对当下批次的均值和方差（sample_mean和sample_var）赋予一个小的权重（小于0.1），毕竟假如我们要训练5万个批次，第5万次批次的均值和方差的权重总不能给到0.1或更大吧，那之前49999个批次的均值和方差不就没有参与的意义了吗，这样就不平衡了。momentum/decay一般设置成0.99、0.99、0.9些数值之一，而且训练步数/批次越多，这个值应该越接近1，即有更多的9。</p>
<h2 id="融合Batch-Normalization"><a href="#融合Batch-Normalization" class="headerlink" title="融合Batch Normalization"></a><strong>融合Batch Normalization</strong></h2><p>关于融合BN的原理可以看这篇文章，那这个在tensorflow里怎么实现呢</p>
<p>参考阅读：<br><a href="https://leimao.github.io/blog/Batch-Normalization/">《Batch Normalization Explained- Lei Mao》</a></p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>将Vue项目部署到Gitee Page上</title>
    <url>/2020/07/11/%E5%B0%86Vue%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E5%88%B0Gitee-Page%E4%B8%8A/</url>
    <content><![CDATA[<p>最近我们的小团队接了一个开发微信小程序的活，因为是给企业用的，我觉得有必要做一个后台管理系统，用来后期给企业里的人自己管理小程序上一些内容的更新和发布，这样才是长久之计。我在github上随便找了个相关的 repo - <a href="https://github.com/lin-xin/vue-manage-system">vue-manage-system</a> ，决定自己先跑通看看，就是把它的后台管理系统从<a href="https://lin-xin.gitee.io/example/work/">作者的域名</a>部署到我自定义的域名上。</p>
<p><img src="https://pic.downk.cc/item/5f905dc61cd1bbb86b8fc5e3.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f905db01cd1bbb86b8fc04e.jpg"></p>
<span id="more"></span>
<p><img src="https://pic.downk.cc/item/5f905d631cd1bbb86b8fad47.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f905d991cd1bbb86b8fb9b6.jpg"></p>
<h2 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a><strong>部署步骤</strong></h2><ul>
<li><p>进入本地一个文件夹里面，在这里执行以下命令，把模板下载到本地</p>
<p><code>git clone https://github.com/lin-xin/vue-manage-system.git</code></p>
</li>
<li><p>进入模板目录</p>
<p><code>cd vue-manage-system</code></p>
</li>
<li><p>安装项目依赖，等待安装完成之后，安装失败可用 cnpm 或 yarn</p>
<p><code>npm install</code></p>
</li>
<li><p>开启服务器，浏览器访问 <a href="http://localhost:8080">http://localhost:8080</a></p>
<p><code>npm run serve</code></p>
</li>
<li><p>执行构建命令，生成最终部署需要的dist文件夹。 构建成功，命令行会提示“Check out deployment instructions at <a href="https://cli.vuejs.org/guide/deployment.html">https://cli.vuejs.org/guide/deployment.html</a> ”</p>
<p><code>npm run build</code></p>
</li>
</ul>
<hr>
<ul>
<li><p>去gitee官网新建一个gitee仓库<br> <img src="https://s1.ax1x.com/2020/07/11/UQ3Hd1.png" alt="UQ3Hd1.png" border="0" /></p>
</li>
<li><p>把origin关联到远程库</p>
<p> <code>git remote add origin 你的远程库地(如：https://gitee.com/wwdok/vue-backend-management.git）</code></p>
<p> 如果遇到“fatal: remote origin already exists.”，可通过执行 <code>git remote rm origin</code> 解决，完了再执行一遍上面的<code>git remote add origin https://gitee.com/wwdok/vue-backend-management.git</code>。</p>
</li>
<li><p>接下来你有两种选择，一种是把vue-manage-system里的18个文件都上传到gitee的仓库中，也可以只上传dist文件夹到gitee仓库，我这里选择的是只上传dist文件夹, 所以依次执行下面的命令：</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b gh-pages  &#x2F;&#x2F;进入gh-pages branch，dist将会被上传到gh-pages这个分支</span><br><span class="line">git add -f dist</span><br><span class="line">git commit -m &#39;first commit&#39;</span><br><span class="line">git subtree push --prefix dist origin gh-pages</span><br></pre></td></tr></table></figure>
<p>  这时候刷新我们创建的仓库，就能看到刚刚push的dist文件夹了<br>  <img src="https://s1.ax1x.com/2020/07/11/UQwFFH.png" alt="UQwFFH.png" border="0" /></p>
</li>
<li><p>点击页面右上角的服务，选择<strong>Gitee Pages</strong>，进入以下界面，在这里的<strong>部署分支</strong>，下拉选择 <strong>gh-pages</strong>，最后点击<strong>启动</strong>就行啦！过一会我们就能体验我们部署的后台管理系统啦：<a href="http://wwdok.gitee.io/vue-backend-management">http://wwdok.gitee.io/vue-backend-management</a> ！<br>  <img src="https://s1.ax1x.com/2020/07/11/UQUqnU.png" alt="UQUqnU.png" border="0" /></p>
</li>
</ul>
<p>我学习一门新的技术喜欢先体验看看，这样会让我更有动力去学习它里面的东西。现在我已经能部署一个vue项目，接下来我会开始学习怎么创建自己的vue项目。</p>
]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>vue</tag>
        <tag>gitee</tag>
      </tags>
  </entry>
  <entry>
    <title>百科知识大收藏</title>
    <url>/2020/07/04/%E7%99%BE%E7%A7%91%E7%9F%A5%E8%AF%86%E5%A4%A7%E6%94%B6%E8%97%8F/</url>
    <content><![CDATA[<p>虽然我开这个博客网站的目的就是为了记录自己学过的各种知识点，但网上也不乏那种做的很好的知识科普资料，比如它是视频形式，那我把它翻译成文字实在是耗时又没创造性，于是我专门创建了这篇博客来汇总这些我有兴趣的且认为干货满满的知识科普视频!</p>
<h1 id="金融理财"><a href="#金融理财" class="headerlink" title="金融理财"></a>金融理财</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">标题</th>
<th style="text-align:center">形式</th>
<th style="text-align:center">链接</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">《力哥说理财_基金生财一课通》</td>
<td style="text-align:center">YouTube视频列表</td>
<td style="text-align:center"><a href="https://www.youtube.com/playlist?list=PLYtO8EzdUTxettH9QRNS74BRH2CbqGEyA">点击前往</a></td>
</tr>
<tr>
<td style="text-align:left">《力哥说理财_股票赚钱一课通》</td>
<td style="text-align:center">YouTube视频列表</td>
<td style="text-align:center"><a href="https://www.youtube.com/playlist?list=PLYtO8EzdUTxecWrKrgnebWhP-ZF0rJoiF">点击前往</a></td>
</tr>
<tr>
<td style="text-align:left">《富爸爸│金融知識101系列(2) 融券與做空》</td>
<td style="text-align:center">YouTube视频</td>
<td style="text-align:center"><a href="https://www.youtube.com/watch?v=eW94MvJWMvM">点击前往</a></td>
</tr>
</tbody>
</table>
</div>
<p><br><br><br></p>
<h1 id="计算机科学"><a href="#计算机科学" class="headerlink" title="计算机科学"></a>计算机科学</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">标题</th>
<th style="text-align:center">作者</th>
<th style="text-align:center">形式</th>
<th style="text-align:center">链接</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">《最好的C++教程》</td>
<td style="text-align:center">Cherno</td>
<td style="text-align:center">bilibili视频列表 / 英语+中文字幕</td>
<td style="text-align:center"><a href="https://www.bilibili.com/video/BV1VJ411M7WR">点击前往</a></td>
</tr>
<tr>
<td style="text-align:left">《Python Tutorials》</td>
<td style="text-align:center">Corey Schafer</td>
<td style="text-align:center">youtube视频列表 / 纯英语</td>
<td style="text-align:center"><a href="https://www.youtube.com/playlist?list=PL-osiE80TeTt2d9bfVyTiXJA-UTHn6WwU">点击前往</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
</div>
<p><br><br><br></p>
<h1 id="机械工程"><a href="#机械工程" class="headerlink" title="机械工程"></a>机械工程</h1><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">标题</th>
<th style="text-align:center">形式</th>
<th style="text-align:center">链接</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Learn Engineering</td>
<td style="text-align:center">blog &amp; YouTube视频</td>
<td style="text-align:center"><a href="https://learnengineering.org/">点击前往</a></td>
</tr>
</tbody>
</table>
</div>
<p><br><br><br></p>
<h1 id="物理"><a href="#物理" class="headerlink" title="物理"></a>物理</h1><div class="table-container">
<table>
<thead>
<tr>
<th>标题</th>
<th>形式</th>
<th>链接</th>
</tr>
</thead>
<tbody>
<tr>
<td>《Physics Videos by Eugene Khutoryansky》</td>
<td>youtube频道</td>
<td><a href="https://www.youtube.com/user/EugeneKhutoryansky/videos">点击前往</a></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>JavaLab</td>
<td>动画仿真</td>
<td><a href="https://javalab.org/en/">点击前往</a></td>
</tr>
</tbody>
</table>
</div>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p><strong>魔术揭秘教学</strong>：魔术师晓东 <a href="https://v.qq.com/s/videoplus/528637840">腾讯视频</a> | <a href="https://space.bilibili.com/97469480">B站视频</a></p>
]]></content>
  </entry>
  <entry>
    <title>object detection api调参详解</title>
    <url>/2020/07/01/object-detection-api%E8%B0%83%E5%8F%82%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a><strong>一、引言</strong></h2><p>使用谷歌提供的<code>object detection api</code>图像识别框架，我们可以很方便地重新训练一个预训练模型，用于自己的具体业务。以我所使用的ssd_mobilenet_v1预训练模型为例，训练所需参数都在<code>research\object_detection\samples\configs</code>文件夹下的<code>ssd_mobilenet_v1_coco.config</code>中预先配置了，只需对少量参数做修改即可训练。</p>
<p>但是如果你只会修改里面的类别数目和文件路径，那么就有很多缺点：一是无法理解训练参数背后的原理，不利于技术积累；二是一旦遇到需要优化的问题时，不知道如何调整训练参数。例如，我使用默认配置的训练参数对模型进行长期训练后，发现模型始终无法收敛，loss值一直在3~5的范围内波动，没有继续下降。但在没有弄清楚训练参数如何调整之前，我一直没能解决该问题。</p>
<p>所以，我们必须弄清楚每个训练参数的出处、含义、数值调整范围，才能自行对训练文件做合理配置，从而灵活解决各类训练问题。</p>
<p>本文以ssd_mobilenet_v1预训练模型为例，详细解释其训练参数的含义及调整范围。对其它预训练模型的训练参数的分析方法类似，不再逐一展开。</p>
<h2 id="二、正文"><a href="#二、正文" class="headerlink" title="二、正文"></a><strong>二、正文</strong></h2><p>首先简单解释一下，object_detection api框架将训练参数的配置、参数的可配置数值的声明、参数类的定义，分开放置在不同文件夹里。训练参数的配置放在了<code>object_detection\samples\configs</code>文件夹下的.config文件中，参数的可配置数值的声明写在了<code>object_detection\protos</code>文件夹下对应参数名的.proto文件中，参数类的定义则放在了object_detection总文件夹下对应参数名的子文件夹里或者是<code>object_detection\core</code>文件夹里。</p>
<p><em>*</em>.config文件里包含5个部分：model, train_config, train_input_reader, eval_config, eval_input_reader。以ssd_mobilenet_v1_pets.config为例，打开该文件，按照从上到下、从外层到内层的顺序，依次解释各训练参数。</p>
<span id="more"></span>
<p><strong>2.1  model{ }</strong></p>
<p>包含了ssd{ }。</p>
<p><strong>2.1.1  ssd { }</strong></p>
<p>包含了SSD算法使用的各类训练参数。从2.1.2开始逐个展开解释。</p>
<p><strong>2.1.2  num_classes</strong></p>
<p>类别数目。例如，我的数据集包含摩托车、自行车、小轿车，那么它就等于3。</p>
<p><strong>2.1.3  box_coder、faster_rcnn_box_coder</strong></p>
<pre><code>box_coder &#123;
    faster_rcnn_box_coder &#123;
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
    &#125;
&#125;
</code></pre><p>这部分参数用于设置box编解码的方式。可选参数值：</p>
<pre><code>FasterRcnnBoxCoder faster_rcnn_box_coder = 1;
MeanStddevBoxCoder mean_stddev_box_coder = 2;
SquareBoxCoder square_box_coder = 3;
KeypointBoxCoder keypoint_box_coder = 4;
</code></pre><p>SSD算法借鉴了Faster-RCNN的思想，所以这里的设置应该选faster_rcnn_box_coder。</p>
<p>Faster-RCNN中，bounding box的坐标值可以用两种不同的坐标系表示：一种坐标系以图片左上角作为原点，我称其为绝对坐标系；另一种坐标系以用于参考的anchor boxes的中心点位置作为原点，我称其为相对坐标系。</p>
<p>所谓box编码就是以anchor box为参照系，将box的绝对坐标值和绝对尺寸，转换为相对于anchor box的坐标值和尺寸。所谓box解码就是将box的相对坐标值和相对尺寸，转换回绝对坐标值和绝对尺寸。</p>
<p>在SSD算法中，box编解码中的box，是指预测框（predicted box）和真实框（ground-truth box）。SSD中的box编码，就是以anchor box为参照系，将predicted box和ground-truth box转换为用相对于anchor box的数值来表示；SSD中的box解码，则是将predicted box和ground-truth box转换回用绝对坐标系数值表示。</p>
<p>faster_rcnn_box_coder的解释详见<code>object_detection\box_coders</code>文件夹下的faster_rcnn_box_coder.py。重点分析一下下面这组转换公式：</p>
<pre><code>ty = (y - ya) / ha
tx = (x - xa) / wa
th = log(h / ha)
tw = log(w / wa)
</code></pre><p>x, y, w, h是待编码box的中心点坐标值、宽、高，即训练集中ground-truth box的数值；xa, ya, wa, ha是anchor box的中心点坐标值、宽、高; tx, ty, tw, th则是编码后的相对中心点坐标值、宽、高。在实际转换中，还会用缩放系数实现了归一化（normalization），归一化简单来说就是把数据分布变成标准正态分布（standard normal distribution），从中可以看出normalization就是来自标准正态分布的英文名词，而中文翻译成归一化是因为标准正态分布的大部分数据分布在（-1，1）之间。</p>
<p>faster_rcnn_box_coder{}里的y_scale、x_scale、height_scale、width_scale是对ty、tx、th、tw的放大比率，根据faster_rcnn_box_coder.py源码里编码/解码的语句可以看出，喂给模型的ty,tx,th,tw还会乘以/除以10和5。</p>
<p>关于上面谈到的归一化和放大比率，可阅读<a href="https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/">这篇博客</a>的[Representation Encoding With Variance]部分。</p>
<p><strong>2.1.4  matcher、argmax_matcher</strong></p>
<pre><code>matcher &#123;
    argmax_matcher &#123;
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
    &#125;
&#125;
</code></pre><p>这部分参数用于设置anchor box和ground-truth box之间的匹配策略。  </p>
<p>可选参数值：</p>
<pre><code>ArgMaxMatcher argmax_matcher = 1;
BipartiteMatcher bipartite_matcher = 2;
</code></pre><p>SSD算法中，采用了ArgMaxMatcher策略，所以这里选择argmax_matcher。所谓ArgMaxMatcher策略，就是选取最大值策略。在<code>object_detection\matchers</code>文件夹的argmax_matcher.py源码里有详细解释。</p>
<p>在SSD算法中，用anchor box和ground-truth box的IOU值（Intersection Over Union）来作为阈值，设置了matched_threshold、unmatched_threshold两个阈值，这两个阈值把anchor box分成了3段，然后再根据negatives_lower_than_unmatched决定某个低于matched_threshold的anchor box是 负样本 还是 被忽略：</p>
<ul>
<li><p>（1） 当IOU &gt;= matched_threshold时：anchor box和ground-truth box匹配，该anchor box记为正样本。</p>
</li>
<li><p>（2） 当matched_threshold &gt; IOU &gt;= unmatched_threshold时，取决于     negatives_lower_than_unmatched。如果negatives_lower_than_unmatched=true时：所有中间态anchor box被忽略，低于unmatched_threshold的anchor box被记为负样本；negatives_lower_than_unmatched=false时：所有中间态anchor box被记为负样本，低于unmatched_threshold则被忽略。上述参数例中两个阈值都是0.5，故没有中间态。</p>
</li>
<li><p>（3） 当IOU &lt;= unmatched_threshold 时，也取决于     negatives_lower_than_unmatched, 规则跟上一条一样，由negatives_lower_than_unmatched来决定该anchor box是 负样本 还是 被忽略。</p>
</li>
</ul>
<p>ignore_thresholds: 源码<code>object_detection/protos/argmax_matcher.proto</code>的解释:Whether to construct ArgMaxMatcher without thresholds.这么看肯定是false的啦，怎么能不用阈值构造呢。</p>
<p>force_match_for_each_row：设置为true，以确保每个ground-truth box都至少有一个anchor box与之对应，防止有些ground-truth没有anchor box对应。否则，这些ground-truth box最后将没有bounding box回归对应，也就是产生了漏检。</p>
<p>在SSD算法中，将所有ground-truth box按行排列、将所有anchor box按列排列，形成一个矩阵。矩阵的每一格记录ground-truth box和anchor box的匹配结果。匹配分两步：</p>
<p>（1） 对每行的ground-truth box，采用ArgMax策略，选择与它的IOU最大的anchor box进行匹配；</p>
<p>（2） 对剩余的每一个没有匹配到ground-truth box的anchor box，选择所有与它的IOU大于match threshold的ground-truth box进行匹配。</p>
<p>这样，每个ground-truth box至少有一个anchor box与它匹配。</p>
<p><strong>2.1.5  similarity_calculator、iou_similarity</strong></p>
<pre><code>similarity_calculator &#123;
    iou_similarity &#123;
    &#125;
&#125;
</code></pre><p>这个参数选择使用何种标准计算相似度。在2.1.4小节中，已经解释了SSD算法是采用IOU值来定量衡量相似度的，故这里选择数值iou_similarity。</p>
<p><strong>2.1.6  anchor_generator、ssd_anchor_generator</strong></p>
<pre><code>anchor_generator &#123;
    ssd_anchor_generator &#123;
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
    &#125;
&#125;
</code></pre><p>Anchor box的生成器可以有如下选择：</p>
<pre><code>GridAnchorGenerator grid_anchor_generator = 1;
SsdAnchorGenerator ssd_anchor_generator = 2;
MultiscaleAnchorGenerator multiscale_anchor_generator = 3;
</code></pre><p>ssd_anchor_generator这部分设置了生成anchor box所需的一些参数。详细解释可参考SSD的论文。这里只解释一下各参数的基本含义。</p>
<p>num_layers: 数值为6，代表提取特征用的6个层。SSD算法借鉴了特征金字塔的思想，从6个feature map层同步提取特征,其大小分别是(38,38),(19,19),(10,10),(5,5),(3,3),(1,1)。</p>
<p><img src="https://s1.ax1x.com/2020/07/01/NHYlh6.png" alt="NHYlh6.png"></p>
<p><img src="https://s1.ax1x.com/2020/07/04/Nv5OIO.jpg" alt="Nv5OIO.jpg"></p>
<p>图像特征提取的原理是Receptive Field（感受域）。如下图所示，由于图像中央位置的特征被kernel扫描的次数最多，所以提取到的特征最多、颜色最深，而图像四个角只被kernel扫描过1、2次，所以提取到的特征最少、颜色最浅。因此，CNN对图像中越靠近中央位置、体型越大的物体的判断越准确。换句话说，边缘位置的物体识别难度更大。<a href="https://www.jianshu.com/p/8d894605bb06">[来源]</a></p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201129234224.png" alt="img"></p>
<p>min_scale和max_scale：我目前的理解是：scale是同层的anchor boxes中的小正方形宽相对于resize的输入图像宽的比率。在SSD论文中，min_scale是指该比率在最低层feature map的值，max_scale是指该比率在最高层feature map的值。至于中间4层feature map上的比率值，论文中是以线性插值的方式来获得的（但参考代码中不是这样确定各层比率的）。更多关于SSD锚框生成的原理见<a href="https://zhuanlan.zhihu.com/p/33544892">这篇知乎文章</a>。</p>
<p>aspect_ratios：指定了同一个feature map层上的anchor box的宽长比。例如，这里aspect ratios指定了5种宽长比，利用图1的公式（Sk: scale*resize width; ar: aspect_ratios）可以计算出6种不同宽长的anchor boxes（包括2种正方形、4种长方形）。注意：某些feature map层不使用3和1/3这一对aspect_ratios，故只生成4个anchor boxes。详细解释可参考SSD的论文。另外这些值的设置一定要根据数据集的特点来，比如检测目标是电线杆这种细长的东西，原本的那6个值就不适用了，如何使用kmeans方法聚类得到合适的anchor，可参见<a href="https://mp.weixin.qq.com/s/2CAA4i9Nml43g5oc4p2-0Q">这篇微信推文</a>。</p>
<p><strong>2.1.7  image_resizer、fixed_shape_resizer</strong>   </p>
<pre><code>image_resizer &#123;
    fixed_shape_resizer &#123;
    height: 300
    width: 300
    &#125;
&#125;
</code></pre><p>这部分参数设置了对输入图像的resize策略。可选参数：</p>
<pre><code>KeepAspectRatioResizer keep_aspect_ratio_resizer = 1;
FixedShapeResizer fixed_shape_resizer = 2;
</code></pre><p>传统SSD300模型中，输入图像被统一resize为300*300。故这里选择fixed_shape_resizer，且height和width均设置为300。</p>
<p><strong>2.1.8  box_predictor</strong></p>
<pre><code>box_predictor &#123;
    convolutional_box_predictor &#123;
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams &#123;
            activation: RELU_6,
            regularizer &#123;
                l2_regularizer &#123;
                    weight: 0.00004
                &#125;
            &#125;
            initializer &#123;
                truncated_normal_initializer &#123;
                    stddev: 0.03
                    mean: 0.0
                &#125;
            &#125;
            batch_norm &#123;
                train: true,
                scale: true,
                center: true,
                decay: 0.9997,
                epsilon: 0.001,
            &#125;
        &#125;
    &#125;
&#125;
</code></pre><p>Box predictors输入高层的feature map，输出两类预测：（1）编码后的predicted box的相对位置；（2）predicted box里的物体类别。</p>
<p>Box predictor的可选参数范围：</p>
<pre><code>ConvolutionalBoxPredictor convolutional_box_predictor = 1;
MaskRCNNBoxPredictor mask_rcnn_box_predictor = 2;
RfcnBoxPredictor rfcn_box_predictor = 3;
WeightSharedConvolutionalBoxPredictor weight_shared_convolutional_box_predictor = 4;
</code></pre><p>这里选用了convolutional_box_predictor，其含义是在所有feature maps后额外增加一个中间1x1卷积层。选择原因不明。</p>
<p>关于convolutional_box_predictor{ }内的许多参数的含义及数值范围，详见predictors文件夹下的convolutional_box_predictor.py文件（此文件在models 1.x和models 2.x里的内容不一样）。下面逐个简单说明一下。</p>
<p>min_depth和max_depth：在位置回归和类别分类之前额外插入的feature map层深度的最小值和最大值。当max_depth=0时，表示在位置回归和类别分类之前，不额外插入feature map。</p>
<p>num_layers_before_predictor：在检测器之前的额外convolutional层的层数。</p>
<p>use_dropout：对class prediction是否使用dropout来防止过拟合。</p>
<p>dropout_keep_probability：如果使用了dropout，dropout的数值保留概率。</p>
<p>kernel_size：最后的卷积核的尺寸。</p>
<p>box_code_size：我的理解是box需要编码的参数个数。SSD算法里是cx, cy, w, h这4个参数需要编码，所以box_code_size=4。</p>
<p>apply_sigmoid_to_scores：最后class prediction输出时是否采用sigmoid。</p>
<p>conv_hyperparams{ }：卷积操作超参数的设置。box_predictor和feature_extractor都有conv_hyperparams{}，conv_hyperparams{}里有activation、regularizer、initializer、batch_norm。源码在<a href="https://github.com/tensorflow/models/blob/451906e4e82f19712455066c1b27e2a6ba71b1dd/research/object_detection/builders/hyperparams_builder.py#L27:7">hyperparams_builder.py</a>。</p>
<p>activation：激活函数。目前可以选择NONE、RELU、RELU_6。这里选择了RELU_6。关于激活函数的细节，请自行查阅资料。</p>
<p>regularizer：正则化操作。目前可以选择l1_regularizer、l2_regularizer。这里选择了l2_regularizer。例子里L2_regularizer的weight值只有0.00004，所以正则化操作对loss的影响较小。</p>
<p>Initializer{ }：随机数初始化机制设置。可选参数如下：</p>
<pre><code>TruncatedNormalInitializer truncated_normal_initializer = 1;
VarianceScalingInitializer variance_scaling_initializer = 2;
RandomNormalInitializer random_normal_initializer = 3; 
</code></pre><p>这里选择了truncated_normal_initializer。</p>
<p>truncated_normal_initializer：截断的正态分布随机数初始化机制。如果数值超过mean两个stddev，则丢弃。</p>
<p>batch_norm{ }：关于Batch Normalization（批归一化）的一些参数设置。Batch Normalization可以强制将输入激活函数的数值分布拉回标准正态分布，以防止训练过程中产生反向梯度消失，加速训练收敛。批归一化中会用到两个参数γ和β，分别定量化缩放和平移操作。细节请自行参阅相关资料。这里解释一下batch_norm的一些参数：：</p>
<p>train: true——如果为true，则在训练过程中batch norm变量的值会更新，也就是得到了训练。如果为false，则在训练过程中batch norm变量的值不会更新。</p>
<p>scale: true——如果为true,则乘以γ；如果为false,则不使用γ。</p>
<p>center: true——如果为true,则加上β的偏移量；如果为false,则忽略β。</p>
<p>decay: 0.9997——衰减率。作用存疑。</p>
<p>epsilon: 0.001——添加到方差的小浮点数,以避免除以零。</p>
<p>关于Box predictor的源码解释详见core文件夹下的box_predictor.py。</p>
<p><strong>2.1.9  feature_extractor</strong></p>
<pre><code>feature_extractor &#123;
    type: &#39;ssd_mobilenet_v1&#39;
    min_depth: 16
    depth_multiplier: 1.0
    conv_hyperparams &#123;
    activation: RELU_6,
        regularizer &#123;
            l2_regularizer &#123;
                weight: 0.00004
            &#125;
        &#125;
        initializer &#123;
            truncated_normal_initializer &#123;
                stddev: 0.03
                mean: 0.0
            &#125;
        &#125;
        batch_norm &#123;
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
        &#125;
    &#125;
&#125;
</code></pre><p>这部分设置了特征提取器相关层的参数。</p>
<p>不同模型的feature extractor的基类定义，详见meta_architectures文件夹下的对应文件。例如SSD模型的SSDFeatureExtractor基类，定义在<code>object_detection\meta_architectures</code>文件夹下的ssd_meta_arch.py文件里。</p>
<p>不同预训练模型的feature extractor的类定义，详见models文件夹下的对应文件。例如ssd_mobilenet_v1预训练模型的feature extractor类，定义在<code>object_detection\models</code>文件夹下的</p>
<p>ssd_mobilenet_v1_feature_extractor.py文件里。</p>
<p>models文件夹下的feature extractor类，是meta_architectures文件夹下的feature extractor基类的子类。</p>
<p>下面解释一下相关参数。</p>
<p>type：例子中使用了预训练模型ssd_moiblenet_v1的feature_extractor，所以这里填写’ssd_mobilenet_v1’。</p>
<p>min_depth：最小的特征提取器的深度。这里填16。原因不明。</p>
<p>depth_multiplier：是施加在每个input通道上的卷积核的数目。用于计算深度卷积分离后的输出通道总数。这里是一个浮点数。</p>
<p>conv_hyperparams超参数的含义和配置同2.1.8小节，不再重复解释。</p>
<p><strong>2.1.10  loss</strong></p>
<pre><code>loss &#123;
    classification_loss &#123;
        weighted_sigmoid &#123;
        &#125;
    &#125;
    localization_loss &#123;
        weighted_smooth_l1 &#123;
        &#125;
    &#125;
    hard_example_miner &#123;
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
    &#125;
    classification_weight: 1.0
    localization_weight: 1.0
&#125;
</code></pre><p>这部分设置了损失函数loss相关的参数，详细源码解释见<code>object_detection\core\losses.py</code>或者github上的<a href="https://github.com/tensorflow/models/blob/420a7253e034a12ae2208e6ec94d3e4936177a53/research/object_detection/core/losses.py">源码</a>。</p>
<p>SSD算法的loss分为目标分类损失函数(classification loss)和目标位置损失函数（localization loss）：</p>
<p><img src="https://s1.ax1x.com/2020/07/04/NvoM1H.png" alt="NvoM1H.png"></p>
<p>这里设置了classification_loss、localization_loss、hard_example_miner、classification_weight、localization_weight这5个参数。下面解释一下这5个参数的具体配置。</p>
<p>classification_loss：可选参数：</p>
<pre><code>WeightedSigmoidClassificationLoss weighted_sigmoid = 1;

WeightedSoftmaxClassificationLoss weighted_softmax = 2;

WeightedSoftmaxClassificationAgainstLogitsLoss weighted_logits_softmax = 5;

BootstrappedSigmoidClassificationLoss bootstrapped_sigmoid = 3;

SigmoidFocalClassificationLoss weighted_sigmoid_focal = 4;
</code></pre><p>这里用到的是weighted_sigmoid, <a href="https://github.com/tensorflow/models/blob/8518d053936aaf30afb9ed0a4ea01baddca5bd17/research/object_detection/core/losses.py#L217">源码</a>。论文里是说用softmax cross entropy，但这里config用的是sigmoid cross entropy，这个问题在<a href="https://github.com/tensorflow/models/issues/2213">github issue</a>也有人提到，有人说用weighted_softmax，准确度会降低。我在tensorflow/models的github里搜了一下weighted_softmax，发现没有一个config用了weighted_softmax，可能是它真的不好用吧。</p>
<p>定义分类输出的激活函数。具体含义请自行查阅资料。这里的设置和前面Box predictor部分的apply_sigmoid_to_scores设置是否有冲突，暂时没能确认。</p>
<p>localization_loss：可选参数：</p>
<pre><code>WeightedL2LocalizationLoss weighted_l2 = 1;

WeightedSmoothL1LocalizationLoss weighted_smooth_l1 = 2;

WeightedIOULocalizationLoss weighted_iou = 3;
</code></pre><p>这里用到的是weighted_smooth_l1,<a href="https://github.com/tensorflow/models/blob/8518d053936aaf30afb9ed0a4ea01baddca5bd17/research/object_detection/core/losses.py#L145">源码</a>里的定义是：Smooth L1 localization loss function aka Huber Loss. The smooth L1_loss is defined elementwise as .5 x^2 if |x| &lt;= delta and delta <em> (|x|- 0.5</em>delta) otherwise, where x is the difference between predictions and target.中文关于Huber loss的介绍见这篇<a href="https://www.cnblogs.com/nowgood/p/Huber-Loss.html">博客</a>。</p>
<p>定义了用于localization loss的正则化方法。具体含义请自行查阅资料。这里的设置和前面Box predictor部分的正则化设置是否有冲突，暂时没能确认。</p>
<p>hard_example_miner：难样本挖掘策略。<br>因为ssd-300共有8732个先验框，前期训练的时候，并不是每一张图片输进去，就基于这8732个先验框给一个损失函数，然后反向传播。而是在8732个先验框中有针对性的选择一部分来作为训练数据，包括正例（含有目标的框）和反例（背景框），反例（背景框）的选取还是有讲究的，毕竟大部分框都是背景框，如果都拿来训练，那么会造成严重的类别不平衡。SSD算法随机抽取一定数量的负样本，按loss_type的损失量进行降序排列，选择前3N个作为训练用负样本，以保证训练时的正负样本比例接近1:3，N为正样本数量。那这些筛选过的样本去训练后会出现许多false positive。把其中得分较高的这些false positive当做所谓的Hard negative，既然mining出了这些Hard negative，就把这些扔进网络再训练一次，从而加强分类器判别假阳性的能力。训练hard negative对提升网络的分类性能具有极大帮助，因为它相当于一个错题集。下面解释一下它的具体子参数含义。</p>
<p>num_hard_examples: 3000——难样本数目。</p>
<p>iou_threshold: 0.99——在NMS（非极大抑制）阶段，如果一个样本相对于最大loss样本的IOU值比此阈值高，则认为此样本是重复的、可丢弃。SSD里设置成0.99 几乎相当于不进行NMS，不丢弃任何负样本，只是单纯的按损失排序。这个参数尽量不要动，否则就违背了SSD模型的原意。</p>
<p>loss_type: CLASSIFICATION——挖掘策略是否只使用classification loss，或只使用localization loss，或都使用。可选参数：</p>
<pre><code>BOTH = 0; 
CLASSIFICATION = 1; 
LOCALIZATION = 2;
</code></pre><p>max_negatives_per_positive: 3   —-  每1个正样本对应的最大负样本数。</p>
<p>min_negatives_per_image: 0  —  在ssd_mobilenet_v2_coco.config里，该值=3，在图片没有正样本的极端情况下，如果把这个值设置为一个正数，可以避免模型在图片上检测出目标来，防止了误检出。</p>
<p>classification_weight：用于配置classifiation loss在总loss中的权重。</p>
<p>localization_weight：用于配置localization loss在总loss中的权重。</p>
<p><strong>2.1.11  normalize_loss_by_num_matches</strong></p>
<p>我的理解：如果选true,则根据匹配的样本数目归一化总loss，也就是总loss公式中，用加权的classification loss和加权的localization loss计算出总loss后，还要再除以一个正样本总数N。如果选false，则计算出总loss后，不用再除以正样本总数N。</p>
<p><strong>2.1.12  post_processing</strong></p>
<pre><code>post_processing &#123;
batch_non_max_suppression &#123;
    score_threshold: 1e-8
    iou_threshold: 0.6
    max_detections_per_class: 100
    max_total_detections: 100
&#125;
score_converter: SIGMOID
&#125;
</code></pre><p>这部分配置了SSD算法的后处理阶段的参数。下面逐一解释。</p>
<p>batch_non_max_suppression{ }：这部分配置了批次的NMS策略的参数。先简单解释下NMS策略的目的。以目标检测为例，在最后阶段，一个目标上可能有很多个bounding box，但是最终目标检测框只有一个。因此，我们可以用NMS策略，逐次过滤掉其余的bounding box，最终只保留一个bounding box作为结果。下面简单解释一下具体参数含义：</p>
<p>score_threshold: 1e-8 —— 分数低于此阈值的box被过滤掉,想参与NMS的门都没有。</p>
<p>iou_threshold: 0.6 —— 和置信度最高的box的IOU值超过此阈值的box被过滤掉，如果场景是目标比较密集，那这个值要设大一点，如果场景比较稀疏，这个值要设小一点。</p>
<p>max_detections_per_class: 10 —— 每个类别可保留的检测框的最大数目。</p>
<p>max_total_detections: 100 —— 所有类别可保留的检测框的最大数目，一般这个值应该等于max_detections_per_class乘以类别数。</p>
<p>score_converter：检测分数的转换器类型选择。可选参数：</p>
<pre><code>// Input scores equals output scores.
IDENTITY = 0;

// Applies a sigmoid on input scores.
SIGMOID = 1;

// Applies a softmax on input scores.
SOFTMAX = 2;
</code></pre><p><strong>2.2  train_config{ }</strong></p>
<p>训练用参数的配置。详见protos文件夹下的train.proto。下面解释.config例中的参数。</p>
<ul>
<li><p><strong>2.2.1  batch_size</strong></p>
<p>每个批次的训练样本数。batch_size × num_steps = num_examples × epochs。batch size不是说你硬件能力强就能设置得很大，太大的batch size会导致参数的更新次数变少，模型的质量（泛化能力）会变差。</p>
</li>
<li><p><strong>2.2.2  optimizer{ }</strong></p>
<p>优化器的参数配置部分。</p>
<p>由于优化器的配置很关键，所以这部分想更详细展开一些。首先介绍一下参数的含义及可选范围，然后贴出一个优化器配置的例子。</p>
<p>目前可选的优化器参数：</p>
<pre><code>  RMSPropOptimizer rms_prop_optimizer
  MomentumOptimizer momentum_optimizer
  AdamOptimizer adam_optimizer
</code></pre><p>关于这三种优化器的特性，可查看我的另一篇<a href="https://wwdok.github.io/2020/06/27/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">文章</a>。</p>
<p> rms_prop_optimizer的可选参数：</p>
<pre><code>  LearningRate learning_rate = 1;
  float momentum_optimizer_value = 2 [default = 0.9];
  float decay = 3 [default = 0.9];
  float epsilon = 4 [default = 1.0];
</code></pre><p>momentum_optimizer的可选参数：</p>
<pre><code>  LearningRate learning_rate = 1;
  float momentum_optimizer_value = 2 [default = 0.9];
</code></pre><p>adam_optimizer的可选参数：</p>
<pre><code>  LearningRate learning_rate = 1;
</code></pre><p>3种优化器都有学习率learning_rate，它的的可选参数有：</p>
<pre><code>  ConstantLearningRate constant_learning_rate = 1;
  ExponentialDecayLearningRate exponential_decay_learning_rate = 2;
  ManualStepLearningRate manual_step_learning_rate = 3;
  CosineDecayLearningRate cosine_decay_learning_rate = 4;
</code></pre><p>  解释如下：</p>
<p>  ============（1）============</p>
<p>  <strong>constant_learning_rate</strong>：恒定学习率。恒定学习率太小则收敛很慢；太大则在极值附近震荡难以收敛。故一般不会使用。</p>
<p>  ============（2）============</p>
<p>  <strong>exponential_decay_learning_rate</strong>：学习率按照指数规律衰减。</p>
<p>  exponential_decay_learning_rate可选参数：</p>
<pre><code>  float initial_learning_rate [default = 0.002];
  uint32 decay_steps [default = 4000000];
  float decay_factor [default = 0.95];
  bool staircase [default = true];
  float burnin_learning_rate [default = 0.0];
  uint32 burnin_steps [default = 0];
  float min_learning_rate [default = 0.0];
</code></pre><p>  initial_learning_rate：初始学习率数值。</p>
<p>  decay_steps：衰减周期。即每隔decay_steps步衰减一次学习率。下面例1中写的是800720步，而总的训练步数不过才200000步，显然decay_steps的设置偏大了，导致在整个训练过程中，学习率实际上没有明显的指数衰减。这个设置不合理。这个decay_steps应该等于训练总步数num_steps。</p>
<p>  decay_factor：每次衰减的衰减率。</p>
<p>  staircase：是否阶梯性更新学习率，也就是每次衰减结果是向下取整还是float型。</p>
<p>  burnin_learning_rate：采用burnin策略进行调整的学习率（初始值？）。SSD算法中，是否有burnin策略、buinin策略又是如何调整学习率的，目前我还不太清楚。存疑。参考：在yolov3所用的darknet中，当学习率更新次数小于burnin参数时，学习率从小到大变化；当更新次数大于burnin参数后，学习率按照配置的衰减策略从大到小变化。</p>
<p>  burnin_steps：按照字面意思，是burnin策略的调整周期。即每隔burnin_steps步调整一次burnin_learning_rate。</p>
<p>  min_learning_rate：最小学习率。采用衰减策略变小的学习率不能小于该值。</p>
<p>  ============（3）============</p>
<p>  <strong>manual_step_learning_rate</strong>：学习率按照人工设置的step逐段变小。</p>
<p>  manual_step_learning_rate可选参数：</p>
<pre><code>  float initial_learning_rate = 1 [default = 0.002];
  message LearningRateSchedule &#123;
  　　optional uint32 step = 1;
  　　optional float learning_rate = 2 [default = 0.002];
  &#125;
  repeated LearningRateSchedule schedule = 2;
  optional bool warmup = 3 [default = false];
</code></pre><p>  简单解释如下：</p>
<p>  initial_learning_rate：初始学习率数值。</p>
<p>  schedule：人工规划策略。包含两个参数：</p>
<p>  step——当前阶梯从全局的第step步开始。</p>
<p>  learning_rate——当前阶梯的学习率。</p>
<p>  warmup：对于全局步数区间[0, schedule.step]之间的steps，是否采用线性插值法来确定steps对应的学习率。缺省是false。</p>
<p>  ============（4）============</p>
<p>  <strong>cosine_decay_learning_rate</strong>：学习率按照余弦规律衰减。<br>  <img src=https://3.bp.blogspot.com/-fAN358JEMLc/Wrv1iH17eiI/AAAAAAAAChg/0djM3boHeJA_V_JfBHH8dMS32ekgtic7QCLcBGAs/s1600/image1.png></p>
<p>  它有四个配置参数：</p>
<pre><code>    learning_rate_base: .2
    total_steps: 50000
    warmup_learning_rate: 0.06
    warmup_steps: 2000
</code></pre><p>  =============================</p>
<p>  优化器还有3个独立参数：</p>
<p>  momentum_optimizer_value： momentum超参数。通过引入这个超参数（公式中一般记为γ），可以使得优化在梯度方向不变的维度上的更新速度变快，在梯度方向有所改变的维度上的更新速度变慢，从而加快收敛并减小震荡。</p>
<p>  decay：衰减率。含义和出处不明。</p>
<p>  epsilon：可能是迭代终止条件。</p>
</li>
</ul>
<p><strong>优化器配置例子：</strong></p>
<p>优化器使用rms_prop_optimizer。</p>
<p>采用指数衰减策略来调整学习率。</p>
<pre><code>optimizer &#123;
    rms_prop_optimizer: &#123;
        learning_rate: &#123;
            exponential_decay_learning_rate &#123;
                initial_learning_rate: 0.0001
                decay_steps: 800720
                decay_factor: 0.95
            &#125;
        &#125;
        momentum_optimizer_value: 0.9
        decay: 0.9
        epsilon: 1.0
    &#125;
&#125;
</code></pre><p><strong>2.2.3  fine_tune_checkpoint</strong></p>
<p>用于设置预训练模型的参数文件model.ckpt的路径。该参数文件用于精调。当训练开始时，导入已预训练好的模型参数，可以缩短训练过程。从零开始训练时，由于没有预训练模型的参数文件，故可以屏蔽这个路径参数。</p>
<p><strong>2.2.4  fine_tune_checkpoint_type</strong></p>
<p>fine_tune_checkpoint_type：用来确定fine tune checkpoint使用的是分类模型参数还是检测模型参数。可选参数值：“”，“classification”，“detection”。 </p>
<p><strong>2.2.5  load_all_detection_checkpoint_vars</strong></p>
<p>用于确定是否导入所有和模型变量名字和大小相符的detection checkpoint变量。只在使用检测模型时有效。</p>
<p><strong>2.2.6  num_steps</strong></p>
<p>训练总步数。如果设置为0，则训练步数为无穷大。</p>
<p><strong>2.2.7  data_augmentation_options</strong></p>
<p>这个例子里数据增强使用了两个具体参数：</p>
<p>random_horizontal_flip——随机水平翻转。</p>
<p>ssd_random_crop——SSD算法图像随机裁剪。</p>
<p>常用的还有random_adjust_brightness、<br>random_adjust_contrast、<br>random_adjust_saturation。</p>
<p>更多可选参数详见protos文件夹下的<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/protos/https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto">preprocessor.proto</a>。</p>
<p><strong>2.3  train_input_reader{ }</strong></p>
<p>训练集数据的路径配置。可选参数详见protos文件夹下的input_reader.proto。</p>
<ul>
<li><p><strong>2.3.1  tf_record_input_reader、input_path</strong></p>
<p>训练用tf_record格式数据集的路径配置。</p>
</li>
<li><p><strong>2.3.2  label_map_path</strong></p>
<p>labelmap.pbtxt文件的路径配置。labelmap.pbtxt文件定义了待分类目标的id号和标签名称之间的映射关系。</p>
</li>
</ul>
<p><strong>2.4  eval_config{ }</strong></p>
<p>测试用参数的配置。可选参数详见protos文件夹下的eval.proto。</p>
<ul>
<li><p><strong>2.4.1  metrics_set</strong></p>
<p>用于配置评估模型性能的标准。</p>
<p>可选参数详见框架总目录下eval_util.py里的EVAL_METRICS_CLASS_DICT。目前有8种。</p>
<p>例子中使用的是coco_detection_metrics, 是使用coco数据集进行目标检测时评估模型性能的标准。</p>
</li>
<li><p><strong>2.4.2  num_examples</strong></p>
<p>测试集的样本数量。</p>
</li>
</ul>
<p><strong>2.5  eval_input_reader{ }</strong></p>
<p>测试集数据的路径配置。可选参数详见protos文件夹下的input_reader.proto。</p>
<ul>
<li><p><strong>2.5.1  tf_record_input_reader、input_path</strong></p>
<p>测试用tf_record格式数据集的路径配置。</p>
</li>
<li><p><strong>2.5.2  label_map_path</strong></p>
<p>labelmap.pbtxt文件的路径配置。labelmap.pbtxt文件定义了待分类目标的id号和标签名称之间的映射关系。</p>
</li>
<li><p><strong>2.5.3  shuffle</strong></p>
<p>随机排序操作配置。如果选false，则对测试样本不进行随机排序操作。</p>
</li>
<li><p><strong>2.5.4  num_readers</strong></p>
<p>用于配置可并行读入的文件分片的数目。</p>
</li>
</ul>
<p>==================================================</p>
<p>以上的配置只是针对ssd_mobilenet_v1_coco.config，再阅读后来的config时，比如ssd_mobilenet_v2_quantized_300x300_coco.config，我们还会看到一些稍微不同的配置：</p>
<ul>
<li><p>对于一些想要量化的模型，它的配置文件最后还会有下面的内容：</p>
  <figure class="highlight"><table><tr><td class="code"><pre><span class="line">graph_rewriter &#123;</span><br><span class="line">    quantization &#123;</span><br><span class="line">        delay: 48000  </span><br><span class="line">        activation_bits: 8 </span><br><span class="line">        weight_bits: 8</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  delay的源码解释是“ Number of steps to delay before quantization takes effect during training.”<br>  delay值一般等于num_steps的90% ~ 96%，90%是偏向那些总步数比较少的情况，比如2千步。96%是偏向于总步数比较多的情况，比如5万步。</p>
</li>
<li><p>classification_loss 由weighted_sigmoid { }变成了weighted_sigmoid_focal {<br>alpha: 0.75, gamma: 2.0 },focal loss的定义如下：<br><img src="https://s1.ax1x.com/2020/07/16/U05Aeg.png" alt="U05Aeg.png" border="0" /><br><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201129234351.png" alt="img"></p>
<p>  从数学公式可以看出，focal loss是scale版的cross entropy，-(1-pt)^γ是可训练的scale值。 Focal loss对well-classified examples降权，降低它们的loss值，也就是减少参数更新值，把更多优化空间留给预测概率较低的样本。</p>
<p>  当gamma == 0时，focal loss就相当于corss entropy(CE)，如蓝色曲线所示，即使probability达到0.6，loss值还是&gt;= 0.5，就好像说：“我判断它不是类别B的概率是60%，恩，我还要继续努力优化参数，我行的，其他事情不要来烦我，我要跟它死磕到底”。而当gamma == 2时，同样是probability达到0.6，loss值却接近于0，就好像是说：“我判断它不是类别B的概率是60%，恩，根据我多年断案经验，它一定不是分类B，虽然判断依据不是很高，但我宣布，结案了，这页翻过去了，接下来我要把精力投入到那些预测准确率还很低的案子”。[<a href="https://www.jianshu.com/p/ad20e1a429fc">来源</a>]</p>
<p>  另外如果用了focal loss，就不能再用HardExampleMiner 。<a href="https://www.zhihu.com/question/293369755/answer/862562443">有人说</a>用了focal loss后，建议把unmatched_threshold调低到0.3,为了忽略掉中间状态的样本，这样focal loss 就不会给这些中间状态（非困难样本）赋予太大的权重。</p>
</li>
<li><p><strong>freeze_batchnorm</strong>: whether to freeze batch norm parameters during training</p>
<pre><code>  or not. When training with a small batch size (e.g. 1), it is desirable
  to freeze batch norm update and use pretrained batch norm params.
</code></pre></li>
<li><strong>inplace_batchnorm_update</strong>: whether to update batch norm moving average<pre><code>  values inplace. When this is false train op must add a control
  dependency on tf.graphkeys.UPDATE_OPS collection in order to update
  batch norm statistics.
</code></pre></li>
<li><strong>use_depthwise</strong> 在SSDLite的config中，box_predictor 和feature_extractor里会多出来 use_depthwise=true。</li>
<li><strong>kernel_size</strong> 为了配合use_depthwise=true，kernel_size会从1变成3。StackOverFlow上有关于SSD和SSDLIte的<a href="https://stackoverflow.com/questions/50674448/what-is-the-different-between-ssd-and-ssd-lite-tensorflow">提问</a>。两者的对比：</li>
</ul>
<p><img src=https://pic2.zhimg.com/80/v2-e9d9684b673410de83a3ada1315a82b0_720w.jpg width=60%></p>
<p><strong>实战心得</strong></p>
<p>在我使用自定义的ssd_mobilenet_v2_quantized_300x300_coco.config训练自己的数据集时，发现它对小目标的检测正确率几乎为0，后来在TensorBoard里点开测试图片，才发现原来我的数据集图片被resize成300×300后，bbox里的目标有多“马赛克”，比如下面这张：</p>
<p><img src="https://s1.ax1x.com/2020/08/16/dEZOQU.png" alt="dEZOQU.png"></p>
<p>放大后看是这样的：</p>
<p><img src="https://s1.ax1x.com/2020/08/16/dEePW6.png" alt="dEePW6.png"></p>
<p>数一下，左边那个目标的宽度只有11个像素！而且凭感觉，也很难看出这么少的像素能提取出多好的特征！我看我下次要试试320×320了！</p>
<blockquote>
<p>本文主要基于<a href="https://www.cnblogs.com/hillsea/p/13216716.html"><strong>这篇博客</strong></a>加上自己的理解思考而来。<br><br>拓展阅读：<br><br><a href="https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/">《Bounding Box Encoding and Decoding in Object Detection》</a><br><br><a href="https://zhuanlan.zhihu.com/p/33544892">《目标检测|SSD原理与实现》</a><br><br><a href="https://zhuanlan.zhihu.com/p/31427288">《SSD目标检测》</a><br><br><a href="https://mp.weixin.qq.com/s/2CAA4i9Nml43g5oc4p2-0Q">《新手也能彻底搞懂的目标检测Anchor是什么？怎么科学设置？》</a><br><br><a href="https://www.cnblogs.com/lliuye/p/9354972.html">《L1正则化与L2正则化的理解》</a><br><br><a href="https://blog.csdn.net/red_stone1/article/details/80755144">《机器学习中 L1 和 L2 正则化的直观解释》</a><br><br><a href="https://zhuanlan.zhihu.com/p/35356992">《L1正则化与L2正则化》</a><br><br><a href="https://zhuanlan.zhihu.com/p/22159946">《Sigmoid vs Softmax 输出层选择》</a><br><br><a href="https://docs.google.com/presentation/d/1rtfeV_VmdGdZD5ObVVpPDPIODSDxKnFSU0bsN_rgZXc/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.g178a005570_0_21143">《Single shot MultiBox Detector<br>》俄文版</a><br><a href="https://zhuanlan.zhihu.com/p/70703846">《轻量级神经网络“巡礼”（二）—— MobileNet，从V1到V3》</a><br><br><a href="https://leimao.github.io/blog/Focal-Loss-Explained/">《Use Focal Loss To Train Model Using Imbalanced Dataset》</a><br><br><a href="https://cloud.tencent.com/developer/article/1396369">《COCO 数据集目标检测等相关评测指标》</a></p>
</blockquote>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>调参</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title>用python投资理财</title>
    <url>/2020/06/30/%E7%94%A8python%E6%8A%95%E8%B5%84%E7%90%86%E8%B4%A2/</url>
    <content><![CDATA[<p>本篇博客主要围绕我在网易云课堂观看的网课《数据分析师微专业直播》整理的一些笔记资料。<br>题目是叫数据分析，不过里面的内容基本都是关于金融理财相关的知识和案例。</p>
<p>直播回放：<a href="https://study.163.com/sl/uRh">https://study.163.com/sl/uRh</a></p>
]]></content>
      <categories>
        <category>理财</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>量化投资</tag>
        <tag>理财</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习colab notebook合集</title>
    <url>/2020/06/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Colab%20NoteBook%E5%90%88%E9%9B%86/</url>
    <content><![CDATA[<h2 id="为什么要使用Colab？"><a href="#为什么要使用Colab？" class="headerlink" title="为什么要使用Colab？"></a><strong>为什么要使用Colab？</strong></h2><p>有时，为了尝试一项新技术或展示一个新概念，而安装一个程序所有需要的库非常耗时，并且会使您的计算机处于不同的状态。良心的Google Colab解决了我们这个痛点。在Google Colab上运行代码程序很快，毕竟谷歌的服务器性能不会太差，而且还有免费的GPU使用。当你在Colab上跑通了一个模型和程序，看到可视化结果后你是不是学起来更有兴趣，信心也更足了呢 ！</p>
<p>GitHub上已经有仓库聚合了各个领域的Colab Notebook。</p>
<ol>
<li><p><a href="https://github.com/tugstugi/dl-colab-notebooks">tugstugi/dl-colab-notebooks</a></p>
</li>
<li><p><a href="https://github.com/firmai/awesome-google-colab">firmai/awesome-google-colab</a></p>
</li>
</ol>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a><strong>目标检测</strong></h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">使用tensorflow训练能检测手枪的mobilenet ssd模型（带数据增强）</td>
<td style="text-align:center"><a href="https://colab.research.google.com/drive/1lPyjbl2YgFEvX5t1Mqe7lbN5AtUtiyd4">打开colab notebook</a></td>
</tr>
<tr>
<td style="text-align:left">使用darknet训练能检测车牌的yolo-v4模型</td>
<td style="text-align:center"><a href="https://colab.research.google.com/drive/1BrfRxLmAP3viHGhTrBT__-dWDvfoI3Wt#scrollTo=O2w9w1Ye_nk1">打开colab notebook</a></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:center">打开colab notebook</td>
</tr>
</tbody>
</table>
</div>
<h2 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a><strong>目标跟踪</strong></h2><div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
<tr>
<td></td>
<td>打开colab notebook</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>colab</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
        <tag>colab</tag>
        <tag>notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习里的验证集</title>
    <url>/2020/06/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%87%8C%E7%9A%84%E9%AA%8C%E8%AF%81%E9%9B%86/</url>
    <content><![CDATA[<p>在机器学习中，我们经常会看到训练集、验证集、测试集，比如coco数据集就分为了train，val，test，但我发现我在在训练yolov5、nanodet时，只需要提供两个部分子数据集，也就是训练集和验证集/测试集，在yolov5里代码直接写成验证集等于测试集，为什么可以这么做呢？</p>
<h2 id="前世-划分验证集的必要性"><a href="#前世-划分验证集的必要性" class="headerlink" title="前世-划分验证集的必要性"></a><strong>前世-划分验证集的必要性</strong></h2><p>我们先来回顾下模型调参的简单流程。</p>
<p>首先，我们建立一个模型，将数据集分成训练集和测试集；然后，我们用模型拟合训练集，用测试集来评价模型的精度；接着，我们更换模型的参数，再次拟合并测试精度；最后，我们选出精度最高时所对应的那个模型参数。</p>
<p>上面的过程看上去好像一气呵成，没什么问题。但实际上，<strong>我们在调整参数，反复用测试集测试精度时，测试集已经被“污染”了。换句话说，测试集中的数据在你不断调整参数的过程中已经泄露给了模型。这样模型面对的就不再是全新的数据集，我们也无法真正判断出模型的泛化能力到底如何。</strong></p>
<p>那怎么解决这个问题呢？</p>
<p>我们只需要再额外划分出一块数据集，称之为“验证集”。我们在训练集上拟合模型，在验证集上对模型进行调参，最后再测试集上对模型进行泛化能力的评估（此阶段不调参）。</p>
<p>验证集需要我们提供ground truth的矩形框，而测试集不需要，用模型跑测试集，推理结果好不好就由我们人脑自己来判断了。</p>
<p><img src="https://s1.ax1x.com/2020/06/29/NhqTJK.png" alt="NhqTJK.png"></p>
<h2 id="今生-交叉验证"><a href="#今生-交叉验证" class="headerlink" title="今生 - 交叉验证"></a><strong>今生 - 交叉验证</strong></h2><p>似乎交叉验证出现之后，训练模型时就很少让你再划分出验证集了，因为交叉验证会自己从训练集里划分出验证集。</p>
<p>交叉验证:将拿到的所有大训练集数据,再分为子训练集和验证集。以下图为例:</p>
<p><img src="https://s1.ax1x.com/2020/06/29/NhLQlF.png" alt="NhLQlF.png"></p>
<p>将训练集分成5等份,其中一份作为验证集。然后经过5次验证,每次都更换不同的验证集。即得到5组模型的结果,取平均值作为最终结果。又称5折交叉验证（cv=cross validation）。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>验证集</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度下降的各种优化算法</title>
    <url>/2020/06/27/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>梯度下降法，是一种非常有名的优化（optimization）算法，也是至今最常用的优化神经网络的方法。当你自己构造一个神经网络时，代码上一般要指定一种优化算法：<br><img src="https://s1.ax1x.com/2020/06/27/N6NZVA.png" alt="N6NZVA.png" border="0" /><br><a href="https://www.youtube.com/watch?v=cvNtZqphr6A">截图来源</a></p>
<p>以下内容主要来源于medium上的一个<a href="https://medium.com/tag/learning-parameters/latest">《Learning Parameters》系列</a>文章的翻译，加上本人的一些理解。</p>
<span id="more"></span>
<h2 id="《Learning-Parameters-Part-1-Gradient-Descent》"><a href="#《Learning-Parameters-Part-1-Gradient-Descent》" class="headerlink" title="《Learning Parameters, Part 1: Gradient Descent》"></a><a href="https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb"><strong>《Learning Parameters, Part 1: Gradient Descent》</strong></a></h2><h2 id="为什么要用梯度下降法"><a href="#为什么要用梯度下降法" class="headerlink" title="为什么要用梯度下降法"></a><strong>为什么要用梯度下降法</strong></h2><p>假设这里的神经网络只有一个神经元，我们训练这个神经元的目标就是找到weight和bias的最佳组合，以使损失函数 L(w，b) 输出值最小。（更直观的感受请参考3blue1brown的<a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">视频</a>）</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_25_5_image-20210325102503389.png" alt="image-20210325102503389"></p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_27_9_image-20210325102706232.png" alt="image-20210325102706232"></p>
<p>假设我们用（x，y）=（0.5，0.2） 和（2.5，0.9）来训练神经网络，在训练结束时，我们期望找到 w<em> 和 b</em> ，使得 f（0.5）输出0.2和 f（2.5）输出0.9，也就是说我们希望看到一个S型函数，使（0.5，0.2）和（2.5，0.9）位于S型函数上。</p>
<p>我们可以手动找到这样的w <em>和b </em>吗？让我们尝试一个随机的猜测，例如:w = 0.5，b = 0<br><img src=https://miro.medium.com/max/1400/1*NkCdVlIPhoMnVhW-Ntu4Kw.png><br>看来不行，两个已知点都不在我们尝试的S型函数上，我们接着尝试…..<br><img src=https://miro.medium.com/max/1400/1*LPxbwM51k2iXDsM5vrqcpw.gif><br><img src=https://miro.medium.com/max/1400/1*bj0KLAu8bluOe1AxIu2VNA.png><br>最后找到 w=1.78, b=-2.27 的S型函数是比较符合我们的（x，y）的。</p>
<p>由于我们只有2个(x,y)和2个参数（w，b），因此我们可以轻松地绘制不同（w，b）的L （w，b）,并选择L（w，b）最小的那个。但是，一旦拥有更多的数据点和更多的参数，这么摸索的话这将变得很棘手！此外，这里我们仅针对（-6，6 ）的小范围（w，b）绘制了误差表面，而不是（-inf，inf）。</p>
<p>梯度下降来救援！</p>
<p>梯度下降为我们提供了遍历误差表面的理论方法，从而我们可以快速求出最小值，而无需借助蛮力的试验和摸索。</p>
<h2 id="梯度下降推理"><a href="#梯度下降推理" class="headerlink" title="梯度下降推理"></a><strong>梯度下降推理</strong></h2><p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_28_23_image-20210325102821705.png" alt="image-20210325102821705"></p>
<p>但是，我们如何找到最“理想的” θ变化呢？使用哪个Δ θ才是正确的呢？答案来自<a href="http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html">泰勒级数</a>。</p>
<p><img src=https://miro.medium.com/max/1400/1*9B5TFde4a7Yh0Yt35nsBeA.png></p>
<p>这意味着我们想要移动的 u 或者 Δθ 的方向是梯度方向的180°。在损失表面上的给定点处，我们往与损失函数的梯度相反的方向上移动。这就是黄金梯度下降法则！</p>
<h3 id="参数更新法则"><a href="#参数更新法则" class="headerlink" title="参数更新法则"></a><strong>参数更新法则</strong></h3><p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_29_16_image-20210325102915720.png" alt="image-20210325102915720"></p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_29_51_image-20210325102949777.png" alt="image-20210325102949777"></p>
<h3 id="python代码"><a href="#python代码" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/703ecc8949a01f472d17db3359d56985">前往Gist查看</a></p>
<h2 id="可视化插图"><a href="#可视化插图" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h2><p>让我们从误差表面上的某个随机点开始，看看梯度是怎么更新的。<br><img src=https://miro.medium.com/max/984/1*ghMZkEOArRtRVGOFMu4aOQ.gif></p>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a><strong>局限性</strong></h2><p>原始形式的Gradient Descent具有明显的缺点。让我们看一个示例曲线f（x）=x²+ 1，如下所示：<br><img src=https://miro.medium.com/max/638/1*_V2aSf3uNzbWEy_JRYak-A.png></p>
<ul>
<li>当曲线陡峭时，坡度（∇y1/∇x1）大。</li>
<li>曲线平缓时，坡度（∇y2/∇x2）小。</li>
</ul>
<p>回想一下，我们的权重更新与梯度w = w — η∇w成正比。因此，在曲线平缓的区域中，更新较小，而在曲线陡峭的区域中，更新较大。让我们看看从表面上的另一个点开始时会发生什么。<br><img src=https://miro.medium.com/max/908/1*qJ4t5n_9-6AnpOufeYBmMA.gif></p>
<p>无论我们从哪里开始，一旦碰到具有平缓坡度的表面，进度就会变慢。对于本来就很耗时的神经网络训练来说，这一点简直是雪上加霜，忍无可忍。</p>
<h2 id="损失函数3D可视化"><a href="#损失函数3D可视化" class="headerlink" title="损失函数3D可视化"></a><strong>损失函数3D可视化</strong></h2><p>大型网络通常具有数百万个参数，如何在3D模式下可视化“大型”网络的成本函数呢？<br>下图是一个例子<br><img src=https://miro.medium.com/max/1400/1*YOkeatQyMDHEK9KLV4Rmgw.png><br>更多大型网络的损失函数3D可视化请浏览<a href="https://www.cs.umd.edu/~tomg/projects/landscapes/"><strong>这里</strong></a>的前4个链接！</p>
<h2 id="《Learning-Parameters-Part-2-Momentum-Based-amp-Nesterov-Accelerated-Gradient-Descent》"><a href="#《Learning-Parameters-Part-2-Momentum-Based-amp-Nesterov-Accelerated-Gradient-Descent》" class="headerlink" title="《Learning Parameters, Part 2: Momentum-Based &amp; Nesterov Accelerated Gradient Descent》"></a><a href="https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12"><strong>《Learning Parameters, Part 2: Momentum-Based &amp; Nesterov Accelerated Gradient Descent》</strong></a></h2><p>在第1部分中，我们知道了在误差表面的平缓区域中，坡度很小，这可能会使速度变慢。本部分让我们看一下动量（momentum）如何克服这一缺陷。</p>
<h2 id="基于动量的梯度下降-Momentum-Based-Gradient-Descent"><a href="#基于动量的梯度下降-Momentum-Based-Gradient-Descent" class="headerlink" title="基于动量的梯度下降/Momentum-Based Gradient Descent"></a><strong>基于动量的梯度下降/Momentum-Based Gradient Descent</strong></h2><p>还是用下山的那个例子，MBGD背后的原理就是：</p>
<blockquote>
<p>如果下山者一再被要求朝同一个方向前进，那么他应该获得一些信心，并朝那个方向迈出更大的一步。就像球在滚下斜坡时获得动力一样。</p>
</blockquote>
<h3 id="基于动量的参数更新法则"><a href="#基于动量的参数更新法则" class="headerlink" title="基于动量的参数更新法则"></a><strong>基于动量的参数更新法则</strong></h3><p>我们将动量概念应用在梯度更新中，如下所示：</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_30_37_image-20210325103035848.png" alt="image-20210325103035848"></p>
<p>除了要考虑当前的更新，我们还查看历史的更新。我们从零开始演示一下怎么应用这个法则：</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_31_2_image-20210325103100657.png" alt="image-20210325103100657" style="zoom:80%;" /></p>
<p>您可以看到，当前更新不仅与当前的梯度成比例，而且与先前步骤的梯度成比例。以$update_3$为例，它跟$\nabla\omega_3$成 $\eta$ 比例，跟之前的梯度$\nabla\omega_2$成 $\gamma\cdot\eta$ 比例，跟$\nabla\omega_1$成 $\gamma^2\cdot\eta$ 比例，$\eta$代表学习率，$\gamma$代表动量率，这就是我们在平缓区域提高更新幅度的方式，即将之前的梯度积累起来，加快当前梯度的更新幅度。</p>
<h3 id="python代码-1"><a href="#python代码-1" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/686a9bc9ed34455c6fe618adb912e439">前往Gist查看</a></p>
<h2 id="可视化插图-1"><a href="#可视化插图-1" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h2><p>让我们看看GD和MBGD的起始点放在一个缓坡上的效果如何。<br><img src="https://miro.medium.com/max/988/1*sbUtb2ySE2DUSBYHhQJRaw.gif"></p>
<p>从动图的一开始我们看到，由于处在缓坡区（等高线图上同一种颜色的区域），批量梯度下降100次迭代后产生了一条黑色线条，而且还没走出缓坡区。而基于动量的梯度下降经过一开始的“动量”积累，3秒之后突然快速迭代了起来。</p>
<p>但是，快速的移动始终是件好事吗？会不会哪一次就突然越过目标了？让我们通过更改输入数据，以此得到一个不同的误差表面，然后再来测试一遍MBGD。</p>
<p><img src="https://miro.medium.com/max/856/1*sVpzlIuRspAhsS7Pnv0pWA.png"></p>
<p>如上图所示，在最低谷的两侧，误差都很高。在这种情况下，动量是否仍然可以发挥良好作用，还是会有害呢？让我们找出答案。</p>
<p><img src="https://miro.medium.com/max/992/1*t-kykynrtQ0olmFeNgIB0w.gif"></p>
<p>我们可以观察到，基于动量的梯度下降在峡谷中左右振荡。这使它在最终收敛之前经历了许多U型走位。尽管有这些U型走位，它的收敛速度仍然比批量梯度下降快。经过100次迭代后，基于动量的方法达到了0.00001的误差，而批量梯度下降的误差仍然为0.36。</p>
<p>我们可以做些减少振动/U型走位的事情吗？没问题，Nesterov加速梯度下降可以帮助我们做到这一点。</p>
<h2 id="Nesterov加速梯度下降-Nesterov-Accelerated-Gradient-Descent"><a href="#Nesterov加速梯度下降-Nesterov-Accelerated-Gradient-Descent" class="headerlink" title="Nesterov加速梯度下降/Nesterov Accelerated Gradient Descent"></a><strong>Nesterov加速梯度下降/Nesterov Accelerated Gradient Descent</strong></h2><p>NAGD背后的直觉可以用一个短语来表达：<br>跳跃前先展望！</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_32_18_image-20210325103212032.png" alt="image-20210325103212032"></p>
<h3 id="NAGD更新法则"><a href="#NAGD更新法则" class="headerlink" title="NAGD更新法则"></a><strong>NAGD更新法则</strong></h3><p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_32_39_image-20210325103238241.png" alt="image-20210325103238241"></p>
<p>但是，为什么展望前方能帮助我们避免冲过头呢？您可以先暂停思考一下，再看下面这张图。</p>
<p><img src="https://s1.ax1x.com/2020/06/27/NcmHHO.png" alt="NcmHHO.png" border="0" /></p>
<p>在图（a）中，因为 $\omega_0$ 增加而 L 减小，说明第1次更新的梯度$update_t$的值为负，但经过 $\omega_{t+1}=\omega_t - update_t$ 后，$\omega_{t+1}$是越来越正/大的。 第2次更新也为正，并且由于动量的关系，可以看到它的更新量略大于更新1。因为动量和持续正的更新历史，更新3就会比更新1和2大。第4次更新让事情变得有趣起来。在原始的动量情况下，由于几次历史更新都为正，因此第4次更新就过头了，最后损失下降通过进行负更新来恢复。</p>
<p>但是在NAG的情况下，见图（b），NAG的前3个更新与基于动量的方法非常相似但不完全相同，真正的区别在第4次更新。NAG的每次更新都分两个步骤进行：首先是前半部分更新4a，我们到达 $\omega_{look ahead}$点，然后是后半部分更新4b，到达$\omega_{t+1}$。NAG仍会导致跳跃过头，但与基于动量的梯度下降相比，它会较小。</p>
<p>从公式上看，MBGD和NAGD最核心的区别在于$update_t$的等式，对于MBGD，它的最后一项是$\eta$✖$\nabla\omega_t$,而对于NAGD，它的最后一项是是$\eta$✖$\nabla\omega_{look ahead}$。<strong>在上图中，第4次更新的$\nabla\omega_t$是负值，$\nabla\omega_{look ahead}$是正值</strong>。这使得MBGD的$update_t$是一个比较小的负值（比如-2）.而NAGD的$update_t$是一个比较大的负值（比如-1），最后体现到$\omega_{t+1}$上就是，MBGD前进的步伐比较大，NAGD前进的步伐比较小，但（这里）都是往正的方向。</p>
<p>对于第1、2、3次更新，NAGD前进的步伐也比MBGD前进的步伐小一点，因为$\omega_2$ 的斜率的负值比$\omega_1$处的小。</p>
<h3 id="python代码-2"><a href="#python代码-2" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/baf51844c288f28ac0d6a34e9edf1c4a">前往Gist查看</a></p>
<h2 id="可视化插图-2"><a href="#可视化插图-2" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h2><p>还是用上面那个误差表面，我们将MBGD与NAGD的收敛性进行比较。</p>
<p><img src="https://miro.medium.com/max/992/1*_Q1plMUkXfLPTRCCTRg36g.gif"></p>
<p>您会看到NAG（蓝色）的U型走位比动量（红色）的更小，。</p>
<h2 id="《Learning-Parameters-Part-3-Stochastic-amp-Mini-Batch-Gradient-Descent》"><a href="#《Learning-Parameters-Part-3-Stochastic-amp-Mini-Batch-Gradient-Descent》" class="headerlink" title="《Learning Parameters, Part 3: Stochastic &amp; Mini-Batch Gradient Descent》"></a><strong><a href="https://towardsdatascience.com/learning-parameters-part-3-ee8558f65dd7">《Learning Parameters, Part 3: Stochastic &amp; Mini-Batch Gradient Descent》</a></strong></h2><h2 id="批量梯度下降法（Batch-Gradient-Descent）"><a href="#批量梯度下降法（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降法（Batch Gradient Descent）"></a><strong>批量梯度下降法（Batch Gradient Descent）</strong></h2><p>也有人叫Vanilla 梯度下降法（注：Vanilla 是早期机器学习算法相关的名词，也是如今一个机器学习 python 库的名字，在该处指的是后者，参见：<a href="https://github.com/vinhkhuc/VanillaML）。">https://github.com/vinhkhuc/VanillaML）。</a></p>
<h3 id="python代码-3"><a href="#python代码-3" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/ea90bccb1d69160e3df3ba3e417228cb">前往Gist查看</a></p>
<h2 id="随机梯度下降法（Stochastic-Gradient-Descent）"><a href="#随机梯度下降法（Stochastic-Gradient-Descent）" class="headerlink" title="随机梯度下降法（Stochastic Gradient Descent）"></a><strong>随机梯度下降法（Stochastic Gradient Descent）</strong></h2><p>在处理大量的数据时，批量梯度下降法就有些力不从心了。批量梯度下降法求参数时，会使用全部的训练数据，这样计算的代价很大，计算量大，耗时很长。而随机梯度法是每次仅仅随机均匀采样的一个样本来求梯度。</p>
<h3 id="python代码-4"><a href="#python代码-4" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/de0495b4f2cdc9f935d88a9422861091">前往Gist查看</a></p>
<h3 id="可视化插图-3"><a href="#可视化插图-3" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h3><p><img src=https://miro.medium.com/max/1020/1*EM62UF9uHIxgNmJQETYgZw.gif></p>
<p>如果仔细观察，您会发现我们的下降最后会产生许多微小的振荡。为什么？因为我们正在做出贪婪的决定。每个点都试图将参数推向最有利的方向（不知道这会如何影响其他点）。在局部上有利于一个点的参数更新可能会损害其他点（几乎就像数据点相互竞争一样）。</p>
<h2 id="小批量梯度下降法（Mini-batch-Gradient-Descent）"><a href="#小批量梯度下降法（Mini-batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降法（Mini-batch Gradient Descent）"></a><strong>小批量梯度下降法（Mini-batch Gradient Descent）</strong></h2><p>批量梯度下降法和随机梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用个n样本来迭代，1&lt;n&lt;m。</p>
<h3 id="python代码-5"><a href="#python代码-5" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/4f60cc010d9fd6eed3230dc9c67c93a8">前往Gist查看</a></p>
<h3 id="可视化插图-4"><a href="#可视化插图-4" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h3><p>让我们看看当mini_batch_size=2时的收敛效果如何</p>
<p><img src=https://miro.medium.com/max/940/1*jGzsLBN07fS4ns4kFZmrvQ.gif></p>
<p>可以看出，当批量大小为2时，振荡（红线）略有减少。批次大小的典型值为16、32、64。当然，只要我们使用“近似梯度”而不是“真实梯度” ，振荡就会一直存在。</p>
<h2 id="SGD与MBGD与NAG"><a href="#SGD与MBGD与NAG" class="headerlink" title="SGD与MBGD与NAG"></a><strong>SGD与MBGD与NAG</strong></h2><p><img src=https://miro.medium.com/max/1400/1*30tXOexQixKbDPkRjJicNA.png><br>左边是单纯SGD的结果，右边是SGD-MBGD与SGD-NAGD的结果。</p>
<p>虽然动量（红色）和NAG（蓝色）的随机形式都显示出震荡，但相对于动量，NAG仍然具有小的震荡。此外，它们都比随机梯度下降快，在60步之后，随机梯度下降[黑色-左图]仍然在很高的误差的位置，而NAG和动量接近收敛。</p>
<h2 id="《Learning-Parameters-Part-4-Tips-For-Adjusting-Learning-Rate-Line-Search》"><a href="#《Learning-Parameters-Part-4-Tips-For-Adjusting-Learning-Rate-Line-Search》" class="headerlink" title="《Learning Parameters, Part 4: Tips For Adjusting Learning Rate, Line Search》"></a><strong><a href="https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b">《Learning Parameters, Part 4: Tips For Adjusting Learning Rate, Line Search》</a></strong></h2><p>前面几部分的内容（包括动量法）它们的学习率$\eta$都是始终没有变的，但是想想，为什么我们不把这个学习率改成可可变动的一个系数呢，比如在缓坡区，梯度不变的前提下，增大学习率不也可以增大步长吗</p>
<p>……<br>下面的内容我觉得不太重要，所以不翻译了，好奇的朋友可以去原文看看<br>……</p>
<h2 id="《Learning-Parameters-Part-5-AdaGrad-RMSProp-and-Adam》"><a href="#《Learning-Parameters-Part-5-AdaGrad-RMSProp-and-Adam》" class="headerlink" title="《Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam》"></a><strong><a href="https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d">《Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam》</a></strong></h2><h3 id="自适应学习率的动机"><a href="#自适应学习率的动机" class="headerlink" title="自适应学习率的动机"></a><strong>自适应学习率的动机</strong></h3><p>考虑以下具有S型激活函数的简单感知器网络(下图画的不好，1234应该用下标而不是上标)。<br><img src="https://miro.medium.com/max/1214/1*CRj9U6_LBVMFEaceunB0CA.png" width=70%></p>
<p>应该很容易看到给定一个输入（$\vec x$，y），$\vec w$的梯度如下(上图用加粗代表向量，我这里用上方的箭头代表向量)：</p>
<p><img src="https://miro.medium.com/max/1250/1*8t9xxheID3mR741SMGBxwA.png" width=70%></p>
<p>输出函数f（x） 对于某个特定权重的梯度取决于其相应的输入。如果有n个输入，我们可以将所有n个输入的梯度求和即可得出总梯度。这个方法既不是新方法也不是特别的方法。但是，如果某个输入（比如$x_1$）非常稀疏（即很多次$x_1$的输入值都为0），会发生什么？可以假设对于大多数输入，$\nabla w2$ 将为0（请参见上方的公式），因此w2将不会获得足够的更新。</p>
<p>为什么那样的结果会打扰我们呢？因为如果x2既稀疏又重要，我们就必须认真对待对w2 的更新。为了确保即使在某个输入是稀疏的情况下也能进行更新，我们是否可以为每个参数设置不同的学习率，以照顾到 the frequency of the features（不知如何翻译）？当然可以。</p>
<h2 id="AdaGrad-—-Adaptive-Gradient-Algorithm"><a href="#AdaGrad-—-Adaptive-Gradient-Algorithm" class="headerlink" title="AdaGrad — Adaptive Gradient Algorithm"></a><strong>AdaGrad — Adaptive Gradient Algorithm</strong></h2><h3 id="AdaGrad的更新法则"><a href="#AdaGrad的更新法则" class="headerlink" title="AdaGrad的更新法则"></a><strong>AdaGrad的更新法则</strong></h3><p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_34_39_image-20210325103437729.png" alt="image-20210325103437729"></p>
<p>从更新法则可以看出，梯度的历史值$\nabla w_t$积累在了$v_t$中。而 $v_t$ 和 $\epsilon$ 作为分母与η组成了一个自适应的学习率，随着更新次数的增加，这个学习率的分母越来越大，但分子始终不变，所以整体上这个学习率会变小衰减。</p>
<h3 id="python代码-6"><a href="#python代码-6" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/f3b229d1747857235899f9e23616c7a2">前往Gist查看</a></p>
<h2 id="可视化插图-5"><a href="#可视化插图-5" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h2><p><img src="https://miro.medium.com/max/1198/1*g8b6r_qAQC4NiJ2h_UFsuw.png" width=70%></p>
<p>在我们研究AdaGrad之前，请先查看上面的其他3个优化器-批量GD（黑色），动量（红色），NAG（蓝色）。这三个优化器对此数据集做了一些有趣的事情。你能发现吗？暂停。思考。答：最初，所有三种算法都主要沿垂直（b）轴移动，而沿水平（w）轴的移动很小。为什么？因为在我们的数据中，与w对应的特征是稀疏的，因此w很少进行更新。另一方面，b非常密集，并经历了许多更新。这种稀疏性在包含数千个输入特征的大型神经网络中非常普遍，因此我们需要解决它。现在让我们来看一下使用AdaGrad的效果。</p>
<p><img src="https://miro.medium.com/max/896/1*aflJY_KSjorG1YHNiz7Rnw.gif"></p>
<p>瞧！通过使用不同参数不同学习速率的策略，AdaGrad可以确保尽管某个特定输入稀疏，但w可获得较高的学习速率，因此更新量更大。此外，它还确保如果b进行大量更新，则由于分母的增加，其有效学习率会降低。在实践中，如果我们从分母中删除平方根（这值得深思），那么这将不能很好地工作。随着时间的流逝，b的有效学习率将下降到无法进一步更新b的程度。我们可以避免这种情况吗？RMSProp可以！</p>
<h2 id="RMSProp-—-Root-Mean-Square-Propagation"><a href="#RMSProp-—-Root-Mean-Square-Propagation" class="headerlink" title="RMSProp — Root Mean Square Propagation"></a><strong>RMSProp — Root Mean Square Propagation</strong></h2><p>随着分母的增长，AdaGrad会极大地降低学习率。结果，一段时间后，由于学习率下降，一些参数将会接收非常小的更新。为了避免这种情况，为什么不阻止分母快速增长。</p>
<h2 id="RMSProp的更新法则"><a href="#RMSProp的更新法则" class="headerlink" title="RMSProp的更新法则"></a><strong>RMSProp的更新法则</strong></h2><p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_34_4_image-20210325103403433.png" alt="image-20210325103403433" style="zoom:80%;" /></p>
<p>看起来与AdaGrad非常相似，尤其是$w_{t+1}$和$b_{t+1}$的表达式，两者都一样，不一样的地方在于RMSProp的$v^w_t$和$v^b_t$多了个超参数β。</p>
<h3 id="python代码-7"><a href="#python代码-7" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/64213c8f9f65bc362d595c7bec359548">前往Gist查看</a></p>
<h2 id="可视化插图-6"><a href="#可视化插图-6" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h2><p><img src="https://miro.medium.com/max/976/1*rn3JEQkOB3Znob1Y_EHakg.gif"></p>
<p>你看到了什么？RMSProp（粉红色）与AdaGrad有何不同？暂停。思考。答： AdaGrad在接近收敛时陷入困境，由于学习速度下降，它不再能够沿垂直（b）方向移动，而RMSProp通过减少学习率的衰减而克服了这个问题，最后RMSProp的终点更往上。</p>
<h2 id="Adam-—-Adaptive-Moment-Estimation"><a href="#Adam-—-Adaptive-Moment-Estimation" class="headerlink" title="Adam — Adaptive Moment Estimation"></a><strong>Adam — Adaptive Moment Estimation</strong></h2><h3 id="Adam更新法则"><a href="#Adam更新法则" class="headerlink" title="Adam更新法则"></a><strong>Adam更新法则</strong></h3><p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_10_33_27_image-20210325103326424.png" alt="image-20210325103326424" style="zoom:80%;" /><br>(bias类似)</p>
<p>Adam的更新法则与RMSProp非常相似（尤指$v_t$），不同之处在于，我们也考虑了梯度的累积历史 $m _t$,这个思路跟momentum是一样的 。请注意，上述更新规则中的第三行是偏差校正。了解为什么偏差修正是必要的，可以查看Mitesh M Khapra教授的<a href="https://www.youtube.com/watch?v=-0ZMU-gnm2g">讲解</a>。</p>
<h3 id="python代码-8"><a href="#python代码-8" class="headerlink" title="python代码"></a><strong>python代码</strong></h3><p><a href="https://gist.github.com/acl21/3e71544463c9739226a2afe42edef665">前往Gist查看</a></p>
<h2 id="可视化插图-7"><a href="#可视化插图-7" class="headerlink" title="可视化插图"></a><strong>可视化插图</strong></h2><p><img src=https://miro.medium.com/max/976/1*m9JMNb9z2bzFlmPfK0WGUQ.gif></p>
<p>很显然，采用累积的渐变历史记录可以加快渐变速度。对于此数据集，它似乎有点超调(overshooting)，但即使这样，它的收敛速度也比其他优化器快。</p>
<h2 id="最终问题：使用哪种优化算法？"><a href="#最终问题：使用哪种优化算法？" class="headerlink" title="最终问题：使用哪种优化算法？"></a><strong>最终问题：使用哪种优化算法？</strong></h2><ul>
<li>亚当现在似乎或多或少是默认选择（β1 = 0.9，β2 = 0.999和ϵ = 1e-8）。</li>
<li>尽管它对初始学习率具有鲁棒性，但我们已经注意到，对于序列生成问题η = 0.001，0.0001效果最佳。</li>
<li>话虽如此，许多论文都报告说，带有动量的SGD（Nesterov或经典）和简单的退火学习速率计划在实践中也很有效（通常，从η = 0.001开始，对于序列生成问题为0.0001）。</li>
<li>亚当可能只是整体上最好的选择。</li>
<li>最近的一些工作表明亚当存在问题，并且在某些情况下不会收敛。</li>
</ul>
<h2 id="拓展阅读："><a href="#拓展阅读：" class="headerlink" title="拓展阅读："></a><strong>拓展阅读</strong>：</h2><blockquote>
<ul>
<li><a href="http://www.cse.iitm.ac.in/~miteshk/CS7015_2018.html">CS7015深度学习课程</a><br></li>
<li><a href="https://ruder.io/optimizing-gradient-descent/">《An overview of gradient descent optimization algorithms》</a><br></li>
<li><a href="https://www.youtube.com/watch?v=mdKjMPmcWjY"><Optimizers - EXPLAINED!></a></li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>自然常数natural constant e</title>
    <url>/2020/06/26/%E8%87%AA%E7%84%B6%E5%B8%B8%E6%95%B0natural-constant-e/</url>
    <content><![CDATA[<p>自然常数e ≈ 2.71828 18284，也有人叫它欧拉数（Euler number），以瑞士数学家欧拉命名，它的其中一个定义是</p>
<script type="math/tex; mode=display">{ e = \lim_{x \to +\infty}(1+\frac{1}{x})^x}</script><p>或</p>
<script type="math/tex; mode=display">e= \sum_{i=0}^n\frac{1}{i!}=\frac{1}{0!}+\frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+...+\frac{1}{n!} = 1+ \frac{1}{1!}+\frac{1}{2!}+\frac{1}{3!}+...+\frac{1}{n!}</script><blockquote>
<p>关于如何在hexo中插入公式，可参考<a href="http://stevenshi.me/2017/06/26/hexo-insert-formula/">【1】</a>、<a href="https://juejin.im/post/5a6721bd518825733201c4a2">【2】</a></p>
</blockquote>
<p>当自然常数e作为指数和对数的底数时，就成了自然指数$e^x$和自然对数$log_e$=$ln$。</p>
<h2 id="e的意义"><a href="#e的意义" class="headerlink" title="e的意义"></a>e的意义</h2><p>e的意义就是e就是 <strong>增长的极限</strong>。</p>
<p><a href="http://www.zhangjs.me/natural-logarithm/"><strong>利滚利的例子</strong></a></p>
<p>假设我们在银行存了1元人民币（对的就这么少），而这1元的年息为100%（超过100%的话最后的结果就不是e）。</p>
<p>如果存够一年取出，那么可以得到1元的利息，连本金余额共2元；<br>如果半年取一次利息并将利息再次存入，那么各阶段本金加利息余额为(利滚利)：</p>
<p>1) 半年后利息加本金：1 + (1 X 0.5) = 1.5<br>2) 一年后利息加本金：1.5 + (1.5 X 0.5) = 2.25</p>
<p>如果每个季度取一次利息并再次存入，各阶段余额为：</p>
<p>1) 一季度后利息加本金：1 + (1 X 0.25) = 1.25<br>2) 半年后利息加本金：1.25 + (1.25 X 0.25) = 1.5625<br>3) 三季度后利息加本金：1.5625 + (1.5625 X 0.25) = 1.953125<br>4) 一年后利息加本金：1.953125 + (1.953125 X 0.25) = 2.44140625</p>
<p>……</p>
<p>如果每天都结算一次利息并再次存入(分为365次)，一年后余额为2.7145674820219727元</p>
<p>如果每秒都结算一次利息并再次存入(分为31536000次)，一年后余额为2.7182817784689974元</p>
<p>如果把利息结算时间再变得更小，利滚利的存款余额将越来越接近e；利滚利的次数趋于无穷时，存款余额将无限接近e，即e是存款增长的最大值。</p>
<p>用一张图来表示，就是：</p>
<p><img src="https://s1.ax1x.com/2020/06/26/Ns6U3R.md.png" width="70%" height="70%" alt="[银行复利的例子]" title="[银行复利的例子]"></p>
<h2 id="e为什么是自然的"><a href="#e为什么是自然的" class="headerlink" title="e为什么是自然的"></a>e为什么是自然的</h2><p>在物理科学领域中，e用得非常多，一般不使用以10为底数的对数。以e为底数，许多自然界的式子都能得到简化，变得整洁，用它是最“自然”的，所以叫“自然对数”。<br>你或许会疑惑为什么这么一个非整数会是一个“自然”的数呢，你可能会觉得0 或 1 或 10 更自然一点吧，因为计算机用0和1存储数据和计算，10的话则是因为我们目前人类计数用的最多的是10进制。其实我们人类之所以会用十进制，只不过是因为我们人类有10根手指。如果有一个外星人有四根手指呢？他们是不是就使用四进制呢？（原话来自知乎用户“<br>你我的噫鸨”）所以10并不能作为这个宇宙一个自然的数。</p>
<h2 id="e的公式拓展"><a href="#e的公式拓展" class="headerlink" title="e的公式拓展"></a>e的公式拓展</h2><p>对最上面的公式进一步研究，我们可以得到更普适的公式：</p>
<script type="math/tex; mode=display">{ \lim_{x \to +\infty}(1+\frac{a}{x})^{bx}} = e^{ab}</script><p>关于此公式的证明，请见<a href="https://www.youtube.com/watch?v=HM-kwHR4VO4">这个youtube视频</a>。</p>
<p>把这个公式应用在上面利滚利的例子，就是年利率如果大于100%的话，比如200%，那最后的结果是$e^2$。虽然结果不等于e，但是用e来表达，结果还是很整洁。</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title>美联储加息是什么意思</title>
    <url>/2020/06/20/%E7%BE%8E%E8%81%94%E5%82%A8%E5%8A%A0%E6%81%AF%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/</url>
    <content><![CDATA[<blockquote>
<p>本文内容是基于<a href="http://forex.cngold.org/school/c7048380.html">这篇文章</a>(2020-5-29)改编而来。</p>
</blockquote>
<p><img src="https://s1.ax1x.com/2020/06/20/NlqUm9.png" alt="NlqUm9.png"></p>
<p><strong>什么是“加息”</strong><br>加息就是增加基准利率/利息（利率/利息分为贷款利率/利息和存款利率/利息）。</p>
<p>银行想最大化利润，就要尽可能多揽存居民的存款，然后再拿这些钱去多贷款， 然后赚取贷款利息。注意，银行为了赚钱，贷款利息会比存款利息更高一点。如果银行的钱不够贷了怎么办?就要向央妈借，而央妈给银行的贷款利率，叫做基准利率。央妈加息(基准利率提高)了，使得商业银行和其它金融机构对央妈的借贷成本提高，进而迫使银行也提高了居民的贷款利率，所以老百姓向商业银行贷款，成本就增加了。</p>
<p>除了提高基准利率，央妈还会通过卖国债逼银行提高居民存款利息，因为如果国债的利息比现在的银行存款利息高，居民就会倾向把钱用来买国债赚利息，而不是存银行赚利息，这样居民们的存款就被国债吸走了，所以各大银行也不得不提高存款利率。</p>
<p>最终，更少的老百姓去借钱，更多的老百姓把钱存在银行里，社会上流通的钱变少了。加息的目的包括减少货币供应、压抑消费、压抑通货膨胀、鼓励民间存款、减缓或抑制市场投机等等。所以加息被认为是货币“紧缩”政策。</p>
<p><strong>美联储加息是咋回事</strong></p>
<p><img src="https://s1.ax1x.com/2020/06/20/Nlq1YV.png" alt="Nlq1YV.png"></p>
<p>美联储(Federal Reserve)是美国的中央银行。它的货币政策决策机构，联邦公开市场委员会(FOMC)通过举行讨论会议投票决定联邦基金利率(基准利率)，它们每年都要在华盛顿特区召开八次例行会议。开会之后宣布是否调整联邦基金利率(基准利率)以及调整多少，并通过在公开市场买卖政府债券的操作来影响市场资金充裕状况，从而使利率趋向设定的联邦基金目标利率(基准利率)。</p>
<p>而美联储加息，对全球经济的影响的基本原理和逻辑是这样的：美国发国债，印了大量的美元，这些美元平时流向全世界，以股票、房地产等资产的形式存在，如果美联储加息，由于投资回报率提高了，所以部分美元会回流美国，于是他国的股票、房地产等资产就会遭到抛售，换成美元后回流美国。于是，这些国家的股票、房地产等市场就面临下跌。而且会造成本币贬值，出国留学、境外旅游等，就要花费更多的钱。</p>
<p>此外，全世界复苏进度不一样，有些国家经济还处于放缓状态，美联储加息，一方面会造成资本外流的压力，另一方面也会考验这些国家的货币政策，如果不跟随美联储加息，可能造成资本外流，而跟随美联储加息，对经济复苏又将是一个巨大的考验。</p>
<p><strong>美联储加息对中国的影响</strong></p>
<p>资金外流</p>
<p>美联储加息会出现国内资本外流，给中国外汇带来压力。资本都是逐利的，美国银行存钱的利息增加，就会造成资本流回美国银行。这里包括海外投资者投入到中国的资金，也包括为了赚取更高利息而流出的国内投资者的资金。这会导致在中国的各类资产被抛售，直接的利空中国的股市和房市，对实体经济也会造成一定冲击。</p>
<p>人民币贬值</p>
<p>央行加息作为货币紧缩政策，会导致市场上的美元减少，供不应求，美元升值。美元作为国际货币，它的升值直接导致人民币可兑换的美元减少，造成人民币的被动贬值。既然人民币出现贬值，我们所参与的任何国际行为，是不是要花费更多的金钱?换句话说，就是像我们购买海外商品，还有出国旅游、留学等，也都会花费我们更多的人民币。人民币贬值也会对国内的进出口市场有很大的影响。</p>
]]></content>
      <categories>
        <category>财经</category>
      </categories>
  </entry>
  <entry>
    <title>CAN bus &amp; LIN bus</title>
    <url>/2020/06/14/CAN-bus-LIN-bus/</url>
    <content><![CDATA[<blockquote>
<p>本文内容主要来源于以下网站的内容收集:<br><br><a href="https://zh.wikipedia.org/wiki/%E6%8E%A7%E5%88%B6%E5%99%A8%E5%8D%80%E5%9F%9F%E7%B6%B2%E8%B7%AF">《CAN bus 维基百科》</a><br><br><a href="https://www.eet-china.com/mp/a602.html">《一文了解嵌入式工程师常用的CAN总线协议》</a><br><br><a href="https://www.youtube.com/watch?v=AeSZ_3T1GCk">《TI Precision Labs - CAN/LIN/SBC: CAN and CAN FD Overview》</a><br><br><a href="https://www.youtube.com/watch?v=2Mhqwt2xTxk&amp;t=605s">《TI Precision Labs - CAN/LIN/SBC: CAN and CAN FD Protocol》</a><br><br><a href="https://www.youtube.com/watch?v=TngJZVb33zc">《TI Precision Labs - CAN/LIN/SBC: What is LIN?》</a><br><br><a href="https://www.youtube.com/watch?v=hjuO1UjoQU0&amp;t=223s">《Local Interconnect Network (LIN) Overview and Training》</a><br><br><a href="https://www.youtube.com/watch?v=4nk8Lb_K8rs">《TI Precision Labs - CAN/LIN/SBC: CAN Physical Layer》</a><br><br><a href="https://www.renesas.com/sg/en/solutions/automotive/technology/networking-solutions.html">《In-Vehicle Networking Solutions》</a><br><br><a href="https://www.youtube.com/watch?v=KOxdqo4eAZM">《Fun and Easy Ethernet - How the Ethernet Protocol Works》</a><br><br><a href="https://zhuanlan.zhihu.com/p/73475429">《车载以太网系统——帧结构信息》</a><br><br><a href="https://www.vector.com/int/en/know-how/technologies/networks/">《vector-networks》</a><br><br><a href="https://support.ixiacom.com/sites/default/files/resources/whitepaper/ixia-automotive-ethernet-primer-whitepaper_1.pdf">《IXIA-Automotive Ethernet:An Overview》</a><br><br><a href="https://www.csselectronics.com/screen/page/lin-bus-protocol-intro-basics">《LIN Bus Explained - A Simple Intro (2020)》</a></p>
</blockquote>
<h2 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h2><p>作为外行人，我们平时接触感受最多的是下图左边这种基于中心服务器的网络结构，因为互联网差不多就是这种结构，但其实生活中还有很多场景下用到的是右边的网络结构，（当我对技术领域的探索越来越广越来越深入后，我经常遇到这种结构），这种结构的特点是网络中的各个设备要能够互相点对点通信，这就是局域网的一大特点，这里我们谈的CAN就属于这种网络结构，这让我想起这个<a href="https://www.youtube.com/watch?v=1z0ULvg_pW8&amp;t=194s">视频</a>里的hub也是“一台设备发送信息，网络里的其他设备统统都收到信息”。</p>
<p><img src="https://s1.ax1x.com/2020/06/14/tzWQ2D.png" alt="tzWQ2D.png"></p>
<h2 id="CAN的由来"><a href="#CAN的由来" class="headerlink" title="CAN的由来"></a><strong>CAN的由来</strong></h2><p>随着汽车的功能越来越丰富，上面的ECU也越来越多（现在大约有70个），于是工程师们面临一个问题：我们要设计一个既能满足应有的ECU间通信功能外，还要求这些通信电缆要够短够简单，这能为汽车制造商节约大量的线束（harness）成本和工时成本，还能为降低车重节约能耗有所作用，在这样的背景下，上世纪80年代罗伯特·博世公司发明了CAN。现在汽车制造商为了进一步节约物料成本，还选择在网络的末端用LIN总线。<br><img src="https://s1.ax1x.com/2020/06/14/tzqbG9.png" alt="tzqbG9.png"><br><span id="more"></span><br>控制器局域网 (Controller Area Network，简称CAN或者CAN bus) 是一种功能丰富的车用总线标准。被设计用于在不需要主机（Host）的情况下，允许网络上的电子设备相互通信。 它基于<a href="https://zh.wikipedia.org/wiki/%E8%A8%8A%E6%81%AF%E5%82%B3%E9%81%9E_(%E8%BB%9F%E9%AB%94">消息传递协议</a>)，设计之初在车辆上采用<a href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8">复用通信线缆</a>，以降低线束使用量，后来也被其他行业所使用。<br>CAN使用信息标志符（Message Identifier，每个标志符在整个网络中独一无二）来定义消息内容和消息传递的优先顺序，也就是说每个节点都能发出信息，且网络上其他节点都能收到，但如果遇到同时传输，会有一个优先级，这种方式区别于指定节点地址（Station Address）发送信息的方式，在这个<a href="https://www.youtube.com/watch?v=1z0ULvg_pW8&amp;t=194s">视频</a>里的switch（交换机）就是采用信息只传递给指定节点地址的方法。<br><img src="https://s1.ax1x.com/2020/06/14/tzfFFP.png" alt="tzfFFP.png"><br>正因为CAN使用了全网广播的信息发送机制，这使得它拥有了良好的弹性调整能力，即可以在现有网络中增加、删除节点而不用对原有电子设备的软、硬件做出调整。除此之外，消息的传递不基于特殊种类的节点，增加了升级网络的便利性。</p>
<h2 id="CAN-架构"><a href="#CAN-架构" class="headerlink" title="CAN 架构"></a><strong>CAN 架构</strong></h2><p>CAN总线整体来看就是下图这么简单，两根线两个电阻还有若干个电子设备/节点。这两根线是CAN_H和CAN_L，各个节点通过这两条线实现信号的串行差分传输，为了避免信号的反射和干扰，还需要在CAN_H和CAN_L之间接上120欧姆的终端电阻，但是为什么是120欧姆呢?那是因为电缆的特性阻抗为120欧。终端电阻可以用来抑制信号反射，同时可以使总线电压回到隐性状态或者闲置状态。</p>
<p><img src="https://s1.ax1x.com/2020/06/14/NSCmX4.png" alt="NSCmX4.png"></p>
<p>终端电阻分两种类型：<br><img src="https://s1.ax1x.com/2020/06/14/NS9WY6.png" alt="NS9WY6.png"></p>
<p>每个节点具体的组成部分如下：</p>
<p><img src="https://s1.ax1x.com/2020/06/14/NSFoCt.png" alt="NSFoCt.png"></p>
<p>每个节点/ECU都包含一个CAN收发（Transceiver）芯片，CAN收发器的作用是负责逻辑电平和信号电平之间的转换，即从CAN控制芯片（上图的Micrpcontroller）输出逻辑电平（一堆0和1）到CAN收发器，然后经过CAN收发器内部电路将逻辑电平转换为差分信号输出到CAN总线上。<br><img src="https://s1.ax1x.com/2020/06/14/NSCJ1O.png" alt="NSCJ1O.png"></p>
<p>关于这个收发芯片，市场上不同的产品有不同的引脚，下图是德州仪器的两款收发芯片：<br><img src="https://s1.ax1x.com/2020/06/14/NSCs9f.png" alt="NSCs9f.png"><br><img src="https://s1.ax1x.com/2020/06/14/NSCT3T.png" alt="NSCT3T.png"><br>下图是一个比较完整的CAN网络结构<br><a href="https://imgchr.com/i/NS9ry4"><img src="https://s1.ax1x.com/2020/06/14/NS9ry4.png" alt="NS9ry4.png"></a></p>
<p><img src="https://s1.ax1x.com/2020/06/14/NS9vp8.png" alt="NS9vp8.png"><br><img src="https://s1.ax1x.com/2020/06/14/NSC0AI.png" alt="NSC0AI.png"></p>
<p><img src="https://s1.ax1x.com/2020/06/14/NSCOb9.png" alt="NSCOb9.png"></p>
<h2 id="CAN仲裁原理"><a href="#CAN仲裁原理" class="headerlink" title="CAN仲裁原理"></a><strong>CAN仲裁原理</strong></h2><table><tr>
<td><img src=https://s1.ax1x.com/2020/07/05/USpZpd.png width=90% ></td>
<td><img src=https://s1.ax1x.com/2020/07/05/USpNXq.png ></td>
</tr></table>

<p>CANH=2.5V, CANL=2.5V, diff=OV</p>
<p>CANH=3.5V, CANL=1.5V, diff= 2V</p>
<p>在CAN总线上，如果有多个节点同时要广播信息怎么办，因为CAN总线某个时刻只能传输一条信息，遇到这种冲突就得靠CAN的仲裁（arbitration）制度了。</p>
<p>具有较低值的标识符（Message identifier）具有较高的优先级。下图中，节点 A 的标识符的值为1,199，而节点 B 的标识符的值为 1,530。这意味着节点 A 在这两条消息之间具有更高的优先级，它应赢得仲裁。</p>
<p>收发器必须知道循环时间和传播延迟，以确保适当的消息仲裁。</p>
<p><img src="https://s1.ax1x.com/2020/07/04/Nz2jRs.png" alt="Nz2jRs.png"></p>
<p>结合下图，让我们看看CAN 或 CAN FD总线上仲裁的简化示例。想象我们有三个节点访问同一条CAN 总线，并且每个节点同时开始发送一条消息，消息标识符位于节点名称的下面。</p>
<p>在第一个位期间，这三个节点都传输一个隐性（recessive）信号，因为它们的消息标识符的第一位数字都是“1”。接下来，这三个节点又传输一个表示各自标识符第二位数字的显性位“0”。到目前为止，由于三个节点都传输了相同的信号，因此还没有仲裁。</p>
<p>不过，在第三个位期间，节点 B传输一个隐性信号“1”。但是，由于网络上至少有一个器件（A和C）正在传输显性信号“0”，此时，节点A和C拥有更高的优先级，因此节点 B 输掉仲裁。</p>
<p>背后的技术细节是，B节点的CAN 收发器在驱动时监视总线，并将其状态与其正在驱动的状态进行比较。如果收发器传输逻辑“1”，但读取逻辑“0”，这里的节点 B 正是这种情况，则会立即停止传输。目前为止，节点 A 和节点 C尚未经历过这种情况。因此，这两个节点继续各自的传输，直到其中一个节点在传输隐性信号时接收到显性信号。</p>
<p>在该示例中，节点 C在第七个位经历该情况。此时，节点 C输掉仲裁。它也立即停止传输。</p>
<p>作为唯一继续传输的器件，节点 A 将完成其 11位消息标识符传输。因为它已经以最低的二进制标识符值赢得了仲裁，所以它可以继续传输其消息的剩余部分，。</p>
<p>节点 B 和 C 将在下一个帧间间隔之后重新尝试发送其优先级较低的消息，该间隔将在节点<br>A 传输结束时发生。</p>
<p><img src="https://s1.ax1x.com/2020/07/04/NzgoEF.png" alt="NzgoEF.png"></p>
<h2 id="CAN数据格式"><a href="#CAN数据格式" class="headerlink" title="CAN数据格式"></a><strong>CAN数据格式</strong></h2><blockquote>
<p>本节以下截图来源于《TI Precision Labs - CAN/LIN/SBC: CAN and CAN FD Protocol》</p>
</blockquote>
<p>整体上看，一个CAN数据帧的格式如下图所示<br><img src="https://s1.ax1x.com/2020/07/04/NzcyS1.png" alt="NzcyS1.png"><br>下面让我们来从头到尾看一个样例 CAN 帧各个部分的定义,一种颜色代表一个部分<br><a href="https://imgchr.com/i/NzTIQe"><img src="https://s1.ax1x.com/2020/07/04/NzTIQe.png" alt="NzTIQe.png"></a><br><img src="https://s1.ax1x.com/2020/07/04/Nz7Bkt.png" alt="Nz7Bkt.png"><br><img src="https://s1.ax1x.com/2020/07/04/Nz7Rmj.png" alt="Nz7Rmj.png"><br><img src="https://s1.ax1x.com/2020/07/04/Nz7OB9.png" alt="Nz7OB9.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzHZNt.png" alt="NzHZNt.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzHKgS.png" alt="NzHKgS.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzHBDJ.png" alt="NzHBDJ.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzbcZj.png" alt="NzbcZj.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzqCeH.png" alt="NzqCeH.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzqJpV.png" alt="NzqJpV.png"></p>
<p><img src="https://s1.ax1x.com/2020/07/04/NzOiGt.png" alt="NzOiGt.png"><br><img src="https://s1.ax1x.com/2020/07/04/NzOuIs.png" alt="NzOuIs.png"></p>
<p><img src="https://s1.ax1x.com/2020/07/05/NzzAHO.png" alt="NzzAHO.png"></p>
<p><img src="https://s1.ax1x.com/2020/07/05/Nzzdvq.png" alt="Nzzdvq.png"></p>
<h2 id="CAN数据传输速度"><a href="#CAN数据传输速度" class="headerlink" title="CAN数据传输速度"></a><strong>CAN数据传输速度</strong></h2><p>长度小于40m的网络最高支持的比特率高达1兆比特/秒。降低比特率可以允许使用更长的网络距离（例如，125千比特/秒支持最大500米）。改进的CAN FD标准允许仲裁后升高比特率，可以将数据区块速度增加至仲裁位速率的八倍。<br><a href="https://imgchr.com/i/NS9Lkt"><img src="https://s1.ax1x.com/2020/06/14/NS9Lkt.png" alt="NS9Lkt.png"></a></p>
<h2 id="LIN-的由来"><a href="#LIN-的由来" class="headerlink" title="LIN 的由来"></a><strong>LIN 的由来</strong></h2><p>CAN的成本太高（线束成本 + CAN许可证费 + 节点芯片成本），无法在车上的每一个设备中都装设。在1990年代末期，由BMW、大众集团、奥迪、富豪汽车及梅赛德斯-奔驰这五家车厂开始了LIN Consortium（联盟），也有从Volcano汽车集团及摩托罗拉而来的软件及硬件协助。第一个完全实现的新LIN协定（LIN version 1.3）是在2002年11月发布。</p>
<p>LIN总线的核心相对简单：<br>一个主节点循环遍历每个从节点，发送信息请求。每个从节点在被轮询时都会响应数据。</p>
<p><img src=https://canlogger1000.csselectronics.com/img/LIN-Bus-Data-Flow-Transmission-Header-Response.svg width=60%></p>
<p>但是，随着每个规范的更新，LIN规范中都添加了新功能，使其更加复杂。</p>
<p>简单来说，LIN总线消息帧由<strong>标头</strong>和<strong>响应</strong>组成。</p>
<p>通常，LIN主设备将标头发送到LIN总线。这将触发一个从机，该从机发送2/4/8个数据字节作为响应。</p>
<p>整个LIN框架如下图所示：</p>
<p><img src=https://canlogger1000.csselectronics.com/img/LIN-Bus-Frame-Message-Header-Response-Master-Slave.svg width=100%></p>
<p><font color=#ff9933 size=3 >间隔</font>：同步间隔字段（SBF）也称为间隔，其长度至少为13 + 1位（实际上，通常为18 + 2位）。中断字段充当总线上所有LIN节点的“帧开始”通知。</p>
<p><font color=#ff9933 size=3 >同步</font>： 8位同步字段的预定义值为0x55（二进制格式01010101）。这种结构允许LIN节点确定上升沿/下降沿之间的时间，从而确定主节点使用的波特率。这使它们每个保持同步。</p>
<p><font color=#ff9933 size=3 >标识符</font>：标识符为6位，后跟2个奇偶校验位。该ID充当每个发送的LIN消息的标识符，以及哪些节点对标头做出反应。从站确定ID字段的有效性（基于奇偶校验位），并通过以下操作：</p>
<ol>
<li>忽略后续数据传输</li>
<li>侦听从另一个节点传输的数据</li>
<li>发布数据以响应标题</li>
</ol>
<p>通常，一次轮询一个从站以获取信息，这意味着零碰撞风险（因此无需仲裁）。<br>请注意，这6位允许使用64个ID，其中ID 60-61用于诊断（下面更多内容），而62-63则保留。</p>
<p><font color=#3399ff size=3 >数据</font>：当LIN从站被主站轮询时，它可以通过传输2、4或8字节的数据进行响应。从LIN 2.0开始，数据长度取决于ID范围（ID 0-31：2个字节，32-47：4个字节，48-63：8个字节）。</p>
<p><font color=#3399ff size=3 >校验和</font>：与CAN中一样，校验和字段可确保LIN帧的有效性。经典的 8位校验是基于所述数据字节只（LIN 1.3）求和，而增强的校验和算法也包括标识符字段（LIN 2.0）。</p>
<p><strong>六种LIN框架类型</strong></p>
<p>尽管存在多种类型的LIN帧，实际上绝大多数通信都是通过“无条件帧”完成的。</p>
<p>还要注意，下面的每一种都遵循相同的基本LIN帧结构。只是在时序或数据字节内容上有所不同。</p>
<p>下面我们简要概述每种LIN框架类型：</p>
<p><img src="https://s1.ax1x.com/2020/08/10/a7TLdO.png" alt="a7TLdO.png"></p>
<p><img src=https://canlogger1000.csselectronics.com/img/intel/lin-bus/LIN-Frame-Type-Identifier-Unconditional-Diagnostic-Event-Triggered.png width=50%></p>
<h2 id="LIN的网络拓扑"><a href="#LIN的网络拓扑" class="headerlink" title="LIN的网络拓扑"></a><strong>LIN的网络拓扑</strong></h2><p>LIN是广播串行网络，其中包括主站(master)和从站（slave），而一台主站可以控制多达16台从站。因为所有的通讯都是由主站开始，且是时间确定性（time deterministic），不需要有碰撞侦测的方案。在长度40米时，可以到19.2 kbit/s。</p>
<p>主站和从站一般都是用单片机实现，不过为了节省成本、空间或是电源，也可以用特殊的硬件或是特殊应用集成电路来实现。</p>
<p><img src="https://s1.ax1x.com/2020/07/05/US5vge.png" alt="US5vge.png"><br><img src="https://s1.ax1x.com/2020/07/05/USIiUP.png" alt="USIiUP.png"><br><img src="https://s1.ax1x.com/2020/07/05/USIl5V.png" alt="USIl5V.png"><br><img src="https://s1.ax1x.com/2020/07/05/USoWy4.png" alt="USoWy4.png"><br><img src="https://s1.ax1x.com/2020/07/05/USINr9.png" alt="USINr9.png"><br><img src="https://s1.ax1x.com/2020/08/10/a7532q.png" alt="a7532q.png"></p>
<h2 id="CAN-和-LIN-的对比"><a href="#CAN-和-LIN-的对比" class="headerlink" title="CAN 和 LIN 的对比"></a><strong>CAN 和 LIN 的对比</strong></h2><p>在汽车电气系统里，CAN与LIN相辅相成<br><img src="https://s1.ax1x.com/2020/07/05/USgkVO.png" alt="USgkVO.png"><br>除了上图右边列出的差异外，CAN和LIN的差异还包括：<br>CAN的工作电压为5V,LIN的工作电压为12V；<br>CAN的identifier位数是11或29，LIN的identifier位数是6；</p>
<h2 id="其他汽车通信协议"><a href="#其他汽车通信协议" class="headerlink" title="其他汽车通信协议"></a><strong>其他汽车通信协议</strong></h2><p><img src=https://www2.renesas.cn/cn/zh/img/solutions/automotive/technology/blockdiagram-networking.jpg></p>
<p>CAN是目前使用最广泛的车载网络，比如它会使用在Airbag,ABS,millimeter-wave radar，dashboard，seat belt control。然而，随着自动驾驶汽车和其他新技术的不断出现和发展，汽车网络对更大的带宽和连接性的需求越来越高，所以后来又诞生了像无线CAN，MOST，FlexRay和汽车以太网等这些车载网络协议。</p>
<p><strong>MOST</strong>（Media Oriented Systems Transport）是一种串行通信系统，用于通过光纤电缆传输控制数据，视频和音频，它以24.8 Mbps的速率提供了点对点的声音和视频信息交换。由MOST协会创建的MOST，它定义了必要的协议，软件和硬件层，以允许使用单个介质/物理层高效，低成本地传输控制，实时和分组数据。可以以最多包括64个MOST设备的环的形式示意性地表示MOST网络。由于其即插即用功能，添加或删除MOST设备应该非常简单。</p>
<p><strong>FlexRay</strong>的传输速率可以达到10Mbps。CAN一帧最多只能传输8字节的数据，CAN FD也只能64字节，而Flexray却能一帧传输254字节。它使用铜或光纤作为物理层的线束。具有双通道配置的FlexRay具有增强的容错能力和/或增加的带宽。FlexRay通信网络的功能使其非常适合下一代汽车行业。</p>
<p>大多数第一代FlexRay网络通常采用单个通道来控制布线成本，但进一步的应用开发和安全要求将导致增加两个通道的使用。<br><img src=https://talks.navixy.com/wp-content/uploads/2020/06/flexray_wiring1.jpg></p>
<p>FlexRay的应用场景有<a href="https://baike.baidu.com/item/x-by-wire/1165018?fr=aladdin">X-by-wire</a>、ADAS。<br><img src=https://www.renesas.com/us/en/img/solutions/automotive/technology/brake-by-wire-system.png></p>
<p><strong>车载以太网Ethernet</strong></p>
<p>以太网1973年被发明出来后大部分场合是应用于家庭，办公室和工厂中，是使用非常广泛的LAN技术。现在，车辆通常可以容纳多个摄像头，车载诊断程序，高级驾驶员辅助系统（ADAS），信息娱乐系统和仪表板显示屏。上面的许多要求超出了已有的CAN总线的技术能力，而以太网就是为了满足这些需求应运而生。与我们所知的标准以太网相比，汽车以太网使用不同的硬件层来满足车辆中恶劣环境和电气条件的要求。</p>
<p>我们笔记本电脑有个RJ45接口，我们不用wifi上网时就会插这根带水晶头的网线，所以以太网就在我们的身边，这个水晶头有8个触点，但其实我们的以太网只需要用来4根，或者说两对，正因为它是两对双绞线，一对的数据传输方向往“左”，所以以太网是全双工的。<br><img src="https://s1.ax1x.com/2020/08/09/a7U80S.png" alt="a7U80S.png"><br><img src="https://i.loli.net/2020/08/09/I8erC1pqEbc6sPA.png" alt="image.png"></p>
<p>以太网的数据帧格式详见这篇文章：<a href="https://zhuanlan.zhihu.com/p/73475429">《车载以太网系统——帧结构信息》</a>。</p>
<p>以太网的标准拓扑结构为总线型拓扑，但目前的快速以太网为了减少冲突，最大化提高网络速度和使用效率，便使用了交换机（Switch hub）来进行网络连接和组织。如此一来，以太网的拓扑结构就成了星型；但在逻辑上，以太网仍然使用总线型拓扑和CSMA/CD（Carrier Sense Multiple Access/Collision Detection，即载波多重访问/碰撞侦测）的总线技术。</p>
<p>以太网的传输速度会随着节点数的增多而降低，但仍然比CAN总线快。</p>
<p><strong>部分以太网类型</strong><br><img src="https://i.loli.net/2020/08/09/hUWtzVsvyfJw9GY.png" alt="image.png"></p>
<p><strong>车载以太网的发展历程</strong></p>
<p><img src="https://s1.ax1x.com/2020/08/09/a7g5uQ.png" alt="a7g5uQ.png"></p>
<p><img src=https://www.renesas.com/us/en/img/solutions/automotive/technology/ethernet-diagnostic.jpg></p>
<p><img src=https://picb.zhimg.com/v2-f1f167c6a996c1d44446047607a022df_r.jpg></p>
<p><img src="https://s1.ax1x.com/2020/08/10/a75OoQ.png" alt="a75OoQ.png"></p>
]]></content>
      <categories>
        <category>汽车电子</category>
      </categories>
      <tags>
        <tag>汽车</tag>
        <tag>通信</tag>
      </tags>
  </entry>
  <entry>
    <title>APK的反编译与加固</title>
    <url>/2020/06/09/apk%E7%9A%84%E5%8F%8D%E7%BC%96%E8%AF%91%E4%B8%8E%E5%8A%A0%E5%9B%BA/</url>
    <content><![CDATA[<h1 id="APK的反编译"><a href="#APK的反编译" class="headerlink" title="APK的反编译"></a>APK的反编译</h1><p>我是按照这篇<a href="https://www.jianshu.com/p/6c4f19331284">博客</a>来的。不过我的实际情况是，在使用apktool时遇到了<code>The filename, directory name, or volume label syntax is incorrect.</code>的异常，后来我就直接把后缀apk改成了zip，然后就直接解压缩了。</p>
<p>下面对上述博客教程做一些补充说明：</p>
<ol>
<li>下载apktool.bat比较特殊，它是通过右击鼠标另存为来下载的，而且还得上得了外网才能下载。</li>
<li>文中提到的第二个工具dex2jar，你很有可能会下载到旧的2.0版，我这里是用了2.1版的才成功，报错与解决方案<a href="https://www.cnblogs.com/onelikeone/p/7594177.html">在此</a>或<a href="https://blog.csdn.net/qq_16666847/article/details/84027949">在此</a>。</li>
<li>博客中第3步讲到“<code>双击运行 jd-gui-1.4.0.jar 文件</code>”,是的，只要你电脑上有安装jdk，那么双击jar文件就能像双击exe一样打开软件界面。</li>
<li>可能就是因为我是通过直接改apk后缀名解压的，所以我用<code>d2j-dex2jar.bat classes.dex</code>生成classes-dex2jar.jar时遇到了一个小报错，不过我依然把它拖进jd-gui-1.4.0.jar的界面里，一看还行，能看到大部分源码，而且都暴露无遗<br><img src="https://s1.ax1x.com/2020/06/09/thjyKP.png" alt="thjyKP.png"></li>
</ol>
<h1 id="APK的加固"><a href="#APK的加固" class="headerlink" title="APK的加固"></a>APK的加固</h1><p>一开始我是用Android Studio的Generate Signed APK打包APK，打包后的apk我上传到腾讯云-移动应用安全加固，结果老是报错<code>此安装包缺少签名</code>，我猜可能是Android Studio用的签名格式是JKS，而腾讯云支持的签名格式是keystore。后来我知道，腾讯云除了这个移动应用安全的<a href="https://console.cloud.tencent.com/ms/reinforce/list">网页版服务</a>之外，还有个客户端软件-<strong>乐固</strong>，不过现在网上似乎很难找到这个安装包了，这是我从CSDN上找到的一个安装包，亲测可用，下载链接：<a href="https://share.weiyun.com/7PDixZRV。">https://share.weiyun.com/7PDixZRV。</a> 建议大家使用客户端加固而不是网页端。<br><span id="more"></span><br>乐固软件初始界面：<br><img src="https://s1.ax1x.com/2020/06/09/thjuEF.png" alt="thjuEF.png"><br>点击辅助工具标签页后，可以看到左侧有<strong>签名APK</strong>的功能<br><img src="https://s1.ax1x.com/2020/06/09/tImq8s.png" alt="tImq8s.png"><br>下面还有<strong>制作签名</strong>的功能，下面有keystore格式和jks格式让你选，我选择keystore格式，用它来给apk签名，腾讯云就能识别出来了<br><img src="https://s1.ax1x.com/2020/06/09/thj3g1.png" alt="thj3g1.png"></p>
<p>这个软件的操作还是比较简单明了的，大家下载后自己应该可以看得懂怎么操作。完成<strong>制作签名</strong>、<strong>签名APK</strong>两个步骤后，就是把签过名的apk上传上去了，最后加固成功的样子如下：<br><img src="https://s1.ax1x.com/2020/06/09/tIp9l4.png" alt="tIp9l4.png"></p>
<p>同一个账户的信息也会更新到网页端去：<br><img src="https://s1.ax1x.com/2020/06/09/thjUED.png" alt="thjUED.png"><br>不过网页版的似乎还麻烦一点，腾讯云说：在加固apk之前，腾讯云得先把apk的签名移除掉才能加固，所以加固过的apk是没有签名的。所以你需要再回到乐固客户端，再对apk进行签名，然后就全部都好了。</p>
<p>客户端的就方便多了，从下图可以看到，软件在app-release_aligned_signed.apk（即用乐固签过名的app-release.apk）的基础上生成了2个apk：<br><img src="https://s1.ax1x.com/2020/06/09/tIpu1e.png" alt="tIpu1e.png"><br>最后，加密过的apk就这么到手啦😄</p>
<p>我们再把乐固加固过的apk导入Android Studio里分析，发现这时候class.dex已经是乱码不规则的形式了（如下图所示），一定程度上增加了反编译的难度。<br>|||<br>|—|—|<br>|加固前|<img src="https://s1.ax1x.com/2020/06/09/thjZuV.png" alt="thjZuV.png">|<br>|加固后|<img src="https://s1.ax1x.com/2020/06/09/thjk3n.png" alt="thjk3n.png">|</p>
<p>后记</p>
<p>听朋友说类似的加固产品还有360加固，因为目前我用腾讯乐固就算比较满意了，感兴趣的朋友可以多试试</p>
<p>参考阅读<br><a href="https://www.jianshu.com/p/e836428d61b9">Android apk加固实现原理</a><br><a href="https://www.jianshu.com/p/53078d03c9bf">Android-APK签名工具-jarsigner和apksigner</a></p>
]]></content>
  </entry>
  <entry>
    <title>pathdata绘制复杂矢量图形</title>
    <url>/2020/06/05/pathdata%E7%BB%98%E5%88%B6%E5%A4%8D%E6%9D%82%E7%9F%A2%E9%87%8F%E5%9B%BE%E5%BD%A2/</url>
    <content><![CDATA[<h2 id="绘制水滴状"><a href="#绘制水滴状" class="headerlink" title="绘制水滴状"></a><strong>绘制水滴状</strong></h2><p><strong>原理</strong></p>
<p>详细pathdata原理请见<a href="https://www.w3.org/TR/SVG/paths.html">这篇SVG教程</a>。</p>
<p>我们看图中右下角最复杂的图形，描述它的pathdata只有6个坐标，而图上曲线有7个点。这是因为图上那个蓝色的点不用我们定义</p>
<p><img src="https://s1.ax1x.com/2020/06/05/ts58sO.png" width="70% " height="70%" ></p>
<p>根据以上原理，我们要绘制水滴状,需要以下构造点：<br><img src="https://s1.ax1x.com/2020/07/02/NLtJ10.png" alt="NLtJ10.png"></p>
<p><strong>代码实现</strong></p>
<p><img src="https://s1.ax1x.com/2020/07/02/NLtRBD.png" alt="NLtRBD.png"></p>
<p><strong>最终效果</strong></p>
<p><a href="https://imgchr.com/i/NLt7gP"><img src="https://s1.ax1x.com/2020/07/02/NLt7gP.png" alt="NLt7gP.png"></a></p>
<p><strong>后记</strong></p>
<p>我这里是自己绘制一个水滴垂涎状的图形，大部分情况下，我们并不需要自己计算出pathdata来绘制矢量图形，如果我们能在网上找到想要的图标的话，把它下载下来，注意得是svg格式，然后以记事本的方式打开，我们就能看到里面的pathdata了。举个实例，我在<a href="https://material.io/resources/icons/?icon=volume_up&amp;style=baseline">material.io</a>网站找到了一个扬声器的图标volume_up.svg<br><img src="https://s1.ax1x.com/2020/06/16/NPd7yn.png" alt="NPd7yn.png"></p>
<p>下载到本地用记事本打开看到的是这样的：<br><img src="https://s1.ax1x.com/2020/06/16/NPdTQs.png" alt="NPdTQs.png"></p>
<p>不过我试着把svg的path数据换到xml的path data里去，结果图形并没有显示出来。我还是乖乖地把这个svg图形直接复制到drawable下面就好了。</p>
]]></content>
      <categories>
        <category>Android</category>
      </categories>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title>效率工具聚宝盒（持续更新）</title>
    <url>/2020/05/28/%E6%95%88%E7%8E%87%E5%B7%A5%E5%85%B7%E8%81%9A%E5%AE%9D%E7%9B%92%EF%BC%88%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%EF%BC%89/</url>
    <content><![CDATA[<h2 id="技巧："><a href="#技巧：" class="headerlink" title="技巧："></a><strong>技巧：</strong></h2><ul>
<li><p>1.设置常用语的快捷输入，比如邮箱、电话、pip源</p>
</li>
<li><p>2.<kbd>win</kbd> + <kbd>←</kbd> / <kbd>→</kbd> 实现多屏显示</p>
</li>
</ul>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a><strong>工具</strong></h2><ul>
<li><p><strong>Typora + PicGo</strong>：如果你经常用markdown写博客，那么建议你用typora编辑器，我用过VSCode，它是那种左侧源码，右侧预览的模式，文章图片一多，就容易出现左右对不齐的情况，而Typora是及时渲染，写完markdown马上渲染出来。而且它配合PicGo，可以实现将粘贴板的图片用Ctrl + V直接上传到Github自建的图床仓库上！【<a href="https://www.cnblogs.com/focksor/p/12402471.html">教程</a>】</p>
</li>
<li><p>取色器<strong>Pipette</strong>,我主要是用它来给图标icon取相同的颜色</p>
<p><img src="https://pic.downk.cc/item/5f958ba71cd1bbb86ba0e926.jpg" width=50%></p>
</li>
<li><p>解除medium的付费阅读限制 - <a href="https://github.com/manojVivek/medium-unlimited">解锁神器</a><br><img src=https://s1.ax1x.com/2020/06/29/NRx4nH.png width=80%></p>
</li>
<li><p>预览SVG图片 - <a href="https://github.com/tibold/svg-explorer-extension">解锁神器</a></p>
</li>
<li><p>替代百度网盘的蓝奏网盘 - <a href="https://up.woozooo.com/mydisk.php">解锁神器</a></p>
</li>
<li><p>下载github仓库的某个文件夹 - <a href="https://tortoisesvn.net/downloads.html"><strong>TortoiseSVN</strong></a>。<br>使用时注意要把<code>/tree/master/</code> 替换成 <code>/trunk/</code><br><img src="https://s1.ax1x.com/2020/07/11/UlsoDI.png" alt="UlsoDI.png" border="0" /></p>
</li>
<li><p>在线LaTex编辑器，快速写出Latex公式，主要是它的图片识别太好用了，妈咪说出品，里面地图片识别据作者说用的mathpix的API，但它是收费的，但妈咪说帮我们付了这些费用 - <a href="https://www.latexlive.com/">解锁神器</a></p>
<p>有快捷工具，有公式模板，还有图片识别，这个我觉得最实用，粘贴剪切板的截图，就能输出公式，再把公式复制到markdown里,最后别忘了首尾加上漏掉的$$：<br><img src="https://s1.ax1x.com/2020/09/26/0PUV6H.png" alt="0PUV6H.png" border="0" /><br>另外这篇博客《<a href="https://www.cnblogs.com/1024th/p/11623258.html">LaTeX公式手册(全网最全)》</a>也汇总了很多LATEX的用法，也有参考意义。</p>
</li>
<li><p>查找并替换软件：fnr<br><img src="https://pic.downk.cc/item/5f944a241cd1bbb86b58cc7c.jpg"></p>
</li>
</ul>
<p><strong>AirDroid</strong></p>
<p><a href="https://www.airdroid.com/zh-cn/index.html">企业网站</a> | <a href="http://web.airdroid.com/">网页版工具</a></p>
<p>这是个强大的设备间传输通信工具，它可以用来在电脑和手机上双向传输文件，对于我来说，如果用微信qq传，你可能不知道传输完的文件保存在哪里，而AirDroid可以让你指定文件的保存路径，这是我用AirDroid的初衷。AIrDroid还是安卓界的TeamViewer，你可以用你的任何设备（电脑或安卓手机）远程控制你的另外一部手机，这个可以用来干嘛，可以让你在地铁上就可以打卡！AirDroid还可以把你的手机屏幕投影到电脑，关键是简单，还可以打开你的相机将拍摄到的画面传到电脑上，还可以找回手机，但是有点误差，可能是故意加密的。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201213134432.png" alt="image-20201213134431848"></p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201213133112.png" alt="image-20201213133112707" style="zoom:67%;" /></p>
<p><strong>Auto Mouse Mover</strong></p>
<p>根据你的设置，定时让你的鼠标按照你的想法移动，我主要是用来保持屏幕常亮，防止colab被踢下线，Unix版的用这个<a href="http://www.murguu.com/linux-auto-mouse-click">Linux Auto Mouse Click</a></p>
<p><strong>NewFileTime</strong></p>
<p>在默认情况下，一个文件的创建时间和修改时间是系统自己生成的，我们不能修改它。但我们有时为了某些特殊需要，为了不让别人一眼看出文件已经给修改了，我们就需要修改文件的创建时间和修改时间，用NewFileTime就可以帮你实现这个功能。<a href="https://www.cr173.com/soft/12992.html">使用教程</a>|<a href="http://www.softwareok.com/?Download=NewFileTime">下载地址</a></p>
<h2 id="word技巧"><a href="#word技巧" class="headerlink" title="word技巧"></a><strong>word技巧</strong></h2><ol>
<li><p>在英文单词和中文释义中间插入/字符（注意{1,}里逗号后面没有空格）：<br><img src="https://i.loli.net/2020/08/22/GrhlaweUtgESsRC.png" alt="image.png"></p>
</li>
<li><p>段落首添加字符#：替换^p为^p#</p>
<p>段落尾添加字符#：替换^p为#^p</p>
</li>
</ol>
<blockquote>
<p>更多word通配符的运用，请见这篇<a href="https://blog.csdn.net/p3118601/article/details/85259552">博客</a>。</p>
</blockquote>
<h2 id="目标检测效率工具"><a href="#目标检测效率工具" class="headerlink" title="目标检测效率工具"></a><strong>目标检测效率工具</strong></h2><p>目标检测开始阶段需要制作数据，这时候你要么自己写python脚本批量处理，要么用现成的软件：</p>
<ul>
<li><strong>光影魔术手</strong></li>
</ul>
<p><img src="https://pic.downk.cc/item/5f9e8e831cd1bbb86b0d4276.jpg"></p>
<p>把两张portrait图片拼接成720p的landscape图片：</p>
<p><img src="https://pic.downk.cc/item/5f9e908d1cd1bbb86b0db1de.jpg"></p>
<ul>
<li><strong>watermarkly</strong>：<a href="https://watermarkly.com/crop-photo/">网站</a></li>
</ul>
<p><img src="https://pic.downk.cc/item/5f9e8f6d1cd1bbb86b0d7141.jpg"></p>
<p>因为我的硬件相机的图像分辨率是720p，所以我把裁剪框大小统一设置成1280×720：</p>
<p><img src="https://pic.downk.cc/item/5f9e8f9a1cd1bbb86b0d7aab.jpg"></p>
<p><strong>美图秀秀网页版</strong>：<a href="https://xiuxiu.web.meitu.com/plugin/batch/taobao.html">网站</a></p>
<p>批量添加铺满水印，防止数据集委托他人标注时被被肆意传播，更进一步，你还可以分批找不同的人标注，不要把所有鸡蛋放一个篮子里：</p>
<p><img src="https://pic.downk.cc/item/5f9eb2101cd1bbb86b158ebf.jpg"></p>
<h1 id="Python-库"><a href="#Python-库" class="headerlink" title="Python 库"></a>Python 库</h1><p>《Top 10 Python libraries of 2020》 <a href="https://tryolabs.com/blog/2020/12/21/top-10-python-libraries-of-2020/">英文</a>|<a href="https://mp.weixin.qq.com/s/JR4U6C4kvDmST7Dw4UoH-A">中文</a></p>
<p><a href="https://github.com/mstamy2/PyPDF2">PyPDF2</a></p>
]]></content>
  </entry>
  <entry>
    <title>PaddleX初体验</title>
    <url>/2020/05/24/PaddleX%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
    <content><![CDATA[<p>今天是2020/5/24，距离PaddleX 发布1.0.0 刚过去4天。正因为paddlesx刚出来不久，所以操作体验和功能完整性还不是那么成熟完美，不过我还是想提前体验一下用paddlex来训练自己数据集的目标检测模型。请看视频：</p>
<iframe width="720" height="480" src="//player.bilibili.com/player.html?aid=883335737&bvid=BV1QK4y1t7yq&cid=196552357&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>这次训练的效果不怎么样，但我当时的目的就是想，快速跑通和体验paddlex的使用流程，准确度高不是我当时的目的~</p>
<p>训练好模型后，接下来要做的就是怎么部署了，<a href="https://paddlex.readthedocs.io/zh_CN/latest/tutorials/deploy/deploy_lite.html"><strong>官网教程</strong></a>是这么介绍的：<br><span id="more"></span></p>
<blockquote>
<p>移动端部署<br><br><strong>step 1: 安装PaddleLite</strong><br><br><code>pip install -i https://mirror.baidu.com/pypi/simple paddlelite</code><br><br><br><strong>step 2: 将PaddleX模型导出为inference模型</strong><br><br><br>参考<a href="https://paddlex.readthedocs.io/zh_CN/latest/tutorials/deploy/deploy_server/deploy_python.html#inference">导出inference模型</a>将模型导出为inference格式模型。<br><br><br><strong>step 3: 将inference模型转换成PaddleLite模型</strong><br><br><br><code>python /path/to/PaddleX/deploy/lite/export_lite.py --model_path /path/to/inference_model --save_dir /path/to/onnx_model</code><br><br><br><code>--model_path</code>用于指定inference模型的路径，<code>--save_dir</code>用于指定Lite模型的保存路径。<br><br><br><strong>step 4: 预测</strong><br><br><br>Lite模型预测正在集成中，即将开源…</p>
</blockquote>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>PaddleX</tag>
      </tags>
  </entry>
  <entry>
    <title>探索Mobilenet SSD家族的TFLite</title>
    <url>/2020/05/22/%E6%8E%A2%E7%B4%A2Mobilenet%20SSD%E5%AE%B6%E6%97%8F%E7%9A%84TFLite/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本篇文章将介绍如何使用脚本将pb格式的预训练模型转换为tflite格式的模型，并使用脚本测试这个生成的tflite的检测效果，以便在部署到移动端之前可以知道这个生成的模型性能如何。您也可以在<a href="https://colab.research.google.com/gist/jvishnuvardhan/321551f11317d0789027fab2f63186ff/ssd_mobilenet_quantized_model_tflite_converter.ipynb#scrollTo=6EMJAeVrs1pO">gist</a>上快速浏览体验本文的代码。</p>
<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>操作系统：Windows 10</p>
<p>tensorflow 版本：1.15（2.0，2.1也可以，2.2不行,因为它的API没有tf.compat.v1）</p>
<h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ol>
<li><p>从<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">Tensorflow detection model zoo</a>下载<a href="http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz">ssd_mobilenet_v2_quantized_coco</a>这个压缩包，解压之后里面能看到一个<strong>tflite_graph.pb</strong>,这个就是接下来我们要转换的pb格式模型。</p>
</li>
<li><p>将pb格式模型转换成tflite格式模型的脚本如下：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># TFLiteConverter_uint8.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line"></span><br><span class="line">graph_def_file = <span class="string">&quot;E:/Tensorflow_detection_model_zoo/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph.pb&quot;</span></span><br><span class="line">input_arrays=[<span class="string">&quot;normalized_input_image_tensor&quot;</span>]</span><br><span class="line">output_arrays=[<span class="string">&#x27;TFLite_Detection_PostProcess&#x27;</span>, <span class="string">&#x27;TFLite_Detection_PostProcess:1&#x27;</span>, <span class="string">&#x27;TFLite_Detection_PostProcess:2&#x27;</span>, <span class="string">&#x27;TFLite_Detection_PostProcess:3&#x27;</span>]</span><br><span class="line">input_shape=&#123;<span class="string">&quot;normalized_input_image_tensor&quot;</span>: [<span class="number">1</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>]&#125;</span><br><span class="line"><span class="comment"># 转换模型前请先确认好这个pb模型是否已经量化过了</span></span><br><span class="line">pb_model_has_quantitized = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shape)</span><br><span class="line">converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]</span><br><span class="line">converter.allow_custom_ops = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">bool</span>(pb_model_has_quantitized) <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">    <span class="comment"># 如果pb模型还没有量化过，则使用以下代码。不过测试结果证明，post_training_quantize的精度非常地差</span></span><br><span class="line">    converter.post_training_quantize = <span class="literal">True</span>  <span class="comment"># 如果没有pb模型还没量化过，才使用这句代码</span></span><br><span class="line">    <span class="comment"># 但用这句代码后会报出一个警告：UserWarning: Property post_training_quantize is deprecated, please use optimizations=[Optimize.DEFAULT] instead.</span></span><br><span class="line">    converter.optimizations = [tf.compat.v1.lite.Optimize.DEFAULT]  <span class="comment"># 这句代码跟上面那句代码选一个就行，最终生成的模型都一样</span></span><br><span class="line">    converter.inference_type = tf.compat.v1.lite.constants.FLOAT  <span class="comment"># `optimizations` require that `inference_type` is set to float.</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 如果pb模型已经量化过了，如ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03，就只使用下面一句代码就够了</span></span><br><span class="line">    converter.inference_type = tf.compat.v1.lite.constants.QUANTIZED_UINT8</span><br><span class="line"></span><br><span class="line">input_arrays = converter.get_input_arrays()</span><br><span class="line">converter.quantized_input_stats = &#123;input_arrays[<span class="number">0</span>]: (<span class="number">128.0</span>, <span class="number">128.0</span>)&#125;  <span class="comment"># mean, std_dev</span></span><br><span class="line">tflite_uint8_model = converter.convert()</span><br><span class="line"><span class="built_in">open</span>(<span class="string">&quot;uint8_model_converted_from_&quot;</span>+os.path.basename(os.path.dirname(graph_def_file))+<span class="string">&quot;.tflite&quot;</span>, <span class="string">&quot;wb&quot;</span>).write(tflite_uint8_model)</span><br></pre></td></tr></table></figure>
<ol>
<li>测试生成的uint8_model_converted_from_ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tflite的检测效果，使用如下脚本：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Test_TFLite_Model_With_Image_Folder.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line"><span class="comment"># Load TFLite model and allocate tensors.</span></span><br><span class="line">interpreter = tf.lite.Interpreter(model_path=<span class="string">&quot;uint8_model_converted_from_ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tflite&quot;</span>)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get input and output tensors.</span></span><br><span class="line">input_details = interpreter.get_input_details()</span><br><span class="line">output_details = interpreter.get_output_details()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input_details : &quot;</span>, input_details)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;output_details : &quot;</span>, output_details)</span><br><span class="line"><span class="comment"># 错误的输出结果：&#x27;TFLite_Detection_PostProcess&#x27;的&#x27;shape&#x27;: array是([ 1, 0,  4])</span></span><br><span class="line"><span class="comment"># 官方模型的输出结果：</span></span><br><span class="line"><span class="comment"># [&#123;&#x27;name&#x27;: &#x27;normalized_input_image_tensor&#x27;, &#x27;index&#x27;: 175, &#x27;shape&#x27;: array([  1, 300, 300,   3]), &#x27;dtype&#x27;: &lt;class &#x27;numpy.uint8&#x27;&gt;, &#x27;quantization&#x27;: (0.0078125, 128)&#125;]</span></span><br><span class="line"><span class="comment"># [&#123;&#x27;name&#x27;: &#x27;TFLite_Detection_PostProcess&#x27;, &#x27;index&#x27;: 167, &#x27;shape&#x27;: array([ 1, 10,  4]), &#x27;dtype&#x27;: &lt;class &#x27;numpy.float32&#x27;&gt;, &#x27;quantization&#x27;: (0.0, 0)&#125;,</span></span><br><span class="line"><span class="comment">#  &#123;&#x27;name&#x27;: &#x27;TFLite_Detection_PostProcess:1&#x27;, &#x27;index&#x27;: 168, &#x27;shape&#x27;: array([ 1, 10]), &#x27;dtype&#x27;: &lt;class &#x27;numpy.float32&#x27;&gt;, &#x27;quantization&#x27;: (0.0, 0)&#125;,</span></span><br><span class="line"><span class="comment">#  &#123;&#x27;name&#x27;: &#x27;TFLite_Detection_PostProcess:2&#x27;, &#x27;index&#x27;: 169, &#x27;shape&#x27;: array([ 1, 10]), &#x27;dtype&#x27;: &lt;class &#x27;numpy.float32&#x27;&gt;, &#x27;quantization&#x27;: (0.0, 0)&#125;,</span></span><br><span class="line"><span class="comment">#  &#123;&#x27;name&#x27;: &#x27;TFLite_Detection_PostProcess:3&#x27;, &#x27;index&#x27;: 170, &#x27;shape&#x27;: array([1]), &#x27;dtype&#x27;: &lt;class &#x27;numpy.float32&#x27;&gt;, &#x27;quantization&#x27;: (0.0, 0)&#125;]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Test model on images in folder</span></span><br><span class="line">test_images_dir = <span class="string">&#x27;test_images/&#x27;</span></span><br><span class="line">test_images_list = os.listdir(test_images_dir)</span><br><span class="line"><span class="comment"># print(test_images_list)</span></span><br><span class="line">class_names = [<span class="string">&#x27;person&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;motorcycle&#x27;</span>, <span class="string">&#x27;airplane&#x27;</span>,<span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>, <span class="string">&#x27;traffic light&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;fire hydrant &#x27;</span>, <span class="string">&#x27;stop sign&#x27;</span>, <span class="string">&#x27;parking meter&#x27;</span>, <span class="string">&#x27;bench&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>,<span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>, <span class="string">&#x27;elephant&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;zebra&#x27;</span>, <span class="string">&#x27;giraffe&#x27;</span>, <span class="string">&#x27;backpack&#x27;</span>, <span class="string">&#x27;umbrella&#x27;</span>, <span class="string">&#x27;handbag&#x27;</span>, <span class="string">&#x27;tie&#x27;</span>,<span class="string">&#x27;suitcase&#x27;</span>, <span class="string">&#x27;frisbee&#x27;</span>, <span class="string">&#x27;skis&#x27;</span>, <span class="string">&#x27;snowboard&#x27;</span>, <span class="string">&#x27;sports ball&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;kite&#x27;</span>, <span class="string">&#x27;baseball bat&#x27;</span>, <span class="string">&#x27;baseball glove&#x27;</span>, <span class="string">&#x27;skateboard&#x27;</span>, <span class="string">&#x27;surfboard&#x27;</span>, <span class="string">&#x27;tennis racket&#x27;</span>, <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;wine glass&#x27;</span>, <span class="string">&#x27; cup&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;fork&#x27;</span>, <span class="string">&#x27;knife&#x27;</span>, <span class="string">&#x27;spoon&#x27;</span>, <span class="string">&#x27;bowl&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;sandwich&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;broccoli&#x27;</span>, <span class="string">&#x27;carrot&#x27;</span>, <span class="string">&#x27;hot dog&#x27;</span>, <span class="string">&#x27;pizza&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;donut&#x27;</span>, <span class="string">&#x27;cake&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;couch&#x27;</span>, <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;bed&#x27;</span>, <span class="string">&#x27;dining table&#x27;</span>, <span class="string">&#x27;toilet&#x27;</span>, <span class="string">&#x27;tv&#x27;</span>, <span class="string">&#x27;laptop&#x27;</span>, <span class="string">&#x27;mouse&#x27;</span>, <span class="string">&#x27;remote&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;keyboard&#x27;</span>, <span class="string">&#x27; cell phone&#x27;</span>, <span class="string">&#x27;microwave&#x27;</span>, <span class="string">&#x27;oven&#x27;</span>, <span class="string">&#x27;toaster&#x27;</span>, <span class="string">&#x27;sink&#x27;</span>, <span class="string">&#x27;refrigerator&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;clock&#x27;</span>, <span class="string">&#x27;vase&#x27;</span>, <span class="string">&#x27;scissors&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;teddy bear&#x27;</span>, <span class="string">&#x27;hair drier&#x27;</span>, <span class="string">&#x27;toothbrush&#x27;</span>]</span><br><span class="line"></span><br><span class="line">total_interpreter_time = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> test_image <span class="keyword">in</span> test_images_list:</span><br><span class="line">    <span class="comment"># print(test_image)</span></span><br><span class="line">    input_shape = input_details[<span class="number">0</span>][<span class="string">&#x27;shape&#x27;</span>]</span><br><span class="line">    image = cv.imread(test_images_dir+test_image)</span><br><span class="line">    <span class="comment"># print(image)</span></span><br><span class="line">    <span class="comment"># print(image.shape)</span></span><br><span class="line">    resize_img = cv.resize(image, (<span class="number">300</span>, <span class="number">300</span>), interpolation=cv.INTER_CUBIC)</span><br><span class="line">    reshape_image = resize_img.reshape(<span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># print(reshape_image)</span></span><br><span class="line">    <span class="comment"># print(reshape_image.shape)  # 输出应该是(300, 300, 3)</span></span><br><span class="line">    image_np_expanded = np.expand_dims(reshape_image, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># print(image_np_expanded.shape)  # 输出应该是(1, 300, 300, 3)</span></span><br><span class="line">    image_np_expanded = image_np_expanded.astype(<span class="string">&#x27;uint8&#x27;</span>)  <span class="comment"># float32</span></span><br><span class="line">    <span class="comment"># print(image_np_expanded)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tflite模型开始正式工作</span></span><br><span class="line">    model_interpreter_start_time = time.time()</span><br><span class="line">    interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>], image_np_expanded)  <span class="comment"># 当输入数据类型与模型的期望类型不一致时，就会出现“Cannot set tensor: Got tensor of type 1（float32） but expected type 3（uint8） for input 175 ”</span></span><br><span class="line">    interpreter.invoke()</span><br><span class="line">    <span class="comment"># print(output_details[0][&#x27;index&#x27;])  # 输出是167</span></span><br><span class="line">    <span class="comment"># print(output_details[1][&#x27;index&#x27;])  # 输出是168，后面两个以此类推</span></span><br><span class="line">    output_data = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data)  # 输出10个目标的位置信息</span></span><br><span class="line">    output_data_1 = interpreter.get_tensor(output_details[<span class="number">1</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data_1)  # 输出10个目标的类别id</span></span><br><span class="line">    output_data_2 = interpreter.get_tensor(output_details[<span class="number">2</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data_2)  # 输出10个目标的置信度（从高到低）</span></span><br><span class="line">    output_data_3 = interpreter.get_tensor(output_details[<span class="number">3</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data_3)  # 输出检测出来的目标个数</span></span><br><span class="line"></span><br><span class="line">    each_interpreter_time = time.time() - model_interpreter_start_time</span><br><span class="line"></span><br><span class="line">    <span class="comment"># max_pointer = output_data.reshape(300, 300, 2).argmax(axis=2)  # Returns the indices of the maximum values along an axis.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(output_data[0][0][0])</span></span><br><span class="line">    <span class="comment"># print(output_data[0][0][1])</span></span><br><span class="line">    <span class="comment"># print(output_data[0][0][2])</span></span><br><span class="line">    <span class="comment"># print(output_data[0][0][3])</span></span><br><span class="line">    <span class="comment"># 输出坐标：（y方向系数，x方向系数，y方向系数，x方向系数），这个系数是乘以原图像的分辨率，不是乘以resize后的300</span></span><br><span class="line">    <span class="comment"># 坐上顶点：（x方向坐标，y方向坐标）</span></span><br><span class="line">    original_image_height, original_image_width, _ = image.shape</span><br><span class="line">    thickness = original_image_height//<span class="number">300</span>  <span class="comment"># 为了适配不同分辨率的图片，动态调整线框厚度和字体缩放比例</span></span><br><span class="line">    fontsize = original_image_height/<span class="number">1000</span></span><br><span class="line">    <span class="comment"># print(test_image + &#x27; width ： &#x27; + str(original_image_width))</span></span><br><span class="line">    <span class="built_in">print</span>(test_image + <span class="string">&#x27; height ： &#x27;</span> + <span class="built_in">str</span>(original_image_height))</span><br><span class="line">    <span class="built_in">print</span>(thickness)</span><br><span class="line">    <span class="built_in">print</span>(fontsize)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(output_data_1[<span class="number">0</span>])):</span><br><span class="line">        <span class="comment"># print(i)</span></span><br><span class="line">        <span class="comment"># print(class_names[int(output_data_1[0][i])])</span></span><br><span class="line">        confidence_threshold = output_data_2[<span class="number">0</span>][i]</span><br><span class="line">        <span class="keyword">if</span> confidence_threshold &gt; <span class="number">0.3</span>:</span><br><span class="line">            <span class="comment"># 只绘制概率大于阈值的矩形框</span></span><br><span class="line">            label = <span class="string">&quot;&#123;&#125;: &#123;:.2f&#125;% &quot;</span>.<span class="built_in">format</span>(class_names[<span class="built_in">int</span>(output_data_1[<span class="number">0</span>][i])], output_data_2[<span class="number">0</span>][i] * <span class="number">100</span>)  <span class="comment"># label包含类别名称、置信度、单张图片推理时间</span></span><br><span class="line">            label2 = <span class="string">&quot;inference time : &#123;:.3f&#125;s&quot;</span> .<span class="built_in">format</span>(each_interpreter_time)</span><br><span class="line">            <span class="comment"># 注意output_data第一个和第二个[]里都是0，然后第三个[]不是按0到3的顺序</span></span><br><span class="line">            left_up_corner = (<span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">1</span>]*original_image_width), <span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">0</span>]*original_image_height))</span><br><span class="line">            left_up_corner_higher = (<span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">1</span>]*original_image_width), <span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">0</span>]*original_image_height)-<span class="number">15</span>)</span><br><span class="line">            right_down_corner = (<span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">3</span>]*original_image_width), <span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">2</span>]*original_image_height))</span><br><span class="line">            cv.rectangle(image, left_up_corner_higher, right_down_corner, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness)</span><br><span class="line">            <span class="comment"># cv.rectangle()里的第2个第3个参数代表的是 左上角点的(x,y)，右下角点的(x,y)，不同于Rect rect(x, y, w, h);//左上坐标（x,y）和矩形的长(w)宽(h)</span></span><br><span class="line">            cv.putText(image, label, left_up_corner_higher, cv.FONT_HERSHEY_DUPLEX, fontsize, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), thickness=thickness)</span><br><span class="line">            cv.putText(image, label2, (<span class="number">30</span>, <span class="number">30</span>), cv.FONT_HERSHEY_DUPLEX, fontsize, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), thickness=thickness)</span><br><span class="line">    cv.namedWindow(<span class="string">&#x27;detect_result&#x27;</span>, cv.WINDOW_NORMAL)</span><br><span class="line">    cv.resizeWindow(<span class="string">&#x27;detect_result&#x27;</span>, <span class="number">800</span>, <span class="number">600</span>)</span><br><span class="line">    cv.imshow(<span class="string">&quot;detect_result&quot;</span>, image)</span><br><span class="line">    key = cv.waitKey(<span class="number">2000</span>) &amp; <span class="number">0xFF</span>  <span class="comment"># waitKey(delay)里的参数代表延时多少毫秒后刷新窗口画面，delay=0则代表无限延时，=2000代表检测结果每隔2秒轮播</span></span><br><span class="line">    <span class="keyword">if</span> key == <span class="built_in">ord</span>(<span class="string">&quot;q&quot;</span>):  <span class="comment"># 按Q键退出程序</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> key == <span class="number">32</span>:  <span class="comment"># 空格键的ASCII码是32</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;暂停&quot;</span>)</span><br><span class="line">        cv.waitKey(<span class="number">0</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;继续&quot;</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol>
<li>查看检测结果</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>ssd_mobilenet_v2_quantized</th>
<th>对标模型detect.tflite</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://s1.ax1x.com/2020/05/31/t1gMl9.png" alt="t1gMl9.png"></td>
<td><img src="https://s1.ax1x.com/2020/05/31/t12k1H.png" alt="t12k1H.png"></td>
</tr>
<tr>
<td><img src="https://s1.ax1x.com/2020/05/31/t1glO1.png" alt="t1glO1.png"></td>
<td><img src="https://s1.ax1x.com/2020/05/31/t12ZnI.png" alt="t12ZnI.png"></td>
</tr>
<tr>
<td><img src="https://s1.ax1x.com/2020/05/31/t1cfZ6.png" alt="t1cfZ6.png"></td>
<td><img src="https://s1.ax1x.com/2020/05/31/t1g6k8.png" alt="t1g6k8.png"></td>
</tr>
<tr>
<td><img src="https://s1.ax1x.com/2020/05/31/t1gSJg.png" alt="t1gSJg.png"></td>
<td><img src="https://s1.ax1x.com/2020/05/31/t1gLp4.png" alt="t1gLp4.png"></td>
</tr>
<tr>
<td><img src="https://s1.ax1x.com/2020/05/31/t1gFLq.png" alt="t1gFLq.png"></td>
<td><img src="https://s1.ax1x.com/2020/05/31/t12S76.png" alt="t12S76.png"></td>
</tr>
<tr>
<td><img src="https://s1.ax1x.com/2020/05/31/t1g0OI.png" alt="t1g0OI.png"></td>
<td><img src="https://s1.ax1x.com/2020/05/31/t12uAf.png" alt="t12uAf.png"></td>
</tr>
</tbody>
</table>
</div>
<p>看上去v2的效果有好一点点，不过这是用更长的推理时间换来的。</p>
<p><strong>视频对比（@小米8SE）：</strong></p>
<iframe width="720" height="480" src="//player.bilibili.com/player.html?aid=968315121&bvid=BV1gp4y1X7Fg&cid=195785533&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<p>后来我才发现我所对标的detect.tflite不论是从图结构、卷积核大小和个数都更像是<a href="http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_18.tar.gz">ssd_mobilenet_v1_0.75_depth_quantized_coco ☆</a>，但是用<a href="https://www.lutzroeder.com/ai/netron/">netron</a>仔细查看后发现它们在custom部分的值有些不一样：</p>
<p><img src="https://s1.ax1x.com/2020/05/31/t1G1l4.png" alt="t1G1l4.png"><br>我还发现ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18和ssd_mobilenet_v1_0.75_depth_quantized_coco ☆虽然都是ssd_mobilenet_v1，但转换出来的tflite模型差别还不小，比如正常深度版的图上还能看出relu6层，而0.75深度版的和detect.tflite一样，都没有relu6层。为什么少了relu6层让我很迷惑。我在网上找了两篇论文（其实是同一个东西）研究：《A Quantization-Friendly<br>Separable Convolution for MobileNets》[<a href="https://www.emc2-ai.org/assets/docs/asplos-18/paper4-presentation.pdf">1</a>][<a href="https://arxiv.org/pdf/1803.08607.pdf">2</a>]</p>
<p>下面我用<a href="https://www.lutzroeder.com/ai/netron/">netron</a>展示一下3个模型的部分结构差别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ssd_mobilenet_v2_quantized</td>
<td><img src="https://s1.ax1x.com/2020/05/27/tEi2p6.png" alt="tEi2p6.png"></td>
</tr>
<tr>
<td>detect.tflite</td>
<td><img src="https://s1.ax1x.com/2020/05/27/tEiJkn.png" alt="tEiJkn.png"></td>
</tr>
<tr>
<td>ssd_mobilenet_v1_quantized</td>
<td><img src="https://s1.ax1x.com/2020/05/27/tEid6U.png" alt="tEid6U.png"></td>
</tr>
</tbody>
</table>
</div>
<p>另外，为了动态连续地查看模型地检测效果，我还写了个对视频测试地脚本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Test_TFLite_Model_With_Video.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br><span class="line"></span><br><span class="line">Model_Path = <span class="string">&quot;C:/MachineLearning/CV/uint8_dequantized_model_converted_from_exported_model.tflite&quot;</span></span><br><span class="line">Video_path = <span class="string">&quot;C:/MachineLearning/CV/Object_Tracking/video2.mp4&quot;</span></span><br><span class="line"></span><br><span class="line">interpreter = tf.lite.Interpreter(model_path=Model_Path)</span><br><span class="line">interpreter.allocate_tensors()</span><br><span class="line"><span class="comment"># Get input and output tensors.</span></span><br><span class="line">input_details = interpreter.get_input_details()</span><br><span class="line">output_details = interpreter.get_output_details()</span><br><span class="line"><span class="comment"># print(&quot;input_details : &quot;, input_details)</span></span><br><span class="line"><span class="comment"># print(&quot;output_details : &quot;, output_details)</span></span><br><span class="line"></span><br><span class="line">class_names = [<span class="string">&#x27;person&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;motorcycle&#x27;</span>, <span class="string">&#x27;airplane&#x27;</span>,<span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>, <span class="string">&#x27;traffic light&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;fire hydrant &#x27;</span>, <span class="string">&#x27;stop sign&#x27;</span>, <span class="string">&#x27;parking meter&#x27;</span>, <span class="string">&#x27;bench&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;cow&#x27;</span>, <span class="string">&#x27;elephant&#x27;</span>, <span class="string">&#x27;bear&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;zebra&#x27;</span>, <span class="string">&#x27;giraffe&#x27;</span>, <span class="string">&#x27;backpack&#x27;</span>, <span class="string">&#x27;umbrella&#x27;</span>, <span class="string">&#x27;handbag&#x27;</span>, <span class="string">&#x27;tie&#x27;</span>, <span class="string">&#x27;suitcase&#x27;</span>, <span class="string">&#x27;frisbee&#x27;</span>, <span class="string">&#x27;skis&#x27;</span>, <span class="string">&#x27;snowboard&#x27;</span>, <span class="string">&#x27;sports ball&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;kite&#x27;</span>, <span class="string">&#x27;baseball bat&#x27;</span>, <span class="string">&#x27;baseball glove&#x27;</span>, <span class="string">&#x27;skateboard&#x27;</span>, <span class="string">&#x27;surfboard&#x27;</span>, <span class="string">&#x27;tennis racket&#x27;</span>, <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;wine glass&#x27;</span>, <span class="string">&#x27; cup&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;fork&#x27;</span>, <span class="string">&#x27;knife&#x27;</span>, <span class="string">&#x27;spoon&#x27;</span>, <span class="string">&#x27;bowl&#x27;</span>, <span class="string">&#x27;banana&#x27;</span>, <span class="string">&#x27;apple&#x27;</span>, <span class="string">&#x27;sandwich&#x27;</span>, <span class="string">&#x27;orange&#x27;</span>, <span class="string">&#x27;broccoli&#x27;</span>, <span class="string">&#x27;carrot&#x27;</span>, <span class="string">&#x27;hot dog&#x27;</span>, <span class="string">&#x27;pizza&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;donut&#x27;</span>, <span class="string">&#x27;cake&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>, <span class="string">&#x27;couch&#x27;</span>, <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;bed&#x27;</span>, <span class="string">&#x27;dining table&#x27;</span>, <span class="string">&#x27;toilet&#x27;</span>, <span class="string">&#x27;tv&#x27;</span>, <span class="string">&#x27;laptop&#x27;</span>, <span class="string">&#x27;mouse&#x27;</span>, <span class="string">&#x27;remote&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;keyboard&#x27;</span>, <span class="string">&#x27; cell phone&#x27;</span>, <span class="string">&#x27;microwave&#x27;</span>, <span class="string">&#x27;oven&#x27;</span>, <span class="string">&#x27;toaster&#x27;</span>, <span class="string">&#x27;sink&#x27;</span>, <span class="string">&#x27;refrigerator&#x27;</span>, <span class="string">&#x27;book&#x27;</span>, <span class="string">&#x27;clock&#x27;</span>, <span class="string">&#x27;vase&#x27;</span>, <span class="string">&#x27;scissors&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;teddy bear&#x27;</span>, <span class="string">&#x27;hair drier&#x27;</span>, <span class="string">&#x27;toothbrush&#x27;</span>]</span><br><span class="line"></span><br><span class="line">cap = cv2.VideoCapture(Video_path)</span><br><span class="line">ok, frame_image = cap.read()</span><br><span class="line">original_image_height, original_image_width, _ = frame_image.shape</span><br><span class="line"><span class="comment"># print(&#x27; 帧宽度 ： &#x27; + str(original_image_width))</span></span><br><span class="line"><span class="comment"># print(&#x27; 帧高度 ： &#x27; + str(original_image_height))</span></span><br><span class="line">thickness = original_image_height // <span class="number">500</span>  <span class="comment"># 为了适配不同分辨率的图片，动态调整线框厚度和字体缩放比例</span></span><br><span class="line">fontsize = original_image_height / <span class="number">1500</span></span><br><span class="line"><span class="built_in">print</span>(thickness)</span><br><span class="line"><span class="built_in">print</span>(fontsize)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    ok, frame_image = cap.read()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> ok:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    model_interpreter_start_time = time.time()</span><br><span class="line">    resize_img = cv2.resize(frame_image, (<span class="number">300</span>, <span class="number">300</span>), interpolation=cv2.INTER_CUBIC)</span><br><span class="line">    reshape_image = resize_img.reshape(<span class="number">300</span>, <span class="number">300</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># print(reshape_image)</span></span><br><span class="line">    <span class="comment"># print(reshape_image.shape)  # 输出是(300, 300, 3)</span></span><br><span class="line">    image_np_expanded = np.expand_dims(reshape_image, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># print(image_np_expanded.shape)  # 输出是(1, 300, 300, 3)</span></span><br><span class="line">    image_np_expanded = image_np_expanded.astype(<span class="string">&#x27;uint8&#x27;</span>)  <span class="comment"># float32</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test model</span></span><br><span class="line">    interpreter.set_tensor(input_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>], image_np_expanded)  <span class="comment"># 当输入数据类型与模型的期望类型不一致时，就会出现“Cannot set tensor: Got tensor of type 1（float32） but expected type 3（uint8） for input 175 ”</span></span><br><span class="line">    interpreter.invoke()</span><br><span class="line">    <span class="comment"># print(output_details[0][&#x27;index&#x27;])  # 输出是167</span></span><br><span class="line">    <span class="comment"># print(output_details[1][&#x27;index&#x27;])  # 输出是168，后面两个以此类推</span></span><br><span class="line">    output_data = interpreter.get_tensor(output_details[<span class="number">0</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data)  # 预测框的4个坐标值</span></span><br><span class="line">    output_data_1 = interpreter.get_tensor(output_details[<span class="number">1</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data_1)  # 类别名称id</span></span><br><span class="line">    output_data_2 = interpreter.get_tensor(output_details[<span class="number">2</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data_2)  # 置信度</span></span><br><span class="line">    output_data_3 = interpreter.get_tensor(output_details[<span class="number">3</span>][<span class="string">&#x27;index&#x27;</span>])</span><br><span class="line">    <span class="comment"># print(output_data_3)  # 这个恒等于10，代表最大输出框个数，是在export_tflite_ssd_graph.py里面设置的</span></span><br><span class="line">    each_interpreter_time = time.time() - model_interpreter_start_time</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(output_data_1[<span class="number">0</span>])):</span><br><span class="line">        <span class="comment"># print(i)</span></span><br><span class="line">        <span class="comment"># print(class_names[int(output_data_1[0][i])])</span></span><br><span class="line">        confidence_threshold = output_data_2[<span class="number">0</span>][i]</span><br><span class="line">        <span class="keyword">if</span> confidence_threshold &gt; <span class="number">0.3</span>:</span><br><span class="line">            <span class="comment"># 只绘制概率大于阈值的矩形框</span></span><br><span class="line">            label = <span class="string">&quot;&#123;&#125;: &#123;:.2f&#125;% &quot;</span>.<span class="built_in">format</span>(class_names[<span class="built_in">int</span>(output_data_1[<span class="number">0</span>][i])], output_data_2[<span class="number">0</span>][i] * <span class="number">100</span>)  <span class="comment"># label包含类别名称、置信度、单张图片推理时间</span></span><br><span class="line">            label2 = <span class="string">&quot;inference time : &#123;:.3f&#125;s&quot;</span> .<span class="built_in">format</span>(each_interpreter_time)</span><br><span class="line">            <span class="comment"># 注意output_data第一个和第二个[]里都是0，然后第三个[]不是按0到3的顺序</span></span><br><span class="line">            left_up_corner = (<span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">1</span>]*original_image_width), <span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">0</span>]*original_image_height))</span><br><span class="line">            left_up_corner_higher = (<span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">1</span>]*original_image_width), <span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">0</span>]*original_image_height)-<span class="number">20</span>)</span><br><span class="line">            right_down_corner = (<span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">3</span>]*original_image_width), <span class="built_in">int</span>(output_data[<span class="number">0</span>][i][<span class="number">2</span>]*original_image_height))</span><br><span class="line">            cv2.rectangle(frame_image, left_up_corner_higher, right_down_corner, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), thickness)</span><br><span class="line">            <span class="comment"># cv.rectangle()里的第2个第3个参数代表的是 左上角点的(x,y)，右下角点的(x,y)，不同于Rect rect(x, y, w, h);//左上坐标（x,y）和矩形的长(w)宽(h)</span></span><br><span class="line">            cv2.putText(frame_image, label, left_up_corner_higher, cv2.FONT_HERSHEY_DUPLEX, fontsize, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), thickness=thickness)</span><br><span class="line">            cv2.putText(frame_image, label2, (<span class="number">30</span>, <span class="number">30</span>), cv2.FONT_HERSHEY_DUPLEX, fontsize, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), thickness=thickness)</span><br><span class="line">    cv2.namedWindow(<span class="string">&#x27;detect_result&#x27;</span>, cv2.WINDOW_NORMAL)</span><br><span class="line">    <span class="comment"># cv2.resizeWindow(&#x27;detect_result&#x27;, 800, 600)</span></span><br><span class="line">    cv2.imshow(<span class="string">&quot;detect_result&quot;</span>, frame_image)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use space to pause the video and use q to terminate the program</span></span><br><span class="line">    key = cv2.waitKey(<span class="number">10</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">    <span class="keyword">if</span> key == <span class="built_in">ord</span>(<span class="string">&quot;q&quot;</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">elif</span> key == <span class="number">32</span>:</span><br><span class="line">        cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">cap.release()</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>实际测试时，我发现帧率非常地低，这是因为tensorflow lite的运算内核是专门为ARM处理器优化的，没有针对x86优化，所以在电脑上跑TFlite格式模型会很慢，这点StackOverflow上也有人提问：[<a href="https://stackoverflow.com/questions/54093424/why-is-tensorflow-lite-slower-than-tensorflow-on-desktop">link</a>],但也有人发现了如果在电脑上跑float32位的tflite模型还更快一点[<a href="https://stackoverflow.com/questions/58349690/tflite-quantized-inference-very-show">link</a>],我这边拿<a href="http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz"><strong>ssdlite_mobilenet_v2_coco</strong></a>导出的tflite模型试了一下，果然是这样的，从它的名字可以看出来它是未量化的，虽然它的normalized_input_image_tensor要求输入是uint8位，但到后面的其他运算操作时还是会反量化（dequantize）为浮点数，这个从它的图结构可以看出来：<br><a href="https://imgchr.com/i/t1aukR"><img src="https://s1.ax1x.com/2020/05/31/t1aukR.png" alt="t1aukR.png"></a></p>
<p>要把下载下来的ssdlite_mobilenet_v2_coco里的东西转换成tflite格式模型，这其中还有一些知识点。解压后我们倒是看到frozen_inference_graph.pb,没有看到tflite_graph.pb，那我们怎么生成ssdlite_mobilenet_v2_coco的tflite模型呢？<br><img src="https://s1.ax1x.com/2020/05/27/tEFR5n.png" alt="tEFR5n.png"></p>
<p>这时候我们要用到<strong>export_tflite_ssd_graph.py</strong>将文件夹里的<strong>model.ckpt</strong>先转化为tflite_graph.pb，然后再转化为tflite格式。我使用的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(tf1.13.1) D:\ChromeDownload\models-r1.13.0\research\object_detection&gt;python export_tflite_ssd_graph.py --pipeline_config_path samples&#x2F;configs&#x2F;ssdlite_mobilenet_v2_coco.config --trained_checkpoint_prefix &quot;E:\Tensorflow_detection_model_zoo\ssdlite_mobilenet_v2_coco_2018_05_09\model.ckpt&quot; --output_directory exported_model</span><br></pre></td></tr></table></figure>
<p>注意，截止2020/5/9，<a href="https://github.com/tensorflow/models/issues/8456">官方</a>说目前tensorflow OD API还只支持TF1.x，所以我是在tf1.13.1的环境、1.13版的models repo（因为我没有clone models 1.15的repo…）里执行这个命令行，否则会报错：No module named ‘tensorflow.tools.graph_transforms’，大家如果是1.15版的话也可以试试。</p>
<p>我建议大家都去看一看<strong>export_tflite_ssd_graph.py</strong>里面的源代码，里面有很多很重要的知识点，比如你可以在里面改检测时画面最多显示几个结果，以下是其部分内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Outputs:</span><br><span class="line">If add_postprocessing_op is true: frozen graph adds a</span><br><span class="line">  TFLite_Detection_PostProcess custom op node has four outputs:</span><br><span class="line">  detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box</span><br><span class="line">  locations</span><br><span class="line">  detection_classes: a float32 tensor of shape [1, num_boxes]</span><br><span class="line">  with class indices</span><br><span class="line">  detection_scores: a float32 tensor of shape [1, num_boxes]</span><br><span class="line">  with class scores</span><br><span class="line">  num_boxes: a float32 tensor of size 1 containing the number of detected boxes</span><br><span class="line">else:</span><br><span class="line">  the graph has two outputs:</span><br><span class="line">   &#39;raw_outputs&#x2F;box_encodings&#39;: a float32 tensor of shape [1, num_anchors, 4]</span><br><span class="line">    containing the encoded box predictions.</span><br><span class="line">   &#39;raw_outputs&#x2F;class_predictions&#39;: a float32 tensor of shape</span><br><span class="line">    [1, num_anchors, num_classes] containing the class scores for each anchor</span><br><span class="line">    after applying score conversion.</span><br><span class="line"></span><br><span class="line">Example Usage:</span><br><span class="line">--------------</span><br><span class="line">python object_detection&#x2F;export_tflite_ssd_graph \</span><br><span class="line">    --pipeline_config_path path&#x2F;to&#x2F;ssd_mobilenet.config \</span><br><span class="line">    --trained_checkpoint_prefix path&#x2F;to&#x2F;model.ckpt \</span><br><span class="line">    --output_directory path&#x2F;to&#x2F;exported_model_directory</span><br><span class="line"></span><br><span class="line">Config overrides (see the &#96;config_override&#96; flag) are text protobufs</span><br><span class="line">(also of type pipeline_pb2.TrainEvalPipelineConfig) which are used to override</span><br><span class="line">certain fields in the provided pipeline_config_path.  These are useful for</span><br><span class="line">making small changes to the inference graph that differ from the training or</span><br><span class="line">eval config.</span><br><span class="line"></span><br><span class="line">Example Usage (in which we change the NMS iou_threshold to be 0.5 and</span><br><span class="line">NMS score_threshold to be 0.0):</span><br><span class="line">python object_detection&#x2F;export_tflite_ssd_graph \</span><br><span class="line">    --pipeline_config_path path&#x2F;to&#x2F;ssd_mobilenet.config \</span><br><span class="line">    --trained_checkpoint_prefix path&#x2F;to&#x2F;model.ckpt \</span><br><span class="line">    --output_directory path&#x2F;to&#x2F;exported_model_directory</span><br><span class="line">    --config_override &quot; \</span><br><span class="line">            model&#123; \</span><br><span class="line">            ssd&#123; \</span><br><span class="line">              post_processing &#123; \</span><br><span class="line">                batch_non_max_suppression &#123; \</span><br><span class="line">                        score_threshold: 0.0 \</span><br><span class="line">                        iou_threshold: 0.5 \</span><br><span class="line">                &#125; \</span><br><span class="line">             &#125; \</span><br><span class="line">          &#125; \</span><br><span class="line">       &#125; \</span><br><span class="line">       &quot;</span><br></pre></td></tr></table></figure>
<p>运行完上面的命令行之后，我们在exported_model目录下就有了tflite_graph.pb和tflite_graph.pbtxt。然后我们再把这个tflite_graph.pb的路径替换到前面<em>TFLiteConverter_uint8.py</em>里，run后就能导出<em>uint8_model_converted_from_exported_model.tflite</em>啦，这个就是ssdlite_mobilenet_v2_coco的tflite版，SSDLite的区别就是把SSD里面的传统卷积替换成深度可分离卷积。</p>
<p>紧接着用这个uint8_model_converted_from_exported_model.tflite放入<em>Test_TFLite_Model_With_Image_Folder.py</em>执行一下，执行后我发现，console打印出来的inference time竟然只有0.10~0.13s ！ 而前面3个模型的inference time都是0.4几秒！与它快速的推理速度相反的是，它的文件体积却有18,055 KB ! </p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>在tensorflow models的 object detection下面有两个用来导出pb格式文件的脚本：export_tflite_ssd_graph.py 和 export_inference_graph.py。两者的区别在于：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">export_tflite_ssd_graph</th>
<th style="text-align:center">export_inference_graph</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">导出的文件名</td>
<td style="text-align:center">tflite_graph.pb</td>
<td style="text-align:center">frozen_inference_graph.pb</td>
</tr>
<tr>
<td style="text-align:center">input_arrays</td>
<td style="text-align:center">[“normalized_input_image_tensor”]</td>
<td style="text-align:center">[“image_tensor”]</td>
</tr>
<tr>
<td style="text-align:center">output_arrays</td>
<td style="text-align:center">[‘TFLite_Detection_PostProcess’, ‘TFLite_Detection_PostProcess:1’, ‘TFLite_Detection_PostProcess:2’, ‘TFLite_Detection_PostProcess:3’]</td>
<td style="text-align:center">[“detection_boxes”, “detection_scores”, “detection_classes”, “num_detections”]</td>
</tr>
</tbody>
</table>
</div>
<h1 id="训练日记"><a href="#训练日记" class="headerlink" title="训练日记"></a>训练日记</h1><p><strong>2020/11/30</strong></p>
<p>备份链接：<a href="https://drive.google.com/drive/folders/1S1W7Tq28y-lNcteAHoyNZ6jGx632g39m?usp=sharing">https://drive.google.com/drive/folders/1S1W7Tq28y-lNcteAHoyNZ6jGx632g39m?usp=sharing</a></p>
<p>问题：一个目标周围有多个矩形框</p>
<p>解决方案：减小<a href="https://github.com/tensorflow/models/blob/60bb50675ed7fab3afd05edab02a45acee57532a/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config#L130">batch_non_max_suppression</a> 里的 iou_threshold（从0.6到0.5）【<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/detection_postprocess.cc">TFLite后处理源代码</a>】</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201130221015.png" alt="image-20201130221015315"></p>
<p><strong>2020/12/1</strong></p>
<p>备份链接：<a href="https://drive.google.com/drive/folders/1gFYxDIVEvmB7tmrqZ7ZDFVtfygsq3Zwy?usp=sharing">https://drive.google.com/drive/folders/1gFYxDIVEvmB7tmrqZ7ZDFVtfygsq3Zwy?usp=sharing</a></p>
<p>问题：1.误检多。把树叶、汽车都误检成目标。 2.矩形框回归效果差</p>
<p>解决方案：增加负样本，调整锚框的宽高比。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201201224046.png" alt="image-20201201224046015"></p>
<h2 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h2><blockquote>
<p>我在github上的提问 : <a href="https://github.com/tensorflow/tensorflow/issues/39688">https://github.com/tensorflow/tensorflow/issues/39688</a><br><br>tensorflow 各版本API : <a href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf</a><br><br>TensorFlow Lite Object Detection Android Demo(含detect.tflite)：<a href="https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android">https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android</a></p>
</blockquote>
]]></content>
      <categories>
        <category>CV</category>
      </categories>
      <tags>
        <tag>目标检测</tag>
      </tags>
  </entry>
  <entry>
    <title>图解YUV采样原理</title>
    <url>/2020/04/18/%E5%9B%BE%E8%A7%A3YUV%E9%87%87%E6%A0%B7%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<h2 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h2><p>不论是对于玩摄影摄像的朋友，还是玩相机开发、计算机视觉的工程师来说，YUV都是一门必备的专业常识，下面我就把我从网上收集到的资料汇总起来，保证浅显易懂，言简意赅。</p>
<h2 id="YUV的特性"><a href="#YUV的特性" class="headerlink" title="YUV的特性"></a>YUV的特性</h2><p>YUV相比RGB的优点：</p>
<ol>
<li>亮度信息（Y）和色彩信息（UV）分开，有利于兼容黑白电视.</li>
<li>运用色度采样的原理，减少了文件大小</li>
</ol>
<h2 id="色度采样的原理"><a href="#色度采样的原理" class="headerlink" title="色度采样的原理"></a>色度采样的原理</h2><p>色度采样的原理在youtube上有不少视频讲的很好，我把它们收集到了下面的视频列表里：</p>
<blockquote>
<p><a href="https://www.youtube.com/playlist?list=PL9Al0kOPjCLQ7vc4LADCof6qxt5Fj1kPm">https://www.youtube.com/playlist?list=PL9Al0kOPjCLQ7vc4LADCof6qxt5Fj1kPm</a></p>
</blockquote>
<p>为什么要色度采样？<br>因为<strong>人眼对于亮度的敏感度大于对色度的敏感度</strong>。</p>
]]></content>
  </entry>
  <entry>
    <title>用python做一个excel自动化软件</title>
    <url>/2020/04/12/%E7%94%A8python%E5%81%9A%E4%B8%80%E4%B8%AAexcel%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BD%AF%E4%BB%B6/</url>
    <content><![CDATA[<h2 id="安装工具包"><a href="#安装工具包" class="headerlink" title="安装工具包"></a>安装工具包</h2><p><strong>1.pyinstaller</strong><br><br>按理说你只需要<code>pip install pyinstaller</code>就可以成功安装了，<br>但我遇到了<code>AttributeError: module &#39;enum&#39; has no attribute &#39;IntFlag&#39;</code><br>的错误，解决办法StackOverflow上给出的<a href="https://stackoverflow.com/questions/43124775/why-python-3-6-1-throws-attributeerror-module-enum-has-no-attribute-intflag">解决办法</a><br>是卸载enum：’pip uninstall -y enum34’。完了之后再’pip install pyinstaller’<br>应该就没问题了<br><br><strong>2.openpyxl</strong><br><br><code>pip install openpyxl</code>这个包在我安装anaconda时就已经替我安装好了<br><br><strong>3.PySide2</strong><br><br><code>pip install PySide2</code>经过一番选择，我决定使用PySide2作为我的Python GUI开发工具。安装好后<br>我们去到（C:\ProgramData\Anaconda3）<code>Lib\site-packages\PySide2\designer.exe</code>，<br>把这个designer.exe发送快捷方式到桌面，后面我会用它作为设计软件的初始界面。<br><br><br><strong>4.pyside2-uic</strong><br><br><br>这个东西不用我们安装，我的这个在<code>C:\ProgramData\Anaconda3\Scripts\pyside2-uic.exe</code>，<br>这个东西是用来将designer.exe导出的.ui文件转换成.py文件。</p>
<h3 id="步骤一：建立基本界面"><a href="#步骤一：建立基本界面" class="headerlink" title="步骤一：建立基本界面"></a>步骤一：建立基本界面</h3><p>首先利用designer帮我们做好最基本的界面搭建，<br><img src="https://s1.ax1x.com/2020/04/12/GObOqH.png" alt="GObOqH.png"><br>这个做起来很简单，就是从左边的控件栏直接拖拽控件过来，并调整好大小位置，文字。保存之后就是.ui文件了。</p>
<h3 id="步骤二：转化-ui文件"><a href="#步骤二：转化-ui文件" class="headerlink" title="步骤二：转化.ui文件"></a>步骤二：<a href="https://doc.qt.io/qtforpython/tutorials/basictutorial/uifiles.html">转化.ui文件</a></h3><p>我们有两种方式把.ui转化为.py。<br><br>第一种是在保存的.ui文件夹里运行命令行，<br>键入<code>pyside2-uic xxxxxx.ui &gt; xxxxxx.py</code>就可以在相同文件夹下生成一个xxxxxx.py了。<br>不过这还没完，生成的.py脚本里面只包含一个class类，你在这里Run这个脚本，是不会出现窗口的，我们还需要新建一个python<br>脚本，基本内容是<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line">from PySide2.QtWidgets import QApplication, QMainWindow</span><br><span class="line">from PySide2.QtCore import QFile</span><br><span class="line">from ui_mainwindow import Ui_MainWindow  # ui_mainwindow替换成刚刚生成的xxxxxx.py名称。</span><br><span class="line"></span><br><span class="line">class MainWindow(QMainWindow):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MainWindow, self).__init__()</span><br><span class="line">        self.ui &#x3D; Ui_MainWindow()</span><br><span class="line">        self.ui.setupUi(self)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    app &#x3D; QApplication(sys.argv)</span><br><span class="line">    window &#x3D; MainWindow()</span><br><span class="line">    window.show()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure><br>又或者你可以直接把上面这段代码加在生成的xxxxxx.py里面，不过你要注释掉这句 <code>from ui_mainwindow import Ui_MainWindow</code><br><br>第二种是动态加载，它的特点是比较方便，因为改动界面后，不需要转化，直接运行。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># File: main.py</span><br><span class="line">import sys</span><br><span class="line">from PySide2.QtUiTools import QUiLoader</span><br><span class="line">from PySide2.QtWidgets import QApplication</span><br><span class="line">from PySide2.QtCore import QFile</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    app &#x3D; QApplication(sys.argv)</span><br><span class="line"></span><br><span class="line">    ui_file &#x3D; QFile(&quot;xxxxxx.ui&quot;)</span><br><span class="line">    ui_file.open(QFile.ReadOnly)</span><br><span class="line"></span><br><span class="line">    loader &#x3D; QUiLoader()</span><br><span class="line">    window &#x3D; loader.load(ui_file)</span><br><span class="line">    ui_file.close()</span><br><span class="line">    window.show()</span><br><span class="line"></span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure></p>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><p>BUG1:<br><img src="https://s1.ax1x.com/2020/04/13/Gv2FJg.png" alt="Gv2FJg.png"><br>DEBUG:来到<code>C:\ProgramData\Anaconda3\Lib\site-packages\PySide2\__init__.py</code>,在<code>def _setupQtDirectories():</code><br>下面添加这段代码<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import sys, os</span><br><span class="line">import PySide2</span><br><span class="line">dirname &#x3D; os.path.dirname(PySide2.__file__)</span><br><span class="line">plugin_path &#x3D; os.path.join(dirname, &#39;plugins&#39;, &#39;platforms&#39;)</span><br><span class="line">os.environ[&#39;QT_QPA_PLATFORM_PLUGIN_PATH&#39;] &#x3D; plugin_path&#96;</span><br></pre></td></tr></table></figure><br>然后CTRL+S保存关闭退出。再次运行就没有问题了。</p>
<p>…..中间的开发记录懒得更新了……</p>
<p>直接看成品：</p>
<iframe src="//player.bilibili.com/player.html?aid=328807447&bvid=BV1rA411e79G&cid=213219091&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<h3 id="参考链接-1"><a href="#参考链接-1" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h3><p>1.<a href="https://doc.qt.io/qtforpython/tutorials/">官方教程</a><br>2.<a href="https://www.youtube.com/watch?v=ksW59gYEl6Q">Python GUI’s with PyQt5</a></p>
<p>3.<a href="https://www.cnblogs.com/focksor/p/13168317.html">Pycharm+PyQt5环境搭建</a></p>
]]></content>
  </entry>
</search>
