<!DOCTYPE html>


<html lang="ch">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     Hello World
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/planets.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-172408389-1', 'auto');
ga('send', 'pageview');

</script>



  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?da92a6672e51fa2d1c3bacf2dba555c6";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

</html>

<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/earth-1388003_1920.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Hello World</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['面朝大海，春暖花开', '与时间赛跑', '人类发展的终极目标是让人类成为多星球物种'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-RNN、LSTM、Transformer"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/01/30/RNN%E3%80%81LSTM%E3%80%81Transformer/"
    >RNN、LSTM、Transformer</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2021/01/30/RNN%E3%80%81LSTM%E3%80%81Transformer/" class="article-date">
  <time datetime="2021-01-30T07:00:25.000Z" itemprop="datePublished">2021-01-30</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>最近频繁被Tansformer刷屏，于是我决定学习一下Transformer，看看它的厉害之处在哪里。</p>
<p>本篇博客就像是一个链接中枢，我会放很多我认为不错的教程链接，因为到现在，网上已经有不少好教程了，我不会再摘抄一遍它们的内容，我只会放上链接，加上一点个人的理解补充。</p>
<p>Shusen Wang做了一个Transformer的youtube系列视频：《<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=aButdUV0dxI&amp;list=PLvOO0btloRntpSWSxFbwPIjIum3Ub4GSC">Transformer模型</a>》，但是在学习这个系列视频之前，还需要对RNN、LSTM、Attention这些专业术语有所了解，这就需要再去看一下他的另一个YouTube系列视频：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=NWcShtqr8kc&amp;list=PLvOO0btloRnuTUGN4XqO85eKPeFSZsEqK">《RNN模型与NLP应用》</a>（建议1.5倍速观看哈）。</p>
<p>本博客将以这两个系列视频为主线展开，对其中的一些知识点进行补充说明。</p>
<ul>
<li><p>《RNN模型与NLP应用(1/9)：数据处理基础》：这一节比较简单，无需补充说明</p>
</li>
<li><p>《RNN模型与NLP应用(2/9)：文本处理与词嵌入》：</p>
<p>本节首次提到了embedding这个重要概念，但介绍的不多，更多embedding的资料可以参考这篇<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42078618/article/details/84553940">博客</a>以及网上的其他一些资料。</p>
<p>视频中有一页ppt是下图这样的，我这里注释了x1和x2，这样从左边的Parameter matrix到右边的坐标位置就很简单易懂了。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2021/01/30/d6jkSXZOEMvplri.png" alt="image-20210130172025692" style="zoom: 67%;" /></p>
<ul>
<li><p>《RNN模型与NLP应用(3/9)：Simple RNN模型》：</p>
<p>这一节跟下一节视频用到的一些插图来自这篇英语博客<a target="_blank" rel="noopener" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">《Understanding LSTM Networks》</a>，这篇博客强烈推荐阅读，英语不太好的可以搭配看这篇翻译版<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/95d5c461924c">博客</a>。插图中C和h的含义，C代表Cell state （也有人说是Conveyor）， h代表Hidden state。</p>
<p>由下图圆圈可知，上一个RNN模块传给下一个RNN模块的东西就是hidden state，同一个东西拷贝了两份而已：</p>
<p><img src="https://i.loli.net/2021/01/30/SfY6Ja4EPqpOTu1.png" alt="image-20210130231449052" style="zoom:50%;" /></p>
<p>而下面要介绍的LSTM每个模块输出的不仅有hidden state，还有cell state。</p>
</li>
<li><p>《RNN模型与NLP应用(4/9)：LSTM模型》：</p>
<p>补充一些个人对这篇英语博客的理解：sigmoid函数输出（0，1）中间的数值，适合用来决定矩阵元素被保留多少，而tanh函数输出范围是（-1，1），适合用来决定更新量是多少，因为更新肯定有增有减，所以用tanh。</p>
</li>
<li><p>《RNN模型与NLP应用(5/9)：多层RNN、双向RNN、预训练》：</p>
</li>
</ul>
<p>这一节主要讲了通过RNN的变体和预训练来提高模型的推理准确率，视频中8:27提到embedding的参数很大，而训练数据很少，容易导致过拟合，这一点我深有感受，因为我之前用kinetics400的模型训练我的4个动作类别，就感受到了过拟合。</p>
<ul>
<li>《RNN模型与NLP应用(6/9)：Text Generation (自动文本生成)》：</li>
</ul>
<p>这一节讲得很通俗易懂，无需补充说明</p>
<ul>
<li>《RNN模型与NLP应用(7/9)：机器翻译与Seq2Seq模型》：</li>
</ul>
<p>Seq2Seq这个模型的原理一个视频的讲解量应该不够，大家可以再看一下B站的这个<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Eb411J7Qm?p=1">视频</a>作为补充，同样建议1.5倍速播放。</p>
<ul>
<li>《RNN模型与NLP应用(8/9)：Attention (注意力机制)》：</li>
</ul>
<p>不是说LSTM就是用来解决记忆久远信息的吗，怎么Seq2Seq还会有记不住太长信息的问题？我的理解是因为不管Encoder输入的词量是多少，Encoder和Decoder中间的那个语义向量Thought Vector的长度总是固定不变的，如果Encoder输入的词量非常多，那么Thought   Vector就容不下那么多语义信息。</p>
<p><img src="https://camo.githubusercontent.com/e63bf968fbd0095c5474d241434a72b4a59df473860857a3871039a73eab2bae/68747470733a2f2f73757269796164656570616e2e6769746875622e696f2f696d672f736571327365712f73657132736571322e706e67" alt="img"></p>
<p>借用下图说明一下这一点，图中800×800和3000×3000代表输入Encoder的不同长度的词量，100KB就是Thought Vector的长度，可见过于密集的输入信息最后编码成Thought Vector必然会变得稀疏：</p>
<p><img src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2019-10-28-yasuo.png" alt="Encoder-Decoder的缺点：输入过长会损失信息" style="zoom:50%;" /></p>
<p>而Attention会考虑到每一个输入词向量产生的$h_i$，有多少个输入词向量，后面Decoder更新每个$s_i$时就考虑多少个Encoder的$h_i$。</p>
<ul>
<li><p>《RNN模型与NLP应用(9/9)：Self-Attention (自注意力机制)》：</p>
<p>听过前面的视频，这个视频的内容不难理解。</p>
</li>
</ul>
<p>———————————————————————分割线————————————————————————-</p>
<p>了解完RNN和LSTM，现在你应该对NLP领域的一些名词有些概念了，开始学习Transformer。</p>
<p>网上有很多讲解Transoformer的博客，但很多博客的内容和图片都来自这篇英文博客《<a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>》，这位<a target="_blank" rel="noopener" href="http://jalammar.github.io/">Jay Alammar</a>的博客主页还有很多利用可视化图片视频讲解机器学习原理的博客，推荐各位去看一看！</p>
<p>在看完Shusen Wan的<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=aButdUV0dxI&amp;list=PLvOO0btloRntpSWSxFbwPIjIum3Ub4GSC">Transformer模型</a>系列视频，我还是很难完全理解positional encoding的原理，在youtube搜索positional encoding时，我遇到了另一个宝藏博主 - <a target="_blank" rel="noopener" href="https://www.youtube.com/channel/UCmUi2gnk9EbJQjvWo2VdZCA">Hedu - Mathematics of Intelligence</a>，目前她推出的<a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PL86uXYUJ7999zE8u2-97i4KG_2Zpufkfb">Visual Guide to Transformer Neural Networks</a>系列视频包含3部分：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=dichIcUZfOw&amp;t=611s">Visual Guide to Transformer Neural Networks - (Part 1) Position Embeddings</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=mMa2PmYJlCo">Visual Guide to Transformer Neural Networks - (Part 2) Multi-Head &amp; Self-Attention</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=gJ9kaJsE78k&amp;t=2s">Visual Guide to Transformer Neural Networks - (Part 3) Decoder’s Masked Attention</a></p>
</li>
</ul>
<p>其中(Part 1) Position Embeddings就很生动易懂地解释了位置编码地原理（有一定基础的人可以从6:04分开始观看），而她地(Part 2) Multi-Head &amp; Self-Attention视频则讲清楚了为什么两个矩阵相乘可以用来求相似度（不像其他博客都想当然），总之，你可以先去看看其他人的Transformer博客，再去看Hedu - Mathematics of Intelligence的视频，你就知道她的视频有多好了！</p>
<p>拓展链接：</p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4Bdc55j80l8">《Illustrated Guide to Transformers Neural Network: A step by step explanation》</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=TrdevFK_am4">《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Paper Explained)》</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=TQQlZhbC5ps">《Transformer Neural Networks - EXPLAINED! (Attention is all you need)》</a></p>
<h2 id="Transformer-在CV领域的应用"><a href="#Transformer-在CV领域的应用" class="headerlink" title="Transformer 在CV领域的应用"></a>Transformer 在CV领域的应用</h2><p>facebook 提出了用transformer执行目标检测的<strong>DETR</strong> (<strong>DE</strong>tection <strong>TR</strong>ansformer)</p>
<p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/detr">https://github.com/facebookresearch/detr</a></p>
<p>我在跑colab notebook时，发现<code>detr_demo.ipynb</code>的效果不是很好，如下图所示：</p>
<p><img src="https://user-images.githubusercontent.com/43233772/110281620-0a946580-8018-11eb-91d4-4b123419e9df.png" alt="image"></p>
<p>而 <code>detr_attention.ipynb</code> 的效果则正常得多了：</p>
<p><img src="https://user-images.githubusercontent.com/43233772/110487672-c3e26080-8128-11eb-96e1-7090395ee6ba.png" alt="下载"></p>
<p>测试原图在<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1X8fC8ZtlH_S7WQVE6jSeXL7jr4oQPDYD/view?usp=sharing">这里</a>。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LSTM/" rel="tag">LSTM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RNN/" rel="tag">RNN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-DeepStream"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/12/20/DeepStream/"
    >DeepStream</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/12/20/DeepStream/" class="article-date">
  <time datetime="2020-12-20T02:34:05.000Z" itemprop="datePublished">2020-12-20</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>门户网站：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/deepstream-sdk">https://developer.nvidia.com/deepstream-sdk</a></p>
<h1 id="DeepStream-入门"><a href="#DeepStream-入门" class="headerlink" title="DeepStream 入门"></a><strong>DeepStream</strong> 入门</h1><h2 id="欢迎使用DeepStream文档"><a href="#欢迎使用DeepStream文档" class="headerlink" title="欢迎使用DeepStream文档"></a>欢迎使用DeepStream文档</h2><h3 id="NVIDIA-DeepStream概述"><a href="#NVIDIA-DeepStream概述" class="headerlink" title="NVIDIA DeepStream概述"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#nvidia-deepstream-overview">NVIDIA DeepStream概述</a></h3><p>DeepStream是一个流分析工具包，用于构建AI驱动的应用程序。它以流数据作为输入, 这些流数据来自USB / CSI摄像机，来自视频文件或基于RTSP的流，并使用AI和计算机视觉从像素生成洞察力，以更好地了解环境。DeepStream SDK可以用作许多视频分析解决方案的基础层，例如了解智慧城市中的交通和行人，医院中的健康和安全监控，零售中的自检和分析，检测制造工厂中的组件缺陷等。</p>
<p><img src="https://i.loli.net/2020/12/20/NfboknCLUWxQ1mh.jpg" alt="img"></p>
<p>DeepStream通过Python bindings支持C / C ++和Python的应用程序开发。为了使入门更加容易，DeepStream附带了C / C ++和Python中的多个参考应用程序。请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html">C / C ++示例应用程序详细信息</a>和<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html">Python示例应用程序详细信息</a>，以了解有关可用应用程序的更多信息。有关某些DeepStream参考应用程序，请参见<a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/deepstream_reference_apps">NVIDIA-AI-IOT</a> Github页面。</p>
<p>核心SDK由几个硬件加速器插件组成，这些插件使用各种加速器，例如VIC，GPU，DLA，NVDEC和NVENC。通过在专用加速器中执行所有计算繁重的操作，DeepStream可以为视频分析应用程序实现最高性能。DeepStream的关键功能之一是边缘和云之间的安全双向通信。DeepStream附带了几种现成的安全协议，例如使用用户名/密码的SASL /普通身份验证和2路TLS身份验证。要了解有关这些安全功能的更多信息，请阅读 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_IoT.html"><strong>IoT</strong></a> 章节。要了解有关双向功能的更多信息，请参阅本指南中的“<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_IoT.html#bi-directional-label">双向消息传递</a>”部分。</p>
<p>DeepStream建立在CUDA-X堆栈的多个NVIDIA库的基础上（如上图所示），例如有CUDA，TensorRT，Triton Inference服务器和多媒体库。TensorRT加速了NVIDIA GPU上的AI推理。DeepStream在插件中抽象了这些库，使开发人员可以轻松地构建视频分析管道，而不必学习所有单独的库。</p>
<p>DeepStream针对NVIDIA GPU进行了优化，应用程序可以部署在运行Jetson平台的嵌入式边缘设备上，也可以部署在较大的边缘或数据中心GPU（例如T4）上。可以使用NVIDIA容器运行时将DeepStream应用程序部署在容器中。这些容器可在NGC（NVIDIA GPU Cloud Registry）上找到。要了解有关使用docker进行部署的更多信息，请参阅Docker容器一章。可以使用GPU上的Kubernetes在边缘上编排（orchestrate）DeepStream应用程序。NGC上提供了用于部署DeepStream应用程序的示例<a target="_blank" rel="noopener" href="https://ngc.nvidia.com/catalog/helm-charts/nvidia:video-analytics-demo">Helm chart</a>。</p>
<h3 id="DeepStream图架构"><a href="#DeepStream图架构" class="headerlink" title="DeepStream图架构"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#deepstream-graph-architecture">DeepStream图架构</a></h3><p>DeepStream是使用开源GStreamer框架构建的优化图形架构。下图显示了典型的视频分析应用程序，从输入视频到输出结果，所有单独的块都是使用到的各种插件。底部是在整个应用程序中使用的不同硬件引擎。插件之间的零内存复制以及使用各种加速器的最佳内存管理确保了最高性能。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/93942a719fb53d5de777aee577f2c268.png" alt=""></p>
<p>DeepStream以GStreamer插件的形式提供了构建基块，可用于构建有效的视频分析管道。有15个以上的插件可以通过硬件加速完成各种任务。</p>
<ol>
<li><p>数据流可以通过RTSP或来自本地文件系统或直接来自摄像机的网络来传输。使用CPU捕获流。一旦帧进入内存，就使用NVDEC加速器发送它们以进行解码。用于解码的插件称为<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvvideo4linux2.html">Gst-nvvideo4linux2</a>。</p>
</li>
<li><p>解码后，有一个可选的图像预处理步骤。预处理可以是图像变形或色彩空间转换。<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvdewarper.html">Gst-nvdewarper</a>插件可以使鱼眼镜头或360度相机的图像变形。<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvvideoconvert.html">Gst-nvvideoconvert</a>插件可以在框架上执行颜色格式转换。这些插件使用GPU或VIC（视觉图像合成器）。</p>
</li>
<li><p>下一步是批处理帧以获得最佳推理性能。批处理使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvstreammux.html">Gst-nvstreammux</a>插件完成。</p>
</li>
<li><p>批处理帧后，将其发送以进行推理。可以使用NVIDIA的推理加速器运行时TensorRT进行推理，也可以使用Triton推理服务器在本机框架（如TensorFlow或PyTorch）中进行推理。本地TensorRT推理是使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinfer.html">Gst-nvinfer</a>插件实现，用Triton推理是通过<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvinferserver.html">Gst-nvinferserver</a>插件实现。对于Jetson AGX Xavier和Xavier NX，推理可以使用GPU或DLA（深度学习加速器）。</p>
</li>
<li><p>推断之后，下一步可能涉及跟踪对象。SDK中有多个内置参考跟踪器，范围从高性能到高精度。使用<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvtracker.html">Gst-nvtracker</a>插件执行对象跟踪。</p>
</li>
<li><p>为了创建可视化工件，例如边界框，分割蒙版，标签，有一个名为<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvdsosd.html">Gst-nvdsosd</a>的可视化插件。</p>
</li>
<li><p>最后，要输出结果，DeepStream提供了各种选项：在屏幕上用边框显示输出，将输出保存到本地磁盘，通过RTSP进行流传输或仅将元数据发送到云。为了将元数据发送到云，DeepStream使用Gst-nvmsgconv和Gst-nvmsgbroker插件。<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvmsgconv.html">Gst-nvmsgconv</a>将元数据转换为架构有效负载，而<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_plugin_gst-nvmsgbroker.html">Gst-nvmsgbroker</a>建立与云的连接并发送遥测数据。有几种内置的代理协议，例如Kafka，MQTT，AMQP和Azure IoT。你可以创建自定义代理适配器（broker adapters）。</p>
</li>
</ol>
<h3 id="DeepStream参考应用"><a href="#DeepStream参考应用" class="headerlink" title="DeepStream参考应用"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#deepstream-reference-app">DeepStream参考应用</a></h3><p>为了上手deepstream，开发人员可以使用我们提供的参考应用程序，这些应用程序包括了它们的源代码。我么这里称呼端到端应用程序称为deepstream-app。该应用程序是完全可配置的-它允许用户配置任何类型和数量的源，配置运行推理的神经网络类型，它预先内置了一个推理插件来进行目标检测，还有一个配置目标跟踪器的选项。对于输出，用户可以选择在屏幕上渲染，保存输出文件或通过RTSP传输视频。</p>
<p><img src="https://img-blog.csdnimg.cn/20201220103254473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NjQ5MDcy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这是开始学习DeepStream功能的很好的参考应用程序。<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_ref_app_deepstream.html">DeepStream参考应用程序-deepstream-app</a>一章将更详细地介绍此应用程序。该应用程序的源代码位于<code>/opt/nvidia/deepstream/deepstream-5.0/sources/apps/sample_apps/deepstream-app</code>。该应用程序适用于所有AI模型，并在各个README文件中提供详细说明。性能基准测试也使用此应用程序运行。</p>
<h3 id="建立应用程序入门"><a href="#建立应用程序入门" class="headerlink" title="建立应用程序入门"></a>建立应用程序入门</h3><p>对于希望构建自定义应用程序的开发人员而言，刚开始开发deepstream-app可能会有些不知所措。SDK附带了几个简单的应用程序，开发人员可以在其中学习DeepStream的基本概念，构造一个简单的管道，然后逐步构建更复杂的应用程序。</p>
<p><img src="https://i.loli.net/2020/12/20/y46GqoNKIYJ2TDt.png" alt="img"></p>
<p>开发人员可以从deepstream-test1开始，它几乎像DeepStream的hello world。在此应用程序中，开发人员将学习如何使用各种DeepStream插件构建GStreamer管道。他们将从文件中获取视频，进行解码，批处理，然后进行对象检测，最后在屏幕上呈现这些框。deepstream-test2在test1的目标检测后面增加了目标分类。deepstream-test3展示了如何添加多个视频源，最后test4将演示如何使用消息代理插件为IoT服务。这4个入门应用程序在本机C / C ++和Python中均可用。要了解有关DeepStream中这些应用程序和其他示例应用程序的更多信息，请参见<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html">C / C ++示例应用程序源详细信息</a>和<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html">Python示例应用程序源详细信息</a>.</p>
<h3 id="用Python的DeepStream"><a href="#用Python的DeepStream" class="headerlink" title="用Python的DeepStream"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Overview.html#deepstream-in-python">用Python的DeepStream</a></h3><p>NVIDIA引入了GStreamer框架的Python绑定 - Gst-Python，以帮助您使用Python构建高性能的AI应用程序。</p>
<p><img src="https://i.loli.net/2020/12/20/oNgUtAyYQ4r8sPM.png" alt="img"></p>
<p>DeepStream Python应用程序使用Gst-Python API构造管道，并使用probe函数访问管道中各个点的数据。数据类型全部是原生C语言（上图灰色部分），并且需要通过PyBindings或NumPy层才能从Python应用程序访问它们。张量数据是推断后得出的原始张量输出。如果要检测对象，则需要通过解析和聚类算法对该张量数据进行后处理，以在检测到的对象周围创建边界框。要开始使用Python，请参阅本指南中的<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html">Python示例应用程序源详细信息</a>以及DeepStream Python API指南中的“ DeepStream Python”。</p>
<h2 id="快速入门指南"><a href="#快速入门指南" class="headerlink" title="快速入门指南"></a>快速入门指南</h2><h3 id="Jetson准备"><a href="#Jetson准备" class="headerlink" title="Jetson准备"></a>Jetson准备</h3><h4 id="安装Jetson-SDK组件"><a href="#安装Jetson-SDK组件" class="headerlink" title="安装Jetson SDK组件"></a>安装Jetson SDK组件</h4><p>从以下位置下载NVIDIA SDK Manager，您将用它来安装JetPack 4.4 GA（对应于L4T 32.4.3版本）：</p>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/embedded/jetpack">https://developer.nvidia.com/embedded/jetpack</a></p>
<ul>
<li>NVIDIA SDK Manager是一个图形界面应用程序，可刷新并安装JetPack软件包。</li>
<li>根据主机系统的不同，刷新过程大约需要10-30分钟。</li>
<li>如果您使用的是Jetson Nano或Jetson Xavier NX开发者套件，则可以从<a target="_blank" rel="noopener" href="https://developer.nvidia.com/embedded/jetpack下载SD卡镜像，它已经随附了CUDA，TensorRT和cuDNN。">https://developer.nvidia.com/embedded/jetpack下载SD卡镜像，它已经随附了CUDA，TensorRT和cuDNN。</a></li>
</ul>
<h4 id="安装依赖项"><a href="#安装依赖项" class="headerlink" title="安装依赖项"></a>安装依赖项</h4><p>输入以下命令以安装必备软件包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install \</span><br><span class="line">libssl1.0.0 \</span><br><span class="line">libgstreamer1.0-0 \</span><br><span class="line">gstreamer1.0-tools \</span><br><span class="line">gstreamer1.0-plugins-good \</span><br><span class="line">gstreamer1.0-plugins-bad \</span><br><span class="line">gstreamer1.0-plugins-ugly \</span><br><span class="line">gstreamer1.0-libav \</span><br><span class="line">libgstrtspserver-1.0-0 \</span><br><span class="line">libjansson4&#x3D;2.11-1</span><br></pre></td></tr></table></figure>
<h4 id="安装librdkafka（为消息代理启用Kafka协议适配器）"><a href="#安装librdkafka（为消息代理启用Kafka协议适配器）" class="headerlink" title="安装librdkafka（为消息代理启用Kafka协议适配器）"></a>安装librdkafka（为消息代理启用Kafka协议适配器）</h4><ol>
<li>从GitHub克隆librdkafka存储库：</li>
</ol>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;edenhill&#x2F;librdkafka.git</span><br></pre></td></tr></table></figure>
</blockquote>
<ol>
<li><p>配置和构建库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd librdkafka</span><br><span class="line">$ git reset --hard 7101c2310341ab3f4675fc565f64f0967e135a6a</span><br><span class="line">.&#x2F;configure</span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure>
</li>
<li><p>将生成的库复制到deepstream目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream-5.0&#x2F;lib</span><br><span class="line">$ sudo cp &#x2F;usr&#x2F;local&#x2F;lib&#x2F;librdkafka* &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream-5.0&#x2F;lib</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="安装NVIDIA-V4L2-GStreamer插件"><a href="#安装NVIDIA-V4L2-GStreamer插件" class="headerlink" title="安装NVIDIA V4L2 GStreamer插件"></a>安装NVIDIA V4L2 GStreamer插件</h4><ol>
<li><p>在文本编辑器中打开apt源配置文件，例如： <code>$ sudo vi /etc/apt/sources.list.d/nvidia-l4t-apt-source.list</code></p>
</li>
<li><p>在如下所示的deb命令中更改存储库名称并下载URL：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">deb https:&#x2F;&#x2F;repo.download.nvidia.com&#x2F;jetson&#x2F;common r32.4 main</span><br><span class="line">deb https:&#x2F;&#x2F;repo.download.nvidia.com&#x2F;jetson&#x2F;&lt;platform&gt; r32.4 main</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li><p>其中<platform>标识平台的处理器：</p>
<p><code>t186</code> 用于Jetson TX2系列，<code>t194</code> 适用于Jetson AGX Xavier系列或Jetson Xavier NX，<code>t210</code> 用于Jetson Nano或Jetson TX1</p>
</li>
</ul>
<p>例如，如果您的平台是Jetson Xavier NX：</p>
<ul>
<li>deb <a target="_blank" rel="noopener" href="https://repo.download.nvidia.com/jetson/common">https://repo.download.nvidia.com/jetson/common</a> r32.4 main</li>
<li>deb <a target="_blank" rel="noopener" href="https://repo.download.nvidia.com/jetson/t194">https://repo.download.nvidia.com/jetson/t194</a> r32.4 main</li>
</ul>
</blockquote>
<ol>
<li><p>保存并关闭源配置文件。</p>
</li>
<li><p>输入命令：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install --reinstall nvidia-l4t-gstreamer</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果apt提示您选择配置文件，请选择<code>Y</code>（YES）（以使用NVIDIA更新版本的文件）。</p>
<p>注意：从SDK Manager刷新Jetson OS后，应更新NVIDIA V4L2 GStreamer插件。</p>
</blockquote>
<h4 id="安装DeepStream-SDK"><a href="#安装DeepStream-SDK" class="headerlink" title="安装DeepStream SDK"></a>安装DeepStream SDK</h4><ul>
<li><p><strong>方法1</strong>：使用SDK Manager</p>
<p>从<code>Additional SDKs</code>中选择DeepStreamSDK以及 JP 4.4 软件组件进行安装。</p>
</li>
<li><p><strong>方法2</strong>：使用DeepStream tar包</p>
<ol>
<li><p>将DeepStream 5.0 Jetson tar包<code>deepstream_sdk_v5.0.1_jetson.tbz2</code>下载到Jetson设备。</p>
</li>
<li><p>输入以下命令以提取并安装DeepStream SDK：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tar -xvf deepstream_sdk_v5.0.1_jetson.tbz2 -C &#x2F;</span><br><span class="line">$ cd &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream-5.0</span><br><span class="line">$ sudo .&#x2F;install.sh</span><br><span class="line">$ sudo ldconfig</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>方法3</strong>：使用DeepStream Debian软件包</p>
<p>将DeepStream 5.0 Jetson Debian软件包<code>deepstream-5.0_5.0.1-1_arm64.deb</code>下载到Jetson设备。然后输入命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install .&#x2F;deepstream-5.0_5.0.1-1_arm64.deb</span><br></pre></td></tr></table></figure>
<p>注意：如果使用<code>dpkg</code>命令安装DeepStream SDK Debian软件包，则必须在安装DeepStream deb软件包之前安装以下软件包：</p>
<blockquote>
<ul>
<li><code>libgstrtspserver-1.0-0</code></li>
<li><code>libgstreamer-plugins-base1.0-dev</code></li>
</ul>
</blockquote>
</li>
<li><p><strong>方法4</strong>：使用apt服务器</p>
<ol>
<li><p>使用类似于以下命令的命令在文本编辑器中打开apt源配置文件。</p>
<p><code>$ sudo vi /etc/apt/sources.list.d/nvidia-l4t-apt-source.list</code></p>
</li>
<li><p>在如下所示的deb命令中更改存储库名称并下载URL：</p>
<p> <code>deb https://repo.download.nvidia.com/jetson/common r32.4 main</code></p>
</li>
<li><p>保存并关闭源配置文件。</p>
<ol>
<li><p>输入命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt update</span><br><span class="line">$ sudo apt install deepstream-5.0</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
</li>
<li><p><strong>方法5</strong>：使用NGC上的DeepStream Docker容器。请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_docker_containers.html">Docker容器</a>部分以了解有关使用Docker容器开发和部署DeepStream的信息。</p>
</li>
</ul>
<h4 id="运行deepstream-app（参考应用程序）"><a href="#运行deepstream-app（参考应用程序）" class="headerlink" title="运行deepstream-app（参考应用程序）"></a>运行deepstream-app（参考应用程序）</h4><ol>
<li>导航到<code>samples</code>目录</li>
<li>输入以下命令以运行参考应用程序：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ deepstream-app -c &lt;path_to_config_file&gt;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>注意：</p>
<ul>
<li><path_to_config_file>是参考应用程序配置文件之一的路径名，您可以在<code>/opt/nvidia/deepstream/deepstream-5.0/samples/configs/deepstream-app/</code>目录下找到示例配置文件。 输入此命令<code>$ deepstream-app --help</code>以查看应用程序用法。</li>
<li>要保存TensorRT Engine / Plan文件，请运行以下命令：<br><code>$ sudo deepstream-app -c &lt;路径配置文件&gt;</code></li>
</ul>
<p>​       ???这不和上面一样吗，到底这个命令能干啥？暂时不知道</p>
<ul>
<li>要在2D平铺显示视图中显示标签，请在源上单击鼠标左键以展开感兴趣的源。 要返回平铺显示，请在窗口中的任意位置单击鼠标右键。</li>
<li>也支持键盘来选择源。 在运行应用程序的控制台上，按<code>z</code>键，然后按所需的行索引（0到9），然后按列索引（0到9）以展开源。 要恢复2D平铺显示视图，请再次按<code>z</code>。</li>
</ul>
<h4 id="提高时钟"><a href="#提高时钟" class="headerlink" title="提高时钟"></a>提高时钟</h4><p>安装DeepStream SDK之后，请在Jetson设备上运行以下命令以提高时钟：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nvpmodel -m 0</span><br><span class="line">$ sudo jetson_clocks</span><br></pre></td></tr></table></figure>
<h4 id="运行预编译的示例应用程序"><a href="#运行预编译的示例应用程序" class="headerlink" title="运行预编译的示例应用程序"></a>运行预编译的示例应用程序</h4><ol>
<li><p>导航到<code>sources/apps/sample_apps</code>选定的应用程序目录中。</p>
</li>
<li><p>按照目录的README文件运行该应用程序。</p>
<blockquote>
<p>注意:</p>
<p>如果应用程序遇到错误且无法创建Gst元素，请删除GStreamer缓存，然后重试。要删除GStreamer缓存，请输入以下命令：</p>
<p> <code>$ rm $&#123;HOME&#125;/.cache/gstreamer-1.0/registry.aarch64.bin</code></p>
<p>当运行应用程序用到的模型没有现有的引擎文件时，可能要花费几分钟的时间，具体取决于要生成引擎文件的平台和模型。为了方便以后运行，可以再次使用这些生成的引擎文件以加快加载速度。</p>
</blockquote>
</li>
</ol>
<h3 id="适用于Ubuntu的dGPU设置"><a href="#适用于Ubuntu的dGPU设置" class="headerlink" title="适用于Ubuntu的dGPU设置"></a>适用于Ubuntu的dGPU设置</h3><p>本节说明在安装DeepStream SDK之前如何准备使用了NVIDIA dGPU设备的<code>Ubuntu x86_64</code>系统。</p>
<blockquote>
<p>注意 :</p>
<p>本文档使用术语dGPU（“discrete GPU”）来指代NVIDIA GPU扩展卡产品，例如NVIDIA Tesla®T4和P4，NVIDIA GeForce® GTX 1080和NVIDIA GeForce® RTX2080。此版本的DeepStream SDK运行在x86_64平台上的特定的dGPU产品。支持NVIDIA驱动程序450.51和NVIDIA TensorRT™7.0及更高版本。</p>
</blockquote>
<p>您必须安装以下组件：Ubuntu 18.04，GStreamer 1.14.1，NVIDIA驱动程序450.51，CUDA 10.2，TensorRT 7.0.X。</p>
<p>以下内容我就不翻译了，我改成用自己的话来写。<br>我的环境是Ubuntu18.04, tensorrt 7.2.1.6, cuda 10.2.89。<br><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html">official docs</a>里有详细的安装步骤教程，分<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html">jetson版</a>和<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html">dGPU版</a>，我最初是想在ubuntu上先安装deepstream，走完一个流程看看，所以我选择的是dGPU的安装教程。文档上提供了4种安装方式，我选择的是第2种方式，用tar包，第1种用deb包的方式我这边有报错。注意点有：</p>
<ol>
<li>确保安装路径是：/opt/nvidia/deepstream/deepstream-5.0，<code>sudo tar -xvf deepstream_sdk_v5.0.1_jetson.tbz2 -C /</code>这句命令最后的<code>-C /</code>是关键。</li>
<li>执行 sudo ldconfig时，我遇到了下面的警告：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(base) weidawang@weidawang-TUF-Gaming-FX506LU-FX506LU:/opt/nvidia/deepstream/deepstream-<span class="number">5.0</span>$ sudo ldconfig</span><br><span class="line">[sudo] weidawang 的密码： </span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_cnn_train.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_ops_train.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_adv_train.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_adv_infer.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_ops_infer.so<span class="number">.8</span> 不是符号链接</span><br><span class="line">/sbin/ldconfig.real: /usr/local/cuda-<span class="number">10.2</span>/targets/x86_64-linux/lib/libcudnn_cnn_infer.so<span class="number">.8</span> 不是符号链接</span><br></pre></td></tr></table></figure>
<p>按<a target="_blank" rel="noopener" href="https://blog.csdn.net/langb2014/article/details/54376716/">这篇博客</a>的思路，我这边依次执行下面的命令就好了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8</span><br><span class="line">sudo ln -sf /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.0.4 /usr/<span class="built_in">local</span>/cuda-10.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8</span><br></pre></td></tr></table></figure>
<ol>
<li>安装好后我试着用<code>deepstream-app --version-all</code>验证是否安装成功，它的输出是：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(gst-plugin-scanner:6378): GStreamer-WARNING **: 08:44:44.000: Failed to load plugin <span class="string">&#x27;/usr/lib/x86_64-linux-gnu/gstreamer-1.0/deepstream/libnvdsgst_inferserver.so&#x27;</span>: libtrtserver.so: cannot open shared object file: No such file or directory</span><br><span class="line">deepstream-app version 5.0.0</span><br><span class="line">DeepStreamSDK 5.0.0</span><br><span class="line">CUDA Driver Version: 11.1</span><br><span class="line">CUDA Runtime Version: 10.2</span><br><span class="line">TensorRT Version: 7.2</span><br><span class="line">cuDNN Version: 8.0</span><br><span class="line">libNVWarp360 Version: 2.0.1d3</span><br></pre></td></tr></table></figure>
<p>这个GStreamer-WARNING在<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_troubleshooting.html#errors-occur-when-deepstream-app-fails-to-load-plugin-gst-nvinferserver-on-dgpu-only">Troubleshooting</a>里有提到，就是说在ubuntu电脑上安装出现这个警告是预料之中的，如果不需要Triton可以不用管它。</p>
<p>我原以为这样就安装成功了，后来在跑<a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/deepstream_pose_estimation">deepstream_pose_estimation</a>的时候才意识到我没有完全正确安装，相关的nvidia forum <a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/failed-to-load-plugin-usr-lib-x86-64-linux-gnu-gstreamer-1-0-deepstream-libnvdsgst-osd-so/163662">topic</a>在这里，我方法1 2 3都有报错，最后通过<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_docker_containers.html">方法4</a>，即docker成功安装好了，又体验到了用docker安装的爽。。。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nvcr.io&#x2F;nvidia&#x2F;deepstream:5.0.1-20.09-devel</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --gpus all -it -v /home/weidawang/volume/deepstream:/home/weidawang/volume/deepstream nvcr.io/nvidia/deepstream:5.0.1-20.09-devel</span><br></pre></td></tr></table></figure>
<p>安装好deepstream的docker镜像之后，我选择用<a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/deepstream_pose_estimation">deepstream_pose_estimation</a>来练手，详细的教程徐需要看它的<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/creating-a-human-pose-estimation-application-with-deepstream-sdk">blog</a>。<br>使用这个deepstream_pose_estimation时，有几个注意点：</p>
<ol>
<li>克隆下来的仓库已经包含了pose_estimation.onnx，不需要再下载</li>
<li>作者说要做这一步：<code>sudo cp libnvds_osd.so /opt/nvidia/deepstream/deepstream-5.0/lib</code></li>
<li>用<code>exit</code>退出容器后, 如果想要重新进入容器，先<code>docker ps -a</code>列出以往你运行过的容器，然后 <code>docker start [container_name]</code>，最后再 <code>docker attach [container_name]</code>进入容器</li>
</ol>
<h3 id="DeepStream-Triton-Inference-Server使用指南"><a href="#DeepStream-Triton-Inference-Server使用指南" class="headerlink" title="DeepStream Triton Inference Server使用指南"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Quickstart.html#deepstream-triton-inference-server-usage-guidelines">DeepStream Triton Inference Server使用指南</a></h3><h4 id="dGPU"><a href="#dGPU" class="headerlink" title="dGPU"></a>dGPU</h4><ol>
<li><p>拉DeepStream Triton Inference Server docker镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nvcr.io&#x2F;nvidia&#x2F;deepstream:5.0.1-20.09-triton</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 docker镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --gpus all -it --rm -v &#x2F;tmp&#x2F;.X11-unix:&#x2F;tmp&#x2F;.X11-unix -e DISPLAY&#x3D;$DISPLAY nvcr.io&#x2F;nvidia&#x2F;deepstream:5.0.1-20.09-triton</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>个人建议，不要加<code>--rm</code>， 否则你退出容器后，你的所有操作都会被情况，<code>docker ps -a</code>都不能找到你刚用过的容器。</p>
<h4 id="Jetson"><a href="#Jetson" class="headerlink" title="Jetson"></a>Jetson</h4><p>Triton Inference Server共享库是Jetson上DeepStream的预装部分。安装Triton Inference Server不需要额外的步骤。</p>
<p>对于这两个平台，要运行这些示例，请遵循README文件的<em>Running the Triton Inference Server samples</em>部分的步骤。</p>
<h2 id="Docker容器"><a href="#Docker容器" class="headerlink" title="Docker容器"></a>Docker容器</h2><p>DeepStream 5.0为dGPU和Jetson平台均提供了Docker容器。这些容器通过将所有相关的依赖关系打包在容器内，提供了一种方便的，即用的方式来部署DeepStream应用程序。相关的Docker映像托管在NGC Web门户<a target="_blank" rel="noopener" href="https://ngc.nvidia.com/">https://ngc.nvidia.com</a>上的NVIDIA容器注册表中。他们使用了<code>nvidia-docker</code>软件包，该软件包使您能够从容器访问所需的GPU资源。</p>
<blockquote>
<p>注意：用于dGPU和Jetson的DeepStream 5.0容器是不同的，因此您必须为您的平台获取正确的映像。</p>
</blockquote>
<h3 id="dGPU的Docker容器"><a href="#dGPU的Docker容器" class="headerlink" title="dGPU的Docker容器"></a>dGPU的Docker容器</h3><p>NGC Web门户中的“<a target="_blank" rel="noopener" href="https://ngc.nvidia.com/catalog/containers?orderBy=modifiedDESC&amp;pageNumber=0&amp;query=&amp;quickFilter=containers&amp;filters=">容器”</a>页面提供了有关拉取和运行容器以及其内容的说明。dGPU容器称为<code>deepstream</code>，而jetson容器称为<code>deepstream-l4t</code>。与DeepStream 3.0中的容器不同，dGPU DeepStream 5.0容器在容器内支持DeepStream应用程序开发。它包含与DeepStream 5.0 SDK相同的构建工具和开发库。在典型情况下，您将在DeepStream容器中构建，执行和调试DeepStream应用程序。准备好应用程序后，您可以使用DeepStream 5.0容器作为基础映像来创建自己的Docker容器，以容纳应用程序文件（二进制文件，库，模型，配置文件等）。这是一个示例片段用于创建自己的Docker容器的Dockerfile：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace with required `container type` e.g. base, devel etc in the following line</span></span><br><span class="line"><span class="keyword">FROM</span> nvcr.io/nvidia/ deepstream:<span class="number">5.0</span>.<span class="number">1</span>-<span class="number">20.09</span>-&lt;container type&gt;</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> myapp  /root/apps/myapp</span></span><br><span class="line"><span class="comment"># To get video driver libraries at runtime (libnvidia-encode.so/libnvcuvid.so)</span></span><br><span class="line"><span class="keyword">ENV</span> NVIDIA_DRIVER_CAPABILITIES $NVIDIA_DRIVER_CAPABILITIES,video</span><br></pre></td></tr></table></figure>
<p>该Dockerfile将您的应用程序（从目录<code>mydsapp</code>）复制到容器（<code>pathname /root/apps</code>）中。请注意，您必须确保NGC的DeepStream 5.0映像位置正确。</p>
<p>下表列出了随DeepStream 5.0发布的dGPU的Docker容器：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>容器</th>
<th>容器拉取命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>base docker（仅包含运行时库和GStreamer插件。可以作为为DeepStream应用程序构建一个自定义docker的基础）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-base</code></td>
</tr>
<tr>
<td>devel docker（包含整个SDK以及用于构建DeepStream应用程序的开发环境）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-devel</code></td>
</tr>
<tr>
<td>Triton Inference Server docker（安装了Triton Inference Server和其依赖项以及用于构建DeepStream应用程序的开发环境）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-triton</code></td>
</tr>
<tr>
<td>DeepStream IoT docker（仅包含Deepstream-test5-app，所有其他参考应用程序被删除了）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-iot</code></td>
</tr>
<tr>
<td>DeepStream samples docker（包含运行时库，GStreamer插件，参考应用程序以及示例流，模型和配置）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream:5.0.1-20.09-samples</code></td>
</tr>
</tbody>
</table>
</div>
<p>有关<code>nvcr.io</code>身份验证等信息，请参阅DeepStream 5.0发行说明。</p>
<p>请参见NGC上的<a target="_blank" rel="noopener" href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream">dGPU容器</a>以了解更多运行dGPU容器细节和说明。</p>
<h3 id="Jetson的Docker容器"><a href="#Jetson的Docker容器" class="headerlink" title="Jetson的Docker容器"></a>Jetson的Docker容器</h3><p>从JetPack 4.2.1版本开始，已添加了用于Jetson的<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-Container-Runtime-on-Jetson">NVIDIA Container Runtime</a>，使您能够在Jetson设备上运行启用GPU的容器。使用此功能，可以使用NGC上的Docker映像在Jetson设备上的容器内运行DeepStream 5.0。拉出容器并按照<a target="_blank" rel="noopener" href="https://ngc.nvidia.com/catalog/containers?orderBy=modifiedDESC&amp;pageNumber=0&amp;query=&amp;quickFilter=containers&amp;filters=">NGC容器</a>上的说明执行页。DeepStream容器希望将CUDA，TensorRT和VisionWorks安装在Jetson设备上，因为它是从主机安装在容器内的。在启动DeepStream容器之前，请确保已在Jetson上使用JetPack安装了这些实用程序。请注意，Jetson Docker容器仅用于部署。它们不支持容器内的DeepStream软件开发。您可以在Jetson目标上本地构建应用程序，并通过将二进制文件添加到Docker映像中来为其创建容器。或者，您可以按照在x86工作站上构建Jetson容器中的说明从工作站生成Jetson容器NVIDIA Container Runtime for Jetson文档中的“部分”。下表列出了随DeepStream 5.0发布的Jetson的Docker容器：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>容器</th>
<th>容器拉取命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base docker（仅包含运行时库和GStreamer插件。可用作构建DeepStream应用程序的自定义docker的基础）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream-l4t:5.0.1-20.09-base</code></td>
</tr>
<tr>
<td>DeepStream IoT docker（仅安装了deepstream-test5-app，并删除了所有其他参考应用程序。）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream-l4t:5.0.1-20.09-iot</code></td>
</tr>
<tr>
<td>DeepStream samples docker（包含运行时库，GStreamer插件，参考应用程序以及示例流，模型和配置）</td>
<td><code>docker pull nvcr.io/nvidia/deepstream-l4t:5.0.1-20.09-samples</code></td>
</tr>
</tbody>
</table>
</div>
<p>有关<code>nvcr.io</code>身份验证等信息，请参阅DeepStream 5.0发行说明。</p>
<p>见NGC<a target="_blank" rel="noopener" href="https://ngc.nvidia.com/catalog/containers/nvidia:deepstream-l4t">Jetson container</a> 更多运行jetson容器的细节和说明。</p>
<h1 id="DeepStream样例"><a href="#DeepStream样例" class="headerlink" title="DeepStream样例"></a><strong>DeepStream样例</strong></h1><h2 id="C-C-示例应用程序源详细信息"><a href="#C-C-示例应用程序源详细信息" class="headerlink" title="C / C ++示例应用程序源详细信息"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html#c-c-sample-apps-source-details">C / C ++示例应用程序源详细信息</a></h2><div class="table-container">
<table>
<thead>
<tr>
<th>参考测试应用</th>
<th>源目录内的路径</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sample test application 1</td>
<td>sources/apps/sample_apps/deepstream-test1</td>
<td>如何对单个H.264流使用DeepStream元素的示例：filesrc→decode→nvstreammux→nvinfer（primary detector）→nvdsosd→renderer。</td>
</tr>
<tr>
<td>Sample test application 2</td>
<td>sources/apps/sample_apps/deepstream-test2</td>
<td>如何对单个H.264流使用DeepStream元素的示例：filesrc→decode→nvstreammux→nvinfer（primary detector）→nvtracker→nvinfer（secondary classifier）→nvdsosd→renderer。</td>
</tr>
<tr>
<td>Sample test application 3</td>
<td>sources/apps/sample_apps/deepstream-test3</td>
<td>基于deepstream-test1构建，演示如何：在管道中使用多个来源；使用uridecodebin接受任何类型的输入（例如RTSP /文件）、任何GStreamer支持的容器格式以及任何编解码器；配置Gst-nvstreammux以生成一批帧并推断出这些帧以提高资源利用率；提取流元数据，其中包含有关批处理缓冲区中帧的有用信息</td>
</tr>
<tr>
<td>Sample test application 4</td>
<td>sources/apps/sample_apps/deepstream-test4</td>
<td>基于deepstream-test1构建，演示如何：在管道中使用Gst-nvmsgconv和Gst-nvmsgbroker插件；创建NVDS_META_EVENT_MSG类型的元数据并将其attach到缓冲区；将NVDS_META_EVENT_MSG用于不同类型的对象，例如车辆和人；Implement “copy” and “free” functions for use if metadata is extended through the extMsg field</td>
</tr>
<tr>
<td>Sample test application 5</td>
<td>sources/apps/sample_apps/deepstream-test5</td>
<td>建立在deepstream-app的基础上，演示如何：在多流管道中使用Gst-nvmsgconv和Gst-nvmsgbroker插件；如何从配置文件中将Gst-nvmsgbroker插件配置为接收器插件（适用于KAFKA，Azure等）；如何处理来自RTSP服务器或摄像机的RTCP sender reports，以及如何将Gst Buffer PTS转换为UTC时间戳。                                              有关更多详细信息，请参考位于<code>deepstream_test5_app_main.c</code>中的RTCP sender report回调函数<code>test5_rtcp_sender_report_callback()</code>的注册和用法。GStreamer callback registration with rtpmanager element’s “handle-sync” signal is documented in <code>apps-common/src/deepstream_source_bin.c</code>。</td>
</tr>
<tr>
<td>AMQP协议测试应用</td>
<td>sources/libs/amqp_protocol_adaptor</td>
<td>用于测试AMQP协议的应用程序。</td>
</tr>
<tr>
<td>Azure MQTT测试应用程序</td>
<td>sources/libs /azure_protocol_adaptor</td>
<td>测试应用程序以显示使用MQTT的Azure IoT device2edge消息传递和device2cloud消息传递。</td>
</tr>
<tr>
<td>DeepStream参考应用程序</td>
<td>sources/apps/sample_apps/deepstream-app</td>
<td>DeepStream参考应用程序的源代码。</td>
</tr>
<tr>
<td>UFF SSD detector</td>
<td>sources/objectDetector_SSD</td>
<td>SSD检测器模型的配置文件和自定义库实现。</td>
</tr>
<tr>
<td>Faster RCNN detector</td>
<td>sources/objectDetector_FasterRCNN</td>
<td>FasterRCNN模型的配置文件和自定义库实现。</td>
</tr>
<tr>
<td>Yolo detector</td>
<td>sources/objectDetector_Yolo</td>
<td>Yolo模型（当前为Yolo v2，v2 tiny，v3和v3 tiny）的配置文件和自定义库实现。</td>
</tr>
<tr>
<td>Dewarper示例</td>
<td>apps / sample_apps / deepstream-dewarper-test</td>
<td>演示单个或多个360度摄像机流的扭曲功能。从CSV文件中读取相机校准参数，并在显示屏上渲染过道和斑点表面。</td>
</tr>
<tr>
<td>光流示例</td>
<td>apps / sample_apps / deepstream-nvof-test</td>
<td>演示单个或多个流的光流功能。本示例使用两个GStreamer插件（Gst-nvof和Gst-nvofvisual）。Gst-nvof元素生成MV（运动矢量）数据并将其作为用户元数据附加。Gst-nvofvisual元素使用预定义的色轮矩阵可视化MV数据。</td>
</tr>
<tr>
<td>自定义元数据示例</td>
<td>apps / sample_apps / deepstream-user-metadata-test</td>
<td>演示如何向DeepStream的任何组件中添加自定义或用户特定的元数据。测试代码将一个填充有用户数据的16字节数组附加到所选组件。数据在另一个组件中检索。</td>
</tr>
<tr>
<td>MJPEG和JPEG解码器以及推理示例</td>
<td>apps / sample_apps / deepstream-image-decode-test</td>
<td>建立在deepstream-test3的基础上，以演示图像解码而不是视频。本示例使用自定义解码箱，因此可以将MJPEG编解码器用作输入。</td>
</tr>
<tr>
<td>图像/视频分割示例</td>
<td>apps / sample_apps / deepstream-segmentation-test</td>
<td>演示使用语义或工业神经网络对多流视频或图像进行分段并将输出呈现到显示器。</td>
</tr>
<tr>
<td>在Gst-nvstreammux之前处理元数据</td>
<td>apps / sample_apps / deepstream-gst-metadata-test</td>
<td>演示如何在DeepStream管道中的Gst-nvstreammux插件之前设置元数据，以及如何在Gst-nvstreammux之后访问元数据。</td>
</tr>
<tr>
<td>GST-Nvinfer张量元流示例</td>
<td>apps / sample_apps / deepstream-infer-tensor-meta-app</td>
<td>演示如何将nvinfer张量输出作为元数据传递和访问。</td>
</tr>
<tr>
<td>性能演示</td>
<td>apps / sample_apps / deepstream-perf-demo</td>
<td>对目录中的所有流顺序执行单通道级联推理和对象跟踪。</td>
</tr>
<tr>
<td>分析示例</td>
<td>apps / sample_apps / deepstream-nvdsanalytics-test</td>
<td>演示批处理分析，例如ROI过滤，线交叉，方向检测和拥挤</td>
</tr>
<tr>
<td>OpenCV示例</td>
<td>apps / sample_apps / deepstream-opencv-test</td>
<td>演示在dsexample插件中使用OpenCV</td>
</tr>
<tr>
<td>图像作为元数据示例</td>
<td>Apps / sample_apps / deepstream-image-meta-test</td>
<td>演示如何将编码的图像附加为元数据并以jpeg格式保存图像。</td>
</tr>
<tr>
<td>Appsrc和Appsink示例</td>
<td>apps / sample_apps / deepstream-appsrc-test</td>
<td>演示AppSrc和AppSink的用法，分别使用和提供非DeepStream代码中的数据。</td>
</tr>
<tr>
<td>迁移学习的例子</td>
<td>apps / sample_apps / deepstream-transfer-learning-app</td>
<td>演示了一种将图像保存为置信度较小的对象的机制，该机制可用于进一步训练</td>
</tr>
<tr>
<td>Mask-RCNN示例</td>
<td>apps / sample_apps / deepstream-mrcnn-test</td>
<td>使用Mask-RCNN模型演示实例分割</td>
</tr>
</tbody>
</table>
</div>
<h3 id="插件和库源详细信息"><a href="#插件和库源详细信息" class="headerlink" title="插件和库源详细信息"></a>插件和库源详细信息</h3><p>下表描述了源目录的内容，但参考测试应用程序除外，它们在下面分别列出：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>插件或库</th>
<th>源目录内的路径</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>DsExample GStreamer插件</td>
<td>gst-plugins / gst-dsexample</td>
<td>用于将自定义算法集成到DeepStream SDK图中的模板插件。</td>
</tr>
<tr>
<td>GStreamer Gst-nvmsgconv插件</td>
<td>gst-plugins / gst-nvmsgconv</td>
<td>GStreamer Gst-nvmsgconv插件的源代码，用于将元数据转换为架构格式。</td>
</tr>
<tr>
<td>GStreamer Gst-nvmsgbroker插件</td>
<td>gst-plugins / gst-nvmsgbroker</td>
<td>GStreamer Gst-nvmsgbroker插件的源代码，用于将数据发送到服务器。</td>
</tr>
<tr>
<td>GStreamer Gst-nvinfer插件</td>
<td>gst-plugins / gst-nvinfer</td>
<td>用于推断的GStreamer Gst-nvinfer插件的源代码。</td>
</tr>
<tr>
<td>GStreamer Gst-nvdsosd插件</td>
<td>gst-plugins / gst-nvdsosd</td>
<td>GStreamer Gst-nvdsosd插件的源代码，用于绘制bbox，文本和其他对象。</td>
</tr>
<tr>
<td>NvDsInfer库</td>
<td>libs / nvdsinfer</td>
<td>NvDsInfer库的源代码，由Gst-nvinfer GStreamer插件使用。</td>
</tr>
<tr>
<td>NvMsgConv库</td>
<td>libs / nvmsgsconv</td>
<td>Gst-nvmsgconv GStreamer插件所需的NvMsgConv库的源代码。</td>
</tr>
<tr>
<td>Kafka协议适配器</td>
<td>libs / kafka_protocol_adapter</td>
<td>Kafka的协议适配器。</td>
</tr>
<tr>
<td>nvdsinfer_customparser</td>
<td>libs / nvdsinfer_customparser</td>
<td>用于检测器和分类器的定制模型输出解析示例。</td>
</tr>
<tr>
<td>GST-V4L2</td>
<td>请参阅下面的注释 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_C_Sample_Apps.html#f1">1</a></td>
<td>v4l2编解码器的源代码。</td>
</tr>
</tbody>
</table>
</div>
<p>脚注1：</p>
<p>DeepStream软件包中不存在Gst-<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/V4L2">v4l2</a>源。要下载，请按照下列步骤操作：</p>
<p>转到：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/embedded/downloads">https</a> : <a target="_blank" rel="noopener" href="https://developer.nvidia.com/embedded/downloads">//developer.nvidia.com/embedded/downloads</a>。在字段中输入<code>Search filter``L4T sources</code>为L4T Release选择适当的项目<code>32.4.3</code>。下载文件并将其解压缩以获取<code>.tbz2</code>文件，展开<code>.tbz2</code>文件，<code>Gst-v4l2</code>源文件在<code>gst-nvvideo4linux2_src.tbz2</code></p>
<h2 id="Python示例应用程序源详细信息"><a href="#Python示例应用程序源详细信息" class="headerlink" title="Python示例应用程序源详细信息"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/text/DS_Python_Sample_Apps.html#python-sample-apps-source-details">Python示例应用程序源详细信息</a></h2><h3 id="Python绑定"><a href="#Python绑定" class="headerlink" title="Python绑定"></a>Python绑定</h3><p>本节提供有关使用Python进行DeepStream应用程序开发的详细信息。DeepStream 5.0 SDK中包含Python绑定，可在以下位置找到示例应用程序： <a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps">https://github.com/NVIDIA-AI-IOT/deepstream_python_apps</a>。在此处阅读有关PyDS API的更多信息：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/metropolis/deepstream/python-api/">https://docs.nvidia.com/metropolis/deepstream/python-api/</a> </p>
<h3 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h3><ul>
<li><p>Ubuntu 18.04</p>
</li>
<li><p>DeepStream SDK 5.0或更高版本</p>
</li>
<li><p>Python 3.6</p>
</li>
<li><p>Gst Python v1.14.5</p>
<p>如果Jetson上缺少Gst python安装，请使用以下命令进行安装：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install python-gi-dev</span><br><span class="line">$ export GST_LIBS&#x3D;&quot;-lgstreamer-1.0 -lgobject-2.0 -lglib-2.0&quot;</span><br><span class="line">$ export GST_CFLAGS&#x3D;&quot;-pthread -I&#x2F;usr&#x2F;include&#x2F;gstreamer-1.0 -I&#x2F;usr&#x2F;include&#x2F;glib-2.0 -I&#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;glib-2.0&#x2F;include&quot;</span><br><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;GStreamer&#x2F;gst-python.git</span><br><span class="line">$ cd gst-python</span><br><span class="line">$ git checkout 1a8f48a</span><br><span class="line">$ .&#x2F;autogen.sh PYTHON&#x3D;python3</span><br><span class="line">$ .&#x2F;configure PYTHON&#x3D;python3</span><br><span class="line">$ make</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="运行示例应用程序"><a href="#运行示例应用程序" class="headerlink" title="运行示例应用程序"></a>运行示例应用程序</h3><ol>
<li><p>在以下位置<code>&lt;DeepStream 5.0 ROOT&gt;/sources</code>克隆仓库<code>deepstream_python_apps</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;NVIDIA-AI-IOT&#x2F;deepstream_python_apps</span><br></pre></td></tr></table></figure>
</li>
<li><p>这将创建以下目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;DeepStream 5.0 ROOT&gt;&#x2F;sources&#x2F;deepstream_python_apps</span><br></pre></td></tr></table></figure>
</li>
<li><p>Python应用程序位于apps`目录下。进入每个应用程序目录，并按照自述文件中的说明进行操作。</p>
<p>注意 : 应用程序配置文件包含模型的相对路径。</p>
</li>
</ol>
<h3 id="管道建设"><a href="#管道建设" class="headerlink" title="管道建设"></a>管道建设</h3><p>可以使用Gst Python（GStreamer框架的Python绑定）构造DeepStream管道。有关管道构造示例，请参见示例应用程序的主要功能。</p>
<h3 id="元数据访问"><a href="#元数据访问" class="headerlink" title="元数据访问"></a>元数据访问</h3><p>DeepStream MetaData包含推理结果和分析中使用的其他信息。元数据被附加到每个管道组件接收到的<code>Gst Buffer</code>。SDK元数据文档和API指南中详细描述了元数据格式。SDK MetaData库是用C / C ++开发的。Python绑定提供了从Python应用程序对MetaData的访问。绑定在已编译的模块中提供，可用于x86_64和Jetson平台。<code>pyds.so</code>模块位于DeepStream SDK安装目录<code>/lib</code>下。示例应用程序通过common / utils.py获取此模块的导入路径。<code>/lib</code>目录还包括setup.py，用于将模块安装到标准路径中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;nvidia&#x2F;deepstream&#x2F;deepstream&#x2F;lib</span><br><span class="line">python3 setup.py install</span><br></pre></td></tr></table></figure>
<p>由于python用法是可选的，因此当前没有通过SDK安装程序自动完成。绑定通常遵循与C / C ++库相同的API，以下几节详细介绍了一些例外。</p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>MetaData的内存由Python和C / C ++代码路径共享。例如，可以通过用Python编写的探测函数添加MetaData项，并且需要由用C / C ++编写的下游插件访问。deepstream-test4应用程序包含此类用法。Python垃圾收集器无法查看C / C ++中的内存引用，因此无法安全地管理此类共享内存的生存期。由于这种复杂性，Python通常通过引用实现对MetaData内存的访问，而无需声明所有权。</p>
<h3 id="分配"><a href="#分配" class="headerlink" title="分配"></a>分配</h3><p>在Python中分配MetaData对象时，绑定将提供分配功能，以确保该对象具有适当的内存所有权。如果使用了构造函数，则垃圾回收器在其Python引用终止时将声明该对象。但是，仍然需要下游的C / C ++代码访问该对象，因此该对象必须在这些Python引用之外仍然存在。示例：要分配<code>NvDsEventMsgMeta</code>实例，请使用此命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msg_meta &#x3D; pyds.alloc_nvds_event_msg_meta() *# get reference to allocated instance without claiming memory ownership*</span><br></pre></td></tr></table></figure>
<p>不是这个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msg_meta &#x3D; NvDsEventMsgMeta() *# memory will be freed by the garbage collector when msg_meta goes out of scope in Python*</span><br></pre></td></tr></table></figure>
<p>分配器可用于以下结构：</p>
<ul>
<li><code>NvDsVehicleObject: alloc_nvds_vehicle_object()</code></li>
<li><code>NvDsPersonObject: alloc_nvds_person_object()</code></li>
<li><code>NvDsFaceObject: alloc_nvds_face_object()</code></li>
<li><code>NvDsEventMsgMeta: alloc_nvds_event_msg_meta()</code></li>
<li><code>NvDsEvent: alloc_nvds_event()</code></li>
<li><code>NvDsPayload: alloc_nvds_payload()</code></li>
<li><code>Generic buffer: alloc_buffer(size)</code></li>
</ul>
<h3 id="字符串访问"><a href="#字符串访问" class="headerlink" title="字符串访问"></a>字符串访问</h3><p>一些MetaData结构包含字符串字段。以下各节提供了有关访问它们的详细信息。</p>
<h4 id="设置字符串字段"><a href="#设置字符串字段" class="headerlink" title="设置字符串字段"></a>设置字符串字段</h4><p>设置字符串字段会导致在基础C ++代码中分配字符串缓冲区。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">obj.type &#x3D; &quot;Type&quot;</span><br></pre></td></tr></table></figure>
<p>这将导致分配内存缓冲区，并将字符串“ TYPE”复制到其中。该内存归C代码所有，稍后将释放。要释放Python代码中的缓冲区，请使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyds.free_buffer(obj.type)</span><br></pre></td></tr></table></figure>
<p>注意</p>
<p><code>NvOSD_TextParams.display_text</code> 现在，分配新字符串后，字符串会自动释放。</p>
<h4 id="读取字符串字段"><a href="#读取字符串字段" class="headerlink" title="读取字符串字段"></a>读取字符串字段</h4><p>直接读取字符串字段将以int的形式返回该字段的C地址，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">obj &#x3D; pyds.NvDsVehicleObject.cast(data);</span><br><span class="line">print(obj.type)</span><br></pre></td></tr></table></figure>
<p>这将打印一个int表示<code>obj.type</code>C中的地址（这是一个char *）。要检索此字段的字符串值，请使用<code>pyds.get_string()</code>，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(pyds.get_string(obj.type))</span><br></pre></td></tr></table></figure>
<h3 id="Casting"><a href="#Casting" class="headerlink" title="Casting"></a>Casting</h3><p>一些MetaData实例以GList形式存储。要访问GList节点中的数据，需要将数据字段强制转换为适当的结构。该转换通过针对目标类型的cast（）成员函数完成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">NvDsBatchMeta.cast</span><br><span class="line">NvDsFrameMeta.cast</span><br><span class="line">NvDsObjectMeta.cast</span><br><span class="line">NvDsUserMeta.cast</span><br><span class="line">NvDsClassifierMeta.cast</span><br><span class="line">NvDsDisplayMeta.cast</span><br><span class="line">NvDsLabelInfo.cast</span><br><span class="line">NvDsEventMsgMeta.cast</span><br><span class="line">NvDsVehicleObject.cast</span><br><span class="line">NvDsPersonObject.cast</span><br></pre></td></tr></table></figure>
<p>在v0.5版中，提供了独立的强制转换功能。现在，上面的cast（）函数已弃用并取代了这些函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">glist_get_nvds_batch_meta</span><br><span class="line">glist_get_nvds_frame_meta</span><br><span class="line">glist_get_nvds_object_meta</span><br><span class="line">glist_get_nvds_user_meta</span><br><span class="line">glist_get_nvds_classifier_meta</span><br><span class="line">glist_get_nvds_display_meta</span><br><span class="line">glist_get_nvds_label_info</span><br><span class="line">glist_get_nvds_event_msg_meta</span><br><span class="line">glist_get_nvds_vehicle_object</span><br><span class="line">glist_get_nvds_person_object</span><br></pre></td></tr></table></figure>
<p>例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l_frame &#x3D; batch_meta.frame_meta_list</span><br><span class="line">frame_meta &#x3D; pyds.NvDsFrameMeta.cast(l_frame.data)</span><br></pre></td></tr></table></figure>
<h3 id="回调功能注册"><a href="#回调功能注册" class="headerlink" title="回调功能注册"></a>回调功能注册</h3><p>添加到NvDsUserMeta的自定义元数据需要自定义复制和发布功能。MetaData库依赖于这些自定义功能来对自定义结构进行深度复制，并释放已分配的资源。这些函数在NvDsUserMeta结构中注册为回调函数指针。使用以下功能注册回调功能：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pyds.set_user_copyfunc(NvDsUserMeta_instance, copy_function)</span><br><span class="line">pyds.set_user_releasefunc(NvDsUserMeta_instance, free_func)</span><br></pre></td></tr></table></figure>
<p>注意</p>
<p>在应用程序退出之前，需要在绑定库中取消注册回调。绑定库当前保留对已注册函数的全局引用，并且这些引用不能超过绑定库在应用程序退出时发生的卸载。使用以下函数注销所有回调：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pyds.unset_callback_funcs()</span><br></pre></td></tr></table></figure>
<p>有关回调注册和注销的示例，请参见deepstream-test4示例应用程序。</p>
<p><strong>限制</strong>：绑定库当前仅为每个应用程序支持一组回调函数。将使用最后注册的功能。</p>
<h3 id="优化和实用程序"><a href="#优化和实用程序" class="headerlink" title="优化和实用程序"></a>优化和实用程序</h3><p>通常，Python解释比运行已编译的C / C ++代码要慢。为了提供更好的性能，某些操作在C中实现，并通过绑定接口公开。目前这是实验性的，并将随着时间的推移而扩展。提供以下优化功能：</p>
<ul>
<li><p><code>pyds.NvOSD_ColorParams.set(double red, double green, double blue, double alpha)</code></p>
<blockquote>
<p>这是一个简单的函数，其执行与以下操作相同的操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">txt_params.text_bg_clr.red &#x3D; red</span><br><span class="line">txt_params.text_bg_clr.green &#x3D; green</span><br><span class="line">txt_params.text_bg_clr.blue &#x3D; blue</span><br><span class="line">txt_params.text_bg_clr.alpha &#x3D; alpha</span><br></pre></td></tr></table></figure>
<p>这些操作是在deepstream_test_4.py中的每个对象上执行的，从而导致合计处理时间减慢了管道的速度。将此功能推入C层有助于提高性能。</p>
</blockquote>
</li>
<li><p><code>generate_ts_rfc3339 (buffer, buffer_size)</code></p>
<blockquote>
<p>此函数使用根据RFC3339生成的时间戳填充输入缓冲区： <code>%Y-%m-%dT%H:%M:%S.nnnZ\0</code></p>
</blockquote>
</li>
</ul>
<h3 id="图像数据访问"><a href="#图像数据访问" class="headerlink" title="图像数据访问"></a>图像数据访问</h3><p>解码后的图像可以<code>NumPy</code>通过该<code>get_nvds_buf_surface</code>函数作为数组访问。API指南中记录了此功能。有关<code>deepstream-imagedata-multistream</code>图像数据使用的示例，请参见示例应用程序。</p>
<h2 id="样本应用程序源详细信息"><a href="#样本应用程序源详细信息" class="headerlink" title="样本应用程序源详细信息"></a>样本应用程序源详细信息</h2><p>下表显示了<a target="_blank" rel="noopener" href="https://github.com/NVIDIA-AI-IOT/deepstream_python_apps下Python示例应用程序的位置">https://github.com/NVIDIA-AI-IOT/deepstream_python_apps下Python示例应用程序的位置</a></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>参考测试应用</th>
<th>GitHub存储库中的路径</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>简单测试应用程序1</td>
<td>apps / deepstream-test1</td>
<td>如何对单个H.264流使用DeepStream元素的简单示例：filesrc→解码→nvstreammux→nvinfer（主检测器）→nvdsosd→渲染器。</td>
</tr>
<tr>
<td>简单测试应用程序2</td>
<td>apps / deepstream-test2</td>
<td>如何对单个H.264流使用DeepStream元素的简单示例：filesrc→解码→nvstreammux→nvinfer（主检测器）→nvtracker→nvinfer（辅助分类器）→nvdsosd→渲染器。</td>
</tr>
<tr>
<td>简单测试应用程序3</td>
<td>apps / deepstream-test3</td>
<td>基于deepstream-test1（简单测试应用程序1）构建，以演示如何：在管道中使用多个来源使用uridecodebin接受任何类型的输入（例如RTSP /文件），任何GStreamer支持的容器格式以及任何编解码器配置Gst-nvstreammux以生成一批帧并推断出这些帧以提高资源利用率提取流元数据，其中包含有关批处理缓冲区中帧的有用信息</td>
</tr>
<tr>
<td>简单测试应用程序4</td>
<td>应用程序/ deepstream-test4</td>
<td>基于deepstream-test1构建单个H.264流：filesrc，decode，nvstreammux，nvinfer，nvdsosd，renderer演示如何：在管道中使用Gst-nvmsgconv和Gst-nvmsgbroker插件创建NVDS_META_EVENT_MSG类型的元数据并将其附加到缓冲区将NVDS_META_EVENT_MSG用于不同类型的对象，例如车辆和人如果通过extMsg字段扩展了元数据，则实现“复制”和“免费”功能以供使用</td>
</tr>
<tr>
<td>USB摄像头源应用</td>
<td>apps / deepstream-test1-usbcam</td>
<td>简单测试应用程序1已修改为处理来自USB摄像机的单个流。</td>
</tr>
<tr>
<td>RTSP输出应用</td>
<td>apps / deepstream-test1-rtsp-out</td>
<td>简单测试应用程序1已修改为通过RTSP输出可视化流。</td>
</tr>
<tr>
<td>图像数据访问应用</td>
<td>apps / deepstream-imagedata-multistream</td>
<td>以简单的测试应用程序3为基础，演示如何：在管道中将解码的帧作为NumPy数组访问检查检测到的对象的检测置信度（需要DBSCAN或NMS群集）使用OpenCV注释框架并将其保存到文件</td>
</tr>
<tr>
<td>SSD检测器输出解析器应用</td>
<td>apps / deepstream-ssd-parser</td>
<td>演示如何对Triton Inference Server的推理输出执行自定义后处理：在Triton Inference Server上使用SSD模型进行对象检测通过配置文件设置为Triton Inference Server启用自定义后处理和原始张量导出访问管道中的推断输出张量以在Python中进行后处理将检测到的对象添加到元数据将OSD可视化输出到MP4文件</td>
</tr>
</tbody>
</table>
</div>
<h3 id="DeepStream参考应用程序-deepstream-test5应用程序"><a href="#DeepStream参考应用程序-deepstream-test5应用程序" class="headerlink" title="DeepStream参考应用程序-deepstream-test5应用程序"></a>DeepStream参考应用程序-deepstream-test5应用程序</h3><p>除常规推理管道外，Test5应用程序还支持以下功能：</p>
<ul>
<li>将消息发送到后端服务器。</li>
<li>充当使用者以从后端服务器接收消息。</li>
<li>基于从服务器收到的消息触发基于事件的记录。</li>
<li>OTA模型更新。</li>
</ul>
<h4 id="支持物联网协议和云配置"><a href="#支持物联网协议和云配置" class="headerlink" title="支持物联网协议和云配置"></a>支持物联网协议和云配置</h4><p><code>nvmsgbroker</code>DeepStream插件指南中列出了插件支持的IoT协议（如KAFKA，Azure，AMQP等）的详细信息。DeepStream Public文档可参考特定于所使用协议的设置IoT中心/服务器/经纪人。与<code>type=6</code>for<code>nvmsgconv</code>和<code>nvmsgbroker</code>configuration相关联的[sink]组密钥在：ref：config-groups-label中讨论。</p>
<h4 id="消息使用者"><a href="#消息使用者" class="headerlink" title="消息使用者"></a>消息使用者</h4><p><code>deepstream-test5-app</code>可以配置为充当云消息的消息使用者。解析收到的消息后，可以根据消息的内容触发特定的操作。例如，保存智能记录上下文的NvDsSrcParentBin <em>作为参数传递，该参数<code>start_cloud_to_device_messaging()</code>用于触发智能记录的启动/停止。默认情况下，已实现基于事件的记录以演示消息使用方的用法。用户需要实现自定义逻辑，以处理其他类型的接收消息。请参阅`deepstream_c2d_msg</em><code>文件以获取有关实现的更多详细信息。要订阅云消息，请相应地配置</code>[message-consumer]`组。</p>
<h4 id="智能记录-基于事件的记录"><a href="#智能记录-基于事件的记录" class="headerlink" title="智能记录-基于事件的记录"></a>智能记录-基于事件的记录</h4><p>可以将Test5应用程序配置为基于从服务器收到的事件来记录原始视频提要。这样，无需始终保存数据，此功能仅允许记录感兴趣的事件。请参阅《 DeepStream插件手册》，并在“组”下。当前，test5应用仅支持源类型= 4（RTSP）。类似的方法也可以用于其他类型的源。可通过两种方式触发智能记录事件：<code>gst-nvdssr.h ``header file for more details about smart record. Event based recording can be enabled by setting ``smart-record``[sourceX]</code></p>
<p>1.通过云消息。要通过云消息触发智能记录，应将Test5应用程序配置为充当消息使用者。可以通过相应地配置[message-consumerX]组来完成。配置消息使用者之后，应在需要基于事件的记录的源上启用智能记录。可以按照以下步骤进行操作： <code>smart-record=1</code> 预计以下最小json消息将触发智能记录的开始/停止。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> &#123;</span><br><span class="line">command: string   &#x2F;&#x2F; &lt;start-recording &#x2F; stop-recording&gt;</span><br><span class="line">start: string     &#x2F;&#x2F; &quot;2020-05-18T20:02:00.051Z&quot;</span><br><span class="line">end: string       &#x2F;&#x2F; &quot;2020-05-18T20:02:02.851Z&quot;,</span><br><span class="line">sensor: &#123;</span><br><span class="line">id: string</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>2.通过当地活动。set <code>smart-record=2</code>，这将通过云消息以及本地事件启用智能记录。为了演示通过本地事件进行的基于事件的记录，默认情况下，应用程序每十秒钟触发一次启动/停止事件。此间隔和其他参数是可配置的。</p>
<h4 id="OTA模型更新"><a href="#OTA模型更新" class="headerlink" title="OTA模型更新"></a>OTA模型更新</h4><p>Test5应用程序可以动态更新正在运行的管道中的模型。为此，该应用程序提供了命令行选项<code>-o</code>。如果使用<code>-o</code>（ota_override_file）选项启动了test5应用，则将监视对该文件的任何更改，并基于该文件中的更改，使用新模型即时更新正在运行的管道。</p>
<h4 id="使用OTA功能"><a href="#使用OTA功能" class="headerlink" title="使用OTA功能"></a>使用OTA功能</h4><p>执行以下操作以使用OTA功能：</p>
<ol>
<li><code>deepstream-test5-app</code>使用选项运行<code>-o &lt;ota_override_file&gt;</code></li>
<li>在DS应用程序运行时，<code>&lt;ota_override_file&gt;</code>使用新的模型详细信息进行更新并保存</li>
<li>文件内容更改被检测到<code>deepstream-test5-app</code>，然后开始模型更新过程。当前，仅模型更新功能受支持为OTA功能的一部分。</li>
</ol>
<p><strong>即时模型更新的假设</strong>：</p>
<ol>
<li>新模型必须具有与先前模型相同的网络参数配置（例如，网络分辨率，网络体系结构，类数）</li>
<li>开发人员将提供的新模型的引擎文件或缓存文件</li>
<li>对于其它更新的值的配置参数等，，，，等等，如果在覆盖文件提供，将不具有模型开关之后的任何效果。<code>primary gie``group-threshold``bbox color``gpu-id``nvbuf-memory-type</code></li>
<li><code>Secondary gie</code> 模型更新未通过验证，仅主模型更新通过了验证。</li>
<li>在动态模型更新过程中，不应观察到丢帧/无推断的帧</li>
<li>如果模型更新失败，错误消息将打印在控制台上，并且管道应继续在旧模型配置下运行</li>
<li>需要config-file参数来抑制配置文件解析错误打印，在模型切换过程中不使用该配置文件中的值</li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NVIDIA/" rel="tag">NVIDIA</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-NCNN"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/11/29/NCNN/"
    >NCNN</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/11/29/NCNN/" class="article-date">
  <time datetime="2020-11-29T11:57:29.000Z" itemprop="datePublished">2020-11-29</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Windows安装NCNN"><a href="#Windows安装NCNN" class="headerlink" title="Windows安装NCNN"></a>Windows安装NCNN</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/Tencent/ncnn.git</span><br><span class="line">$ <span class="built_in">cd</span> ncnn</span><br><span class="line">$ git submodule update --init</span><br></pre></td></tr></table></figure>
<p>如果按教程说的<code>git submodule update --init</code>会很慢，可以去这里<a target="_blank" rel="noopener" href="https://gitee.com/wwdok/glslang下载这个仓库到ncnn/glslang里面。">https://gitee.com/wwdok/glslang下载这个仓库到ncnn/glslang里面。</a></p>
<h3 id="Build-for-Windows-x64-using-Visual-Studio-Community-2019"><a href="#Build-for-Windows-x64-using-Visual-Studio-Community-2019" class="headerlink" title="Build for Windows x64 using Visual Studio Community 2019"></a>Build for Windows x64 using Visual Studio Community 2019</h3><p>【<a target="_blank" rel="noopener" href="https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-visual-studio-community-2017">offcial tutorial</a>】</p>
<p>从Visual Studio文件夹里打开命令行窗口，如下图橙圈所示，不要简单地通过cmd打开命令行窗口，否则后面执行cmake语句时会遇到cl.exe找不到的问题。同时，以管理员身份打开，以便后面运行<code>mkdir</code>命令。</p>
<p><img src="https://pic.downk.cc/item/5fa667791cd1bbb86be8cc89.jpg" width=70% style="zoom: 50%;" ></p>
<h2 id="Build-protobuf-library"><a href="#Build-protobuf-library" class="headerlink" title="Build protobuf library:"></a>Build protobuf library:</h2><p>Download protobuf-3.4.0(6.03MB) from <a target="_blank" rel="noopener" href="https://github.com/google/protobuf/archive/v3.4.0.zip">https://github.com/google/protobuf/archive/v3.4.0.zip</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="built_in">cd</span> &lt;protobuf-root-dir&gt;</span><br><span class="line">&gt; mkdir build_folder</span><br><span class="line">&gt; <span class="built_in">cd</span> build_folder</span><br><span class="line">&gt; cmake -G<span class="string">&quot;NMake Makefiles&quot;</span> -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=%<span class="built_in">cd</span>%/install -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_MSVC_STATIC_RUNTIME=OFF ../cmake</span><br><span class="line">&gt; nmake</span><br><span class="line">&gt; nmake install</span><br></pre></td></tr></table></figure>
<p>备注：</p>
<ul>
<li><p>nmake的用法：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/juluwangshier/p/11789311.html">WINDOWS CMAKE与NMAKE</a></p>
</li>
<li><p>因为 protobuf-3.4.0 目录下已经有一个BUILD文件，所以<code>mkdir build</code>不了，我改成build_forlder</p>
</li>
</ul>
<p><img src="https://pic.downk.cc/item/5fa6732a1cd1bbb86beb2387.jpg" width=70%></p>
<ul>
<li>第四句命令最后是<code>../cmake</code>，代表指向<code>protobuf-3.4.0/cmake</code>。</li>
<li>cmake语句中含有<code>%cd%</code>，代表当前的路径。</li>
</ul>
<p>我的实际安装过程是：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">**********************************************************************</span><br><span class="line">** Visual Studio 2019 Developer Command Prompt v16.7.5</span><br><span class="line">** Copyright (c) 2020 Microsoft Corporation</span><br><span class="line">**********************************************************************</span><br><span class="line">[vcvarsall.bat] Environment initialized <span class="keyword">for</span>: <span class="string">&#x27;x64&#x27;</span></span><br><span class="line"></span><br><span class="line">C:\Windows\System32&gt;<span class="built_in">cd</span> C:\MachineLearning\CV\protobuf-3.4.0</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0&gt;mkdir build</span><br><span class="line">子目录或文件 build 已经存在。</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0&gt;mkdir build_folder</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0&gt;<span class="built_in">cd</span> build_folder</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0\build_folder&gt;cmake -G<span class="string">&quot;NMake Makefiles&quot;</span> -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=%<span class="built_in">cd</span>%/install -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_MSVC_STATIC_RUNTIME=OFF ../cmake</span><br><span class="line">-- The C compiler identification is MSVC 19.27.29112.0</span><br><span class="line">-- The CXX compiler identification is MSVC 19.27.29112.0</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/Hostx64/x64/cl.exe</span><br><span class="line">-- Check <span class="keyword">for</span> working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/Hostx64/x64/cl.exe - works</span><br><span class="line">-- Detecting C compiler ABI info</span><br><span class="line">-- Detecting C compiler ABI info - <span class="keyword">done</span></span><br><span class="line">......</span><br><span class="line">-- Configuring <span class="keyword">done</span></span><br><span class="line">-- Generating <span class="keyword">done</span></span><br><span class="line">-- Build files have been written to: C:/MachineLearning/CV/protobuf-3.4.0/build_folder</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0\build_folder&gt;nmake</span><br><span class="line"></span><br><span class="line">Microsoft (R) 程序维护实用工具 14.27.29112.0 版</span><br><span class="line">版权所有 (C) Microsoft Corporation。  保留所有权利。</span><br><span class="line"></span><br><span class="line">Scanning dependencies of target libprotobuf-lite</span><br><span class="line">[  0%] Building CXX object CMakeFiles/libprotobuf-lite.dir/src/google/protobuf/arena.cc.obj</span><br><span class="line">arena.cc</span><br><span class="line">[  1%] Building CXX object CMakeFiles/libprotobuf-lite.dir/src/google/protobuf/arenastring.cc.obj</span><br><span class="line">arenastring.cc</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[100%] Linking CXX executable protoc.exe</span><br><span class="line">[100%] Built target protoc</span><br><span class="line"></span><br><span class="line">C:\MachineLearning\CV\protobuf-3.4.0\build_folder&gt;nmake install</span><br><span class="line"></span><br><span class="line">Microsoft (R) 程序维护实用工具 14.27.29112.0 版</span><br><span class="line">版权所有 (C) Microsoft Corporation。  保留所有权利。</span><br><span class="line"></span><br><span class="line">[ 12%] Built target libprotobuf-lite</span><br><span class="line">[ 52%] Built target libprotobuf</span><br><span class="line">[ 52%] Built target js_embed</span><br><span class="line">[ 99%] Built target libprotoc</span><br><span class="line">[100%] Built target protoc</span><br><span class="line">Install the project...</span><br><span class="line">-- Install configuration: <span class="string">&quot;Release&quot;</span></span><br><span class="line">-- Installing: C:/MachineLearning/CV/protobuf-3.4.0/build_folder/install/lib/libprotobuf-lite.lib</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">-- Installing: C:/MachineLearning/CV/protobuf-3.4.0/build_folder/install/cmake/tests.cmake</span><br></pre></td></tr></table></figure>
<h2 id="Build-ncnn-library"><a href="#Build-ncnn-library" class="headerlink" title="Build ncnn library"></a>Build ncnn library</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="built_in">cd</span> &lt;ncnn-root-dir&gt;</span><br><span class="line">&gt; mkdir -p build</span><br><span class="line">&gt; <span class="built_in">cd</span> build</span><br><span class="line">&gt; cmake -G<span class="string">&quot;NMake Makefiles&quot;</span> -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=%<span class="built_in">cd</span>%/install -DProtobuf_INCLUDE_DIR=%protobuf-root-dir%/build/install/include -DProtobuf_LIBRARIES=%protobuf-root-dir%/build/install/lib/libprotobuf.lib -DProtobuf_PROTOC_EXECUTABLE=%protobuf-root-dir%/build/install/bin/protoc.exe -DNCNN_VULKAN=ON ..</span><br><span class="line">&gt; nmake</span><br><span class="line">&gt; nmake install</span><br></pre></td></tr></table></figure>
<p>备注：我这里在命令行里使用了%protobuf-root-dir%，而不是官方的 <protobuf-root-dir> ，因为我设置了下面的环境变量：</p>
<p><img src="https://pic.downk.cc/item/5fa676231cd1bbb86beba7e7.jpg" style="zoom:80%;" ></p>
<p>如果上面的语句<code>-DNCNN_VULKAN=ON</code>没有关掉的话，在安装ncnn前需要安装vulkan，否则安装ncnn时会遇到Cmake错误。但去官网下载的话速度很慢，还容易断开网络连接，一个方法是去csdn上下载别人上传的，</p>
<p>另外一个办法是用这位网友的<a target="_blank" rel="noopener" href="https://blog.csdn.net/tankweight/article/details/103704682?utm_medium=distribute.pc_relevant_download.none-task-blog-baidujs-1.nonecase&amp;depth_1-utm_source=distribute.pc_relevant_download.none-task-blog-baidujs-1.nonecase">shell脚本</a>执行下载，你直接复制粘贴到文本里的话，部门代码可能复制会出错，一个窍门是查看该网页的源代码，将出错的那句代码用源代码里的复制粘贴代替</p>
<p><img src="https://pic.downk.cc/item/5fa6c7ca1cd1bbb86bfe19f7.jpg"></p>
<p>完整shell脚本我还是放一下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash">curl: (28) Connection timed out after 15000 milliseconds</span></span><br><span class="line"><span class="meta">#</span><span class="bash">curl: (28) Operation timed out after 20001 milliseconds with 0 out of 0 bytes received</span></span><br><span class="line"><span class="meta">#</span><span class="bash">curl: (56) OpenSSL SSL_read: SSL_ERROR_SYSCALL, errno 104</span></span><br><span class="line">attempt_counter=0</span><br><span class="line">max_attempts=20</span><br><span class="line">run_step=0</span><br><span class="line">max_run_steps=2</span><br><span class="line">fileUrl=&#x27;https://sdk.lunarg.com/sdk/download/1.1.130.0/linux/vulkansdk-linux-x86_64-1.1.130.0.tar.gz?Human=true&#x27;</span><br><span class="line">until $(curl --connect-timeout 30 --retry-delay 10 --retry-max-time 25 -C - -o 1.1.130.0.tar.gz $fileUrl); do</span><br><span class="line">if [ attemptcounter−eq&#123;max_attempts&#125; ];</span><br><span class="line">then</span><br><span class="line">echo &quot;Max attempts reached&quot;</span><br><span class="line">exit 1</span><br><span class="line">elif [ runstep−eq&#123;max_run_steps&#125; ];</span><br><span class="line">then</span><br><span class="line">echo retrying in 45 seconds...</span><br><span class="line">run_step=0</span><br><span class="line">sleep 45</span><br><span class="line">else</span><br><span class="line">echo Transfer distrupted,retrying in 20 seconds...</span><br><span class="line">sleep 20</span><br><span class="line">fi</span><br><span class="line">printf &#x27;.&#x27;</span><br><span class="line">attempt_counter=(attempt_counter+1)</span><br><span class="line">run_step=(run_step+1)</span><br><span class="line">done</span><br><span class="line">printf &#x27;OK\n&#x27;</span><br></pre></td></tr></table></figure>
<p>然后在命令行窗口执行 <code>sh downloadVulkanSDK.sh</code> 即可。</p>
<h2 id="格式转换"><a href="#格式转换" class="headerlink" title="格式转换"></a>格式转换</h2><p>将其他框架的模型格式转换为ncnn格式。在ncnn/tool目录下就包含了这些转换工具：</p>
<p><img src="https://pic.downk.cc/item/5f9ed3a11cd1bbb86b1f24fb.jpg" style="zoom:80%;" ></p>
<p>也有网页版在线转换工具：<a target="_blank" rel="noopener" href="https://convertmodel.com/">https://convertmodel.com/</a></p>
<h1 id="NCNN-Wiki"><a href="#NCNN-Wiki" class="headerlink" title="NCNN Wiki"></a>NCNN Wiki</h1><ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure">《param and model file structure》</a>    </p>
<p>引言：<code>.param</code>文件不仅可以通过Netron打开查看图形化结构，也可以用VSCode打开用文字的形式查看网络结构，但你打开后需要理解param里的每一行每一列的含义，这就需要查看这篇wiki了。</p>
<p>其中，层参数词典分成4小块来看可能逻辑上更清晰一点，也可以搭配这篇<a target="_blank" rel="noopener" href="https://liumin.blog.csdn.net/article/details/103247724">博客</a>理解。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201121221315580.png" alt="image-20201121221315580" style="zoom:80%;" /></p>
<p>看源码更深入理解这个数组类型key的用法：</p>
</li>
</ul>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201121221545507.png" alt="image-20201121221545507" style="zoom:80%;" /></p>
<p>注意，如果参数键的值用的是<a target="_blank" rel="noopener" href="https://github.com/Tencent/ncnn/wiki/operation-param-weight-table">参数表</a>里的默认值，那.param里就没有写出来了。</p>
<p><strong><a target="_blank" rel="noopener" href="https://github.com/dog-qiuqiu/Yolo-Fastest">Yolo-Fastest</a></strong>：About ncnn_sample Compile Guide：<a target="_blank" rel="noopener" href="https://github.com/dog-qiuqiu/Yolo-Fastest/issues/25">https://github.com/dog-qiuqiu/Yolo-Fastest/issues/25</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36113487/article/details/100676205">《onnx2ncnn并在pc端调用ncnn模型》</a></p>
<h2 id="NCNN使用案例"><a href="#NCNN使用案例" class="headerlink" title="NCNN使用案例"></a>NCNN使用案例</h2><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nihui/ncnn-android-mobilenetssd">ncnn-android-mobilenetssd</a></li>
</ul>
<p>如果你想要在SDK&lt;24的情况下使用这个App，因为vulkan最低要求SDK是24，那怎么对下下来的项目进行修改，使其能用正常运行呢</p>
<p>首先你想到的是Ctrl + Shift + R，把ncnn-android-vulkan-lib全部替换成ncnn-android-lib，但当你build的时候，会遇到下面报错：</p>
<p>ninja: error: ‘C:/AndroidDev/ncnn-android-mobilenetssd/app/src/main/jni/ncnn-android-lib/armeabi-v7a/libglslang.a’, needed by ‘C:/AndroidDev/ncnn-android-mobilenetssd/app/build/intermediates/cmake/debug/obj/armeabi-v7a/libmobilenetssdncnn.so’, missing and no known rule to make it</p>
<p>简单说就是缺少<code>libglslang.a</code>， 但这个东西是vulkan才需要的，所以我们代码上要改成不需要这个，在CMakeLists.txt里注释掉</p>
<p>它：</p>
<p><img src="https://pic.downk.cc/item/5fb0c5efdf3cf7596d331566.jpg"></p>
<p>然后在mobilenetssdncnn_jni.cpp里注释掉一些报错的代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//    ncnn::create_gpu_instance();</span></span><br><span class="line"><span class="comment">//    ncnn::destroy_gpu_instance();</span></span><br><span class="line">    <span class="comment">// use vulkan compute</span></span><br><span class="line"><span class="comment">//    if (ncnn::get_gpu_count() != 0)</span></span><br><span class="line"><span class="comment">//        opt.use_vulkan_compute = true;</span></span><br><span class="line"><span class="comment">//    if (use_gpu == JNI_TRUE &amp;&amp; ncnn::get_gpu_count() == 0)</span></span><br><span class="line"><span class="comment">//    &#123;</span></span><br><span class="line"><span class="comment">//        return NULL;</span></span><br><span class="line"><span class="comment">//        //return env-&gt;NewStringUTF(&quot;no vulkan capable gpu&quot;);</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"><span class="comment">//        ex.set_vulkan_compute(use_gpu);</span></span><br></pre></td></tr></table></figure>
<p>在此解读一下ncnn android示例里的<strong>CMakeLists.txt</strong>：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import ncnn library</span></span><br><span class="line"><span class="keyword">add_library</span>(ncnn STATIC IMPORTED)</span><br><span class="line"><span class="comment"># change this folder path to yours</span></span><br><span class="line"><span class="keyword">set_target_properties</span>(ncnn PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libncnn.a)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="keyword">include</span>/ncnn)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(glslang STATIC IMPORTED)</span><br><span class="line"><span class="keyword">add_library</span>(OGLCompiler STATIC IMPORTED)</span><br><span class="line"><span class="keyword">add_library</span>(OSDependent STATIC IMPORTED)</span><br><span class="line"><span class="keyword">add_library</span>(SPIRV STATIC IMPORTED)</span><br><span class="line"><span class="keyword">set_target_properties</span>(glslang PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libglslang.a)</span><br><span class="line"><span class="keyword">set_target_properties</span>(OGLCompiler PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libOGLCompiler.a)</span><br><span class="line"><span class="keyword">set_target_properties</span>(OSDependent PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libOSDependent.a)</span><br><span class="line"><span class="keyword">set_target_properties</span>(SPIRV PROPERTIES IMPORTED_LOCATION <span class="variable">$&#123;CMAKE_SOURCE_DIR&#125;</span>/ncnn-android-vulkan-lib/<span class="variable">$&#123;ANDROID_ABI&#125;</span>/libSPIRV.a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># openmp</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -fopenmp&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fopenmp&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_SHARED_LINKER_FLAGS <span class="string">&quot;$&#123;CMAKE_SHARED_LINKER_FLAGS&#125; -fopenmp&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">DEFINED</span> ANDROID_NDK_MAJOR <span class="keyword">AND</span> <span class="variable">$&#123;ANDROID_NDK_MAJOR&#125;</span> <span class="keyword">GREATER</span> <span class="number">20</span>)</span><br><span class="line">    <span class="keyword">set</span>(CMAKE_SHARED_LINKER_FLAGS <span class="string">&quot;$&#123;CMAKE_SHARED_LINKER_FLAGS&#125; -static-openmp&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -fomit-frame-pointer -fstrict-aliasing -ffast-math&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fomit-frame-pointer -fstrict-aliasing -ffast-math&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_C_FLAGS <span class="string">&quot;$&#123;CMAKE_C_FLAGS&#125; -fvisibility=hidden&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fvisibility=hidden -fvisibility-inlines-hidden&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable rtti and exceptions</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -fno-rtti -fno-exceptions&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_library</span>(mobilenetssdncnn SHARED mobilenetssdncnn_jni.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(mobilenetssdncnn</span><br><span class="line">    ncnn</span><br><span class="line">    glslang SPIRV OGLCompiler OSDependent</span><br><span class="line">    android</span><br><span class="line">    z</span><br><span class="line">    log</span><br><span class="line">    jnigraphics</span><br><span class="line">    vulkan</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>add_library()这个命令顾名思义就是添加库，添加已有的库，但它的功能不止这个，它还可以先生成库再添加进去，cmake的<a target="_blank" rel="noopener" href="https://cmake.org/cmake/help/latest/command/add_library.html">官方帮助文档</a>也说明了这两点，ncnn这里前面几个用到的是<a target="_blank" rel="noopener" href="https://cmake.org/cmake/help/latest/command/add_library.html#imported-libraries">imported-libraries</a>的形式，一般这种形式的add_library后面都会跟着set_target_properties来指定PROPERTIES IMPORTED_LOCATION，而最后一个<code>add_library(mobilenetssdncnn SHARED mobilenetssdncnn_jni.cpp)</code>就是更常见的<a target="_blank" rel="noopener" href="https://cmake.org/cmake/help/latest/command/add_library.html#normal-libraries">Normal Libraries</a>形式，也就是将指定的cpp源文件先生成目标文件，然后添加到工程中去。</p>
<p>include_directories() 命令是为了让 CMake 在编译时期能找到头文件，里面传进去的就是头文件地址。</p>
<p><a target="_blank" rel="noopener" href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html">target_link_libraries()</a>会把 ncnn、glslang、SPIRV、OGLCompiler、OSDependent、android、z、log、jnigraphics、vulkan这些库都链接到mobilenetssdncnn身上，最后一起打包进libmobilenetssdncnn.so里。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201122154500293.png" alt="image-20201122154500293"></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nihui/ncnn-android-yolov5">ncnn-android-yolov5</a></li>
</ul>
<h1 id="加速ncnn"><a href="#加速ncnn" class="headerlink" title="加速ncnn"></a>加速ncnn</h1><ol>
<li><p><strong>使用bf16</strong></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/112564372">【official blog】</a></p>
</li>
</ol>
<p>bf16的首字母b是brain，因为这个格式是google-brain团队发明的。</p>
<p><strong>fp16</strong> V.S. <strong>fp32</strong> V.S. <strong>bf16</strong></p>
<p><img src="https://pic.downk.cc/item/5fb1372c1a64424b032677af.jpg"></p>
<p>就是把float 32bit后面的16bit直接砍掉，跟tflite的int8有点类似，不过int8砍的更猛</p>
<p>启用这个功能只需要在jni cpp里加上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.opt.use_packing_layout &#x3D; true;</span><br><span class="line">net.opt.use_bf16_storage &#x3D; true;</span><br></pre></td></tr></table></figure>
<p>就打开了。</p>
<p>以ncnn-android-yolov5为例：</p>
<p><img src="https://pic.downk.cc/item/5fb13821607ab2c3ed8bf1d8.jpg"></p>
<ol>
<li><p>图优化</p>
<p>图优化就是把融合、去除、替代网络结构中的层（如下图蓝框所示）。进入ncnn/build/tools目录，可以发现tools目录下存在ncnnoptimize的可执行文件，它的使用命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;ncnnoptimize [inparam] [inbin] [outparam] [outbin] [flag]</span><br></pre></td></tr></table></figure>
<p>最后那个参数flag是代表<code>storage_type</code>，从下图的源码（ncnn\tools\ncnnoptimize.cpp）截图可以看出，0代表fp32存储方式， 1代表fp16储存方式，</p>
</li>
</ol>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201121121202.png" alt="20201121121202" style="zoom:80%;" /></p>
<p>你可能看网上一些ncnnoptimize教程，还会出现flag=65536，同样从源码中可以看出其效果跟flag=1一样，都是fp16的存储方式。</p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/image-20201121135135709.png" alt="image-20201121135135709" style="zoom: 80%;" /></p>
<p>ncnnoptimize优化器是优化整个网络模型，即将一个模型匹配优化器中所有适用的优化方法，进而优化整个ncnn网络模型；而不是单独可选择的优化方法。</p>
<ol>
<li><p><strong>int 8 量化</strong></p>
<p>《<a target="_blank" rel="noopener" href="https://www.cnblogs.com/wanggangtao/p/11352948.html">NCNN量化之ncnn2table和ncnn2int8</a>》</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/71881443">《NCNN Conv量化详解（一）》</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/72375164">《NCNN量化详解（二）》</a></p>
<pre><code>[《yolov3：ncnn之int8量化》](https://zhuanlan.zhihu.com/p/299722824)

 量化工具：[EasyQuant](https://github.com/deepglint/EasyQuant)（[技术答疑](https://github.com/deepglint/EasyQuant/issues/3)）、[eq-ncnn](https://github.com/deepglint/eq-ncnn)
</code></pre></li>
<li><p><strong>手工优化ncnn模型结构</strong></p>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93017149?from_voters_page=true">https://zhuanlan.zhihu.com/p/93017149?from_voters_page=true</a></p>
<h1 id="其他NCNN技巧"><a href="#其他NCNN技巧" class="headerlink" title="其他NCNN技巧"></a>其他NCNN技巧</h1><ul>
<li>如何加密ncnn模型 : <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/268327784">https://zhuanlan.zhihu.com/p/268327784</a></li>
</ul>
<p>CSDN NCNN 专栏</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_31425585/category_9312419.html">https://blog.csdn.net/sinat_31425585/category_9312419.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/shanglianlm/category_9529596.html">https://blog.csdn.net/shanglianlm/category_9529596.html</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NCNN/" rel="tag">NCNN</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-YOLOV5"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/21/YOLOV5/"
    >YOLOV5</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/10/21/YOLOV5/" class="article-date">
  <time datetime="2020-10-21T14:26:53.000Z" itemprop="datePublished">2020-10-21</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="YOLOV5-Github-Repo"><a href="#YOLOV5-Github-Repo" class="headerlink" title="YOLOV5 Github Repo"></a><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5"><strong>YOLOV5 Github Repo</strong></a></h2><p><strong>推荐阅读：</strong></p>
<p>《<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/172121380"><strong>深入浅出Yolo系列之Yolov5核心基础知识完整讲解</strong></a>》</p>
<p><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183838757">《进击的后浪yolov5深度可视化解析》</a></strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/159371985">《目标检测之yolov5深度讲解》</a></strong></p>
<p>YOLOV5的特色：</p>
<ol>
<li>集成了混合精度训练</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/607">超参数可以自己演化</a></li>
<li>矩形图像训练和推理</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/1289">集成了Weights &amp; Biases</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/304">支持模型剪枝稀疏</a></li>
</ol>
<h2 id="经验与建议："><a href="#经验与建议：" class="headerlink" title="经验与建议："></a><strong>经验与建议</strong>：</h2><ol>
<li>下载pytorch和torchvision去官网获取命令行下载，不要自己输个pip install pytorch/torchvision就下载，很可能会报错。</li>
</ol>
<p><img src="https://pic.downk.cc/item/5fa632791cd1bbb86bdd8aef.png" width=70%></p>
<p>细心的你可能会发现通过conda和pip安装pytorch，命令行内容是不一样的，一个是torch一个是pytorch，这个问题我在这个<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/47333">issue</a>里问过pytorch的贡献者，总之不需要通过conda 和 pip安装两次。</p>
<p>如果通过这个命令行安装太慢，那你可以选择去清华镜像源网站（<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/）下载自己对应的版本，然后把压缩包放到`C:\ProgramData\Anaconda3\pkgs`里面，以管理员身份打开命令行窗口，cd到这里，然后运行`conda">https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/）下载自己对应的版本，然后把压缩包放到`C:\ProgramData\Anaconda3\pkgs`里面，以管理员身份打开命令行窗口，cd到这里，然后运行`conda</a> install —offline pytorch-1.7.0-py3.7_cuda101_cudnn7_0.tar.bz2<code>。torchvision以此类推。最后运行</code>python -m torch.utils.collect_env`检查一下安装是否成功（需要先安装typing_extensions）。</p>
<p>注意，通过pip来安装或升级torch和torchvision时，务必在最后加上<code>-f https://download.pytorch.org/whl/torch_stable.html</code>, 比如升级torchvision到0.8.1的命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torchvision&#x3D;&#x3D;0.8.1 -f https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.html --user</span><br></pre></td></tr></table></figure>
<ol>
<li><p>自己训练模型在准备数据集时，需要注意图片和标签的文件夹结构和命名，我采用的是像coco128.yaml一样的指定文件夹路径而不是指定txt文件路径，这样的话图片文件夹和标签文件夹需要命名为images和labels，且属于同级目录，子目录均包括train和val，相关的文档在<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data#3-organize-directories">这里</a>，相关的代码是dataset.py里的<code>img2label_paths()</code>和<code>LoadImagesAndLabels()</code>。因为我的这个数据集也会用nanodet训练，它们理论上应该共用一个图片文件夹，但是一开始nanodet的数据集格式是VOC，所以图片文件夹叫<code>JPEGImages</code>, 但是yolov5这边得叫images，为了不改nanodet的训练脚本，也不重复一遍图片，我用了软链接的方法，windows下软链接命令和运行成功的样子是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mklink &#x2F;d D:\MachineLearning\DataSet\SpecialVehicle\images D:\MachineLearning\DataSet\SpecialVehicle\JPEGImages</span><br><span class="line">为 D:\MachineLearning\DataSet\SpecialVehicle\images &lt;&lt;&#x3D;&#x3D;&#x3D;&gt;&gt; D:\MachineLearning\DataSet\SpecialVehicle\JPEGImages 创建的符号链接</span><br></pre></td></tr></table></figure>
<p>详细的windows软链接使用方法见<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37861937/article/details/79064841">这里</a>。</p>
</li>
<li><p>训练开始时会让你输入wandb API key，这个去<code>https://wandb.ai/settings</code>里找，复制过来后粘贴，粘贴完是不可见的，所以不要以为“咦怎么粘贴不了”。另外，当你使用yolov5训练新的目标检测任务时，请更改train.py里的wandb.init()里的project名，这样你在wandb网站上才能区分不同的目标检测任务。</p>
</li>
<li><p>你可能第一次训练完100个epochs后发现，曲线像下图这样还没平稳，感觉精度还可以更高，损失还可以更低：</p>
</li>
</ol>
<p><img src="https://pic.downk.cc/item/5fa6329d1cd1bbb86bdd9188.png"></p>
<p>然后你想重新训练，可以的。</p>
<p>第一，修改命令行的epochs为你第二次想要训练的次数；</p>
<p>第二，修改超参数。打开hyp.custom.yaml,里面有一堆超参数让我们设置，最下面几个是数据增强，第二次重新训练不需要改变，要改变的是 <code>lr0</code> 和 <code>warmup_epochs</code> 。<code>lr0</code>要改成多少，要看你上次训练结束时的学习率是多少，从下图可以看出我这里是0.002，所以<code>lr0</code>从0.01改成0.002，然后<code>warmup_epochs</code>改成0，设成0后，就是说我们第二次就不需要这个热身的过程了，因为我们的意图很明显，就是让曲线顺延着第一次训练结果走下去，如果热身的话，学习率就会像下图那样会有个短期的下降。这个warm up就是让你的学习率一开始以较小的值训练，经过几个epoch后，学习率再恢复到你设置的<code>lr0</code>学习率，这就是图中曲线一开始是一个爬坡样子的原因。</p>
<p><img src="https://pic.downk.cc/item/5fa632b31cd1bbb86bdd9551.png"></p>
<p>如果第二次训练不改这两个超参数会怎么样呢？会导致你第二次所有epoch跑完了，检测效果跟第一次结束时差不多，甚至更糟糕！</p>
<p>其他参数,<code>momentum</code>是控制历史权重值的影响因子，一般是0.9几；<code>weight_decay</code> 是加在正则项前面的，设置得大一点可以减小过拟合。</p>
<p>这么一直接着训练下去，怎么判断模型是否到底过拟合了没有呢？我是看验证集val的曲线，只要它还没出现损失上升，就认为它还没过拟合</p>
<p><img src="https://pic.downk.cc/item/5fa632c81cd1bbb86bdd9980.png"></p>
<p>4.我们可以利用训练好的模型去检测新增的数据集，将检测结果导出生成txt文件，然后在labelimg里再把annotation的保存目录改到txt文件保存的目录就可以了，这样labelimg就会把检测结果矩形框显示出来，你只需要调整一下矩形框，就可以完成标注了，实现半自动化标注。</p>
<p>作者<strong><a target="_blank" rel="noopener" href="https://github.com/glenn-jocher">glenn-jocher</a></strong>说：</p>
<blockquote>
<p>如果你是想把检测结果导出为txt格式的文件，使用下面代码：<br><code>python detect.py --save-txt</code></p>
<p>如果你是想把检测结果导出为coco json格式的文件，使用下面代码：<br><code>python test.py --save-json</code></p>
</blockquote>
<p>不过我看了一下<code>detect.py</code>和<code>test.py</code>的源码，发现两者都支持导出json，但只有detext.py支持导出txt，然后，两者指定输入图片的位置也不太一样，detect.py通过<code>--source</code>指定输入图片位置，比较好理解，默认位于<code>inference/images</code>，而test.py是通过<code>--data</code>指定输入图片位置，两者都可以通过<code>--save-dir</code>指定输出文件位置，但<code>detect.py</code>的输出位置默认位于<code>inference/output</code>， <code>test.py</code>的输出位置默认位于<code>runs/test</code>。</p>
<p><code>detect.py</code>  :</p>
<p><img src="https://pic.downk.cc/item/5fa635511cd1bbb86bde1741.jpg"></p>
<p>有两个参数比较有意思，一个是 <code>--classes</code>，可用来过滤不想要的类别，一个是<code>--augment</code>，用于检测时提高检测结果的准确度，相关的讨论在<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/303">这里</a>， 更多TTA的资料（<a target="_blank" rel="noopener" href="https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it">link1</a>， <a target="_blank" rel="noopener" href="https://github.com/qubvel/ttach">link2</a>）。</p>
<p><code>test.py</code> ：</p>
<p><img src="https://pic.downk.cc/item/5fa635c61cd1bbb86bde2bbf.jpg"></p>
<p>我这边实际运行的命令行是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python detect.py --weights weights&#x2F;iguard-best.pt --source D:\MachineLearning\DataSet\iGuardDataset\IMAGE\temp --save-dir D:\MachineLearning\DataSet\iGuardDataset\IMAGE\temp\labels --save-txt --augment</span><br></pre></td></tr></table></figure>
<p>导出的结果如下所示，我看了一下效果很好：</p>
<p><img src="https://pic.downk.cc/item/5fabe49d1cd1bbb86b0a3641.jpg"></p>
<p>现在的yolov5（2020年11月12日）导出的txt有问题，在每一行最后面它都有个空格，这在LabelImg里打开是会报错的，得把这个空格删掉，我提了个<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/1355">issue</a>告诉作者这个问题，后来作者让我PR了，再后来我发现 —source 跟 —save-dir 不能是同一个目录，否则代码会删除所有 —source 里的图片，不过那时候作者告诉我yolov5的文件结构已经重新设计了，大家请自己去看一下最新的detect.py里的参数。</p>
<ol>
<li>如果你的epochs设置的很大，但你看到val loss还没结束就已经升高很多了，于是你中断了训练，这时候模型文件会比预训练模型大很多，因为为了后面你resume，模型文件里还保存了下次接着训练的数据，所以你确定后面不会再训练的话，可以把模型里的多余数据删除，方法是在根目录下新建一个py脚本，运行以下代码：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils.general <span class="keyword">import</span> strip_optimizer</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">save_dir = <span class="string">r&#x27;E:/Repo/yolov5/runs/train/exp&#x27;</span></span><br><span class="line"></span><br><span class="line">last = os.path.join(save_dir, <span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;last.pt&#x27;</span>)</span><br><span class="line">best = os.path.join(save_dir, <span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;best.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#strip_optimizer(last)  # 一般resume是从加载last.pt开始，如果best.pt够用的话，last.pt就先留着</span></span><br><span class="line">strip_optimizer(best)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># actual_anchors.py</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">r&quot;E:\Repo\yolov5\runs\train\exp\weights\best.pt&quot;</span>)[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line"></span><br><span class="line">m = model.model[-<span class="number">1</span>]  <span class="comment"># Detect()</span></span><br><span class="line">m.anchors  <span class="comment"># in stride units</span></span><br><span class="line">m.anchor_grid  <span class="comment"># in pixel units</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(m.anchor_grid.view(-<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h2 id="疑问与解释"><a href="#疑问与解释" class="headerlink" title="疑问与解释"></a><strong>疑问与解释</strong></h2><ol>
<li>10月份的比7月份的版本在models/yolov5s.yaml里面的backbone多了   [-1, 3, BottleneckCSP, [1024, False]]，这一行是什么意思，什么作用，有什么影响？</li>
</ol>
<ol>
<li><p><strong>yolov5在哪里配置数据增强？</strong></p>
<p> SSD是在config里通过选择数据增强项来配置数据增强用哪些，yolov5的数据增强配置在<code>data/hyp.scratch.yaml</code>里,这里除了有数据增强的参数外，还有诸如学习率等超参数的配置。yolov5默认支持并应用多种图像增强：</p>
<p> <img src="https://pic.downk.cc/item/5f9ce17c1cd1bbb86b89a76d.jpg" width =70%></p>
</li>
<li><p><strong>怎样减小false positive？</strong></p>
<p> false positive高的话就是说图像中没目标却把一些背景识别成目标，这个issue里讨论了这个问题，下面的兄弟的做法是：<br> <img src="https://pic.downk.cc/item/5fa632de1cd1bbb86bdd9db0.png"></p>
<p> （1）把无目标的图片放到images文件夹里</p>
<p> （2）创建一个同名的txt文件，里面是空的</p>
<p> 但是也不宜过多，因为带目标的图片里那些背景部分其实就是很多的负样本了。</p>
</li>
<li><p><strong>为什么yolov5使用GIOU而不是使用CIOU？</strong></p>
<p> 这个问题在这个<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/1113">issue</a>里也有讨论</p>
</li>
<li><p><strong>yolov5是如何实现输入矩形图像进行训练和推理的？</strong></p>
<p> mobilenetssd要求训练和推理的输入图片得是正方形，但yolov5不一样，推理时默认就是输入矩形图像（填充到长宽是32的最小倍数），因为作者glenn-jocher发现把输入图像处理为32倍数的矩形图像比处理成正方形图像更省推理时间【<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov3/issues/232">相关issue</a>】，而且转换成onnx格式后依然支持矩形推理【<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov3/issues/232#issuecomment-597760790">相关comment</a>】。<br> 矩形推理的图像处理思路是：把图像的长边缩放成<code>img_size</code>规定的大小,然后短边填充到一个最小32倍数的值。</p>
<p> 训练的话也可以做到，但需要你输入命令行参数进行确认，在train.py里有这么一句：</p>
<p> <code>parser.add_argument(&#39;--rect&#39;, action=&#39;store_true&#39;, help=&#39;rectangular training&#39;)</code></p>
<p> 从<code>action=&#39;store_true&#39;</code>可知，你只要在命令行里加上<code>--rect</code>(不需要输 <code>--rect True</code>)就可以进行矩形图像训练了。</p>
<p> 矩形训练时的预处理比矩形推理时的预处理更复杂一些，因为训练时需要先找到这一个batch里面最方形的那张图像，然后按上面所说的思路把这张最方形图像变成矩形后，接下来所有同批次的图像就要变成跟它一样大小。</p>
<p> 但这种处理方式有个小漏洞，就是当一个批次里，有一张图像很正方形时，那么最后整个batch里的所有图像都会被填充成类正方形【<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov3/issues/232#issuecomment-562870597">相关comment</a>】。</p>
<p> yolov5/utils/dataloader.py里有个<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/blob/481d46cffb0b7a2ec5cec76d9ec85357128b28ea/utils/datasets.py#L721">letterbox()</a>函数，就是用来预处理图像供神经网络矩形训练和矩形推理，维基百科对letterbox的释义是：</p>
<p> <img src=https://img-blog.csdnimg.cn/20201030101451732.png width=50%></p>
<p> 不过yolov5这里不是填充纯黑色，从letterbox()函数里的color=(114, 114, 114)可知是用灰色填充。</p>
</li>
<li><p>怎么看训练时runs文件夹里的labels.jpg那张图？</p>
<p> <img src="https://pic.downk.cc/item/5fa632371cd1bbb86bdd7757.png" width=70%></p>
<p> runs里的这张labels图展示了标签的分布情况，第一幅图展示的是数据集各个类别的标签数量，我只有一个类型，且其数目有4000多个，第二幅图展示的是标签/矩形框的中心点在图片中的位置，我这里的图说明大部分矩形框分布在中心，但四周的分布也有不少，总体比较平均，第三幅图说明的是矩形框的宽和高相对原图的宽和高的比例，我这里的类别是香烟，目标比较小，所以大部分是（0.05，0.05）左右。</p>
</li>
<li><p><strong>如何将yolov5的模型转换成ncnn的格式和tflite格式</strong></p>
</li>
</ol>
<p>转换成ncnn格式：作者告诉我有两种方式<a target="_blank" rel="noopener" href="https://github.com/cmdbug/YOLOv5_NCNN/issues/26">https://github.com/cmdbug/YOLOv5_NCNN/issues/26</a></p>
<p>文中谈到yolo focus op在转换时会有问题，focus的示意图和代码如下，代码中像<code>x[..., 1::2, ::2]</code>等4个切片操作会生成下图4种颜色的tensor，这里面的<code>1: :2</code> 或<code>: : 2</code> 的含义就是 <code>start:stop:step</code> ，但第一个<code>...</code>我不确定是什么，感觉跟 <code>: : :</code> 一样，代表所有channel。</p>
<p><a target="_blank" rel="noopener" href="https://imgchr.com/i/Bof1fg"><img src="https://s1.ax1x.com/2020/11/08/Bof1fg.png" alt="Bof1fg.png"></a></p>
<p>在<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.13630.pdf">TResNet paper</a>. p2. 它又被叫做 SpaceToDepth</p>
<p>代码演示：<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/804#issuecomment-678141008">https://github.com/ultralytics/yolov5/issues/804#issuecomment-678141008</a></p>
<ol>
<li>yolov5使用的LambdaLR的工作原理是什么</li>
</ol>
<p>相关代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># train.py</span><br><span class="line">    if opt.linear_lr:</span><br><span class="line">        lf &#x3D; lambda x: (1 - x &#x2F; (epochs - 1)) * (1.0 - hyp[&#39;lrf&#39;]) + hyp[&#39;lrf&#39;]  # linear</span><br><span class="line">    else:</span><br><span class="line">        lf &#x3D; one_cycle(1, hyp[&#39;lrf&#39;], epochs)  # cosine 1-&gt;hyp[&#39;lrf&#39;]</span><br><span class="line">    scheduler &#x3D; lr_scheduler.LambdaLR(optimizer, lr_lambda&#x3D;lf)</span><br><span class="line">    </span><br><span class="line"># general.py </span><br><span class="line">def one_cycle(y1&#x3D;0.0, y2&#x3D;1.0, steps&#x3D;100):</span><br><span class="line">    # lambda function for sinusoidal ramp from y1 to y2</span><br><span class="line">    return lambda x: ((1 - math.cos(x * math.pi &#x2F; steps)) &#x2F; 2) * (y2 - y1) + y1</span><br></pre></td></tr></table></figure>
<p>当y1=1.0， y2=0.2时，lambda x: ((1 - math.cos(x <em> math.pi / steps)) / 2) </em> (y2 - y1) + y1的函数图如下：</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_16_46_35_image-20210325164632607.png" alt="image-20210325164632607"></p>
<p>从wandb上的学习率变化图可以看出，学习率在热身阶段先从极小值升高到0.01，再按sin函数下降到0.002。这两个值看起来是不是是上面函数的极值1和0.2的0.01倍。</p>
<p><img src="https://gitlab.com/wwdok/my-image-bed/-/raw/master/pictures/2021/03/25_16_51_38_image-20210325165136718.png" alt="image-20210325165136718"></p>
<p>因为一般学习率主要还受optimizer里设置的学习率的影响：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># train.py</span><br><span class="line">if opt.adam:</span><br><span class="line">        optimizer &#x3D; optim.Adam(pg0, lr&#x3D;hyp[&#39;lr0&#39;], betas&#x3D;(hyp[&#39;momentum&#39;], 0.999))  # adjust beta1 to momentum</span><br><span class="line">    else:</span><br><span class="line">        optimizer &#x3D; optim.SGD(pg0, lr&#x3D;hyp[&#39;lr0&#39;], momentum&#x3D;hyp[&#39;momentum&#39;], nesterov&#x3D;True)</span><br></pre></td></tr></table></figure>
<p><strong>转换为TFLite格式</strong>：</p>
<p>这里使用<a target="_blank" rel="noopener" href="https://github.com/zldrobit">zldrobit</a> 开源的<a target="_blank" rel="noopener" href="https://github.com/zldrobit/yolov5/tree/tf-android">fork yolov5的项目</a>，相关的话题在这个<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/pull/1127">issue</a>里。前面的步骤请阅读README.md，这里从下面的内容开始：</p>
<p>注意，在windows上面，执行以下语句前需要先执行<code>set PYTHONPATH=.</code>，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\MachineLearning\CV\Object_Detection\yolov5-tf-android&gt;set PYTHONPATH&#x3D;.</span><br></pre></td></tr></table></figure>
<p>不执行这句的话，会遇到报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;models&#x2F;tf.py&quot;, line 17, in &lt;module&gt;</span><br><span class="line">    from models.common import Conv, Bottleneck, SPP, DWConv, Focus, BottleneckCSP, Concat, autopad, C3</span><br><span class="line">ModuleNotFoundError: No module named &#39;models&#39;</span><br></pre></td></tr></table></figure>
<ul>
<li>Export TensorFlow models (GraphDef and saved model) using:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python models&#x2F;tf.py --weights weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --img 640</span><br></pre></td></tr></table></figure>
<ul>
<li>Export non-quantized TFLite models using:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml</span><br></pre></td></tr></table></figure>
<p>or</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --no-tfl-detect</span><br></pre></td></tr></table></figure>
<p>with <code>--no-tfl-detect</code>, the generated TFLite model can be used in the Android project.</p>
<ul>
<li>Export quantized TFLite models using:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --no-tfl-detect --tfl-int8 --source D:\MachineLearning\DataSet\coco\val2017 --ncalib 100</span><br></pre></td></tr></table></figure>
<p>导出的模型位于 <code>--weight</code>的同级目录下。</p>
<p>这句代码我在执行过程中遇到了Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED的报错，参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40394402/article/details/109012083">这里</a>解决了问题，解决方法是在tf.py文件里、import tensorflow as tf后面加上下面代码块里的后两行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">if tf.__version__.startswith(&#39;1&#39;):</span><br><span class="line">    tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line">gpus &#x3D; tf.config.experimental.list_physical_devices(device_type&#x3D;&#39;GPU&#39;)</span><br><span class="line">tf.config.experimental.set_memory_growth(gpus[0], True)</span><br></pre></td></tr></table></figure>
<p>转换过程中输出的模型结构信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">Model: &quot;functional_1&quot;</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">input_1 (InputLayer)            [(None, 640, 640, 3) 0</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__focus (tf_Focus)            (None, 320, 320, 32) 3584        input_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_1 (tf_Conv)            (None, 160, 160, 64) 18688       tf__focus[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp (tf_Bottlene (None, 160, 160, 64) 20352       tf__conv_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_6 (tf_Conv)            (None, 80, 80, 128)  74240       tf__bottleneck_csp[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_1 (tf_Bottle (None, 80, 80, 128)  162560      tf__conv_6[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_15 (tf_Conv)           (None, 40, 40, 256)  295936      tf__bottleneck_csp_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_2 (tf_Bottle (None, 40, 40, 256)  644608      tf__conv_15[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_24 (tf_Conv)           (None, 20, 20, 512)  1181696     tf__bottleneck_csp_2[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf_spp (tf_SPP)                 (None, 20, 20, 512)  658432      tf__conv_24[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_3 (tf_Bottle (None, 20, 20, 512)  1252352     tf_spp[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_31 (tf_Conv)           (None, 20, 20, 256)  132096      tf__bottleneck_csp_3[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__upsample (tf_Upsample)      (None, 40, 40, 256)  0           tf__conv_31[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat (tf_Concat)          (None, 40, 40, 512)  0           tf__upsample[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_2[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_4 (tf_Bottle (None, 40, 40, 256)  380416      tf__concat[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_36 (tf_Conv)           (None, 40, 40, 128)  33280       tf__bottleneck_csp_4[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__upsample_1 (tf_Upsample)    (None, 80, 80, 128)  0           tf__conv_36[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat_1 (tf_Concat)        (None, 80, 80, 256)  0           tf__upsample_1[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_5 (tf_Bottle (None, 80, 80, 128)  96000       tf__concat_1[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_41 (tf_Conv)           (None, 40, 40, 128)  147968      tf__bottleneck_csp_5[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat_2 (tf_Concat)        (None, 40, 40, 256)  0           tf__conv_41[0][0]</span><br><span class="line">                                                                 tf__conv_36[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_6 (tf_Bottle (None, 40, 40, 256)  314880      tf__concat_2[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__conv_46 (tf_Conv)           (None, 20, 20, 256)  590848      tf__bottleneck_csp_6[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__concat_3 (tf_Concat)        (None, 20, 20, 512)  0           tf__conv_46[0][0]</span><br><span class="line">                                                                 tf__conv_31[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__bottleneck_csp_7 (tf_Bottle (None, 20, 20, 512)  1252352     tf__concat_3[0][0]</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">tf__detect (tf_Detect)          ((1, 25200, 85), [(N 229245      tf__bottleneck_csp_5[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_6[0][0]</span><br><span class="line">                                                                 tf__bottleneck_csp_7[0][0]</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">Total params: 7,489,533</span><br><span class="line">Trainable params: 7,468,157</span><br><span class="line">Non-trainable params: 21,376</span><br></pre></td></tr></table></figure>
<p>2021/3/22更新：今天我再执行以上步骤时，发现Total params等3个参数量变了，模型的输入尺寸也从之前的640变成了现在的320。</p>
<p>因为我们命令行里指定校准的图片的数量是100张，所以可以看到下图的输出，代码在用val2017里的前100张图片来校准：</p>
<p><img src="https://pic.downk.cc/item/5fa955611cd1bbb86b8bcc2a.jpg"></p>
<p>最后成功导出以下模型：</p>
<p><img src="https://pic.downk.cc/item/5fa954c21cd1bbb86b8ba7b6.jpg" width=70%></p>
<p>检测的结果默认输出在inference/output文件夹，来看看检测的效果，可以看出对小目标的检测效果还不错，就是执行时间很长，一张图片推理需要86秒：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">C:\MachineLearning\CV\Object_Detection\yolov5-tf-android&gt;python detect.py --weight weights&#x2F;yolov5s-int8.tflite --tfl-int8 --img 320</span><br><span class="line">Namespace(agnostic_nms&#x3D;False, augment&#x3D;False, cfg&#x3D;&#39;.&#x2F;models&#x2F;yolov5s.yaml&#39;, classes&#x3D;None, conf_thres&#x3D;0.4, device&#x3D;&#39;&#39;, img_size&#x3D;[640], iou_thres&#x3D;0.5, no_tf_nms&#x3D;False, output&#x3D;&#39;inference&#x2F;output&#39;, save_conf&#x3D;False, save_dir&#x3D;&#39;inference&#x2F;output&#39;, save_txt&#x3D;False, source&#x3D;&#39;inference&#x2F;images&#39;, tfl_detect&#x3D;True, tfl_int8&#x3D;True, update&#x3D;False, view_img&#x3D;False, weights&#x3D;[&#39;weights&#x2F;yolov5s-int8.tflite&#39;])</span><br><span class="line">Using CUDA device0 _CudaDeviceProperties(name&#x3D;&#39;GeForce GTX 1660 Ti&#39;, total_memory&#x3D;6144MB)</span><br><span class="line"></span><br><span class="line">2020-11-09 22:05:35.711239: I tensorflow&#x2F;stream_executor&#x2F;platform&#x2F;default&#x2F;dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll</span><br><span class="line">image 1&#x2F;3 C:\MachineLearning\CV\Object_Detection\yolov5-tf-android\inference\images\IMG_20201107_120437_DRO.jpg: 640x640 5 persons, 4 motorcycles, 2 buss, Done. (86.168s)</span><br><span class="line">image 2&#x2F;3 C:\MachineLearning\CV\Object_Detection\yolov5-tf-android\inference\images\bus.jpg: 640x640 4 persons, 1 buss, Done. (86.730s)</span><br><span class="line">image 3&#x2F;3 C:\MachineLearning\CV\Object_Detection\yolov5-tf-android\inference\images\zidane.jpg: 640x640 2 persons, 1 ties, Done. (85.811s)</span><br><span class="line">Results saved to inference\output</span><br><span class="line">Done. (346.666s)</span><br></pre></td></tr></table></figure>
<p><img src="https://pic.downk.cc/item/5fa957341cd1bbb86b8c3239.jpg"></p>
<p>TFLite export is only supported under TensorFlow 2.3.0.</p>
<p><code>auto</code> controls whether to pad a resized input image to square. If True, input image is rectangle, otherwise, imput image is square.</p>
<p>TensorFlow does not require square inference, but require fixed input size.<br>Set <code>auto=False</code> to assure the padded image size equals <code>new_shape</code> in <code>letterbox</code>:</p>
<p><img src="https://pic.downk.cc/item/5fa7f2a11cd1bbb86b416a07.jpg"></p>
<p>Thus, the input image sizes after preprocess are the same.</p>
<p>The input size is fixed while exporting TensorFlow and TFLite:</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/blob/23fe35efeb09e366b142e5a4757031f9b208f528/models/tf.py#L394-L395">yolov5/models/tf.py</a></p>
<p>Lines 394 to 395 in <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/commit/23fe35efeb09e366b142e5a4757031f9b208f528">23fe35e</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = keras.Input(shape=(*opt.img_size, <span class="number">3</span>)) </span><br><span class="line">keras_model = keras.Model(inputs=inputs, outputs=tf_model.predict(inputs)) </span><br></pre></td></tr></table></figure>
<p>Take COCO dataset for example, in int8 calibration, if one use <code>auto=True</code>, different size images will be fed to TFLite model<br>while model’s input size is fixed.<br>For rectangle inference of TFLite int8 calibration, one could set <code>--img-size</code> to a rectangle (e.g. 640x320) while setting <code>auto=False</code>.</p>
<p>You could change it to multiples of 32 using <code>--img</code>.<br>For example, use</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --img 320 --no-tfl-detect --tfl-int8 --source &#x2F;data&#x2F;dataset&#x2F;coco&#x2F;coco2017&#x2F;train2017 --ncalib 100</span><br></pre></td></tr></table></figure>
<p>to generate TF and TFLite models.<br>Then, use one of</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python detect.py --weight weights&#x2F;yolov5s.pb --img 320</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s_saved_model&#x2F; --img 320</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s-fp16.tflite --img 320 --tfl-detect</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s-int8.tflite --img 320 --tfl-int8 --tfl-detect</span><br></pre></td></tr></table></figure>
<p>to detect objects.</p>
<p>Or put the TFLite models to <code>asset</code> folder of Android project, and replace</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/blob/eb626a611aba0a83535ff72e9581014be8402a59/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java#L32-L33">yolov5/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java</a></p>
<p>Lines 32 to 33 in <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/commit/eb626a611aba0a83535ff72e9581014be8402a59">eb626a6</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputSize &#x3D; 640; </span><br><span class="line">output_width &#x3D; new int[]&#123;80, 40, 20&#125;; </span><br></pre></td></tr></table></figure>
<p>and</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/blob/eb626a611aba0a83535ff72e9581014be8402a59/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java#L42-L43">yolov5/android/app/src/main/java/org/tensorflow/lite/examples/detection/tflite/DetectorFactory.java</a></p>
<p>Lines 42 to 43 in <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/commit/eb626a611aba0a83535ff72e9581014be8402a59">eb626a6</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputSize &#x3D; 640; </span><br><span class="line">output_width &#x3D; new int[]&#123;80, 40, 20&#125;; </span><br></pre></td></tr></table></figure>
<p>with</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputSize &#x3D; 320;</span><br><span class="line">output_width &#x3D; new int[]&#123;40, 20, 10&#125;;</span><br></pre></td></tr></table></figure>
<p>to build and run the Android project.<br>This reduces around</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">75% time of fp16 model inference on Snapdragon 820 CPU (4 threads) from 1.9s to 0.5s, </span><br><span class="line">70%                              on Snapdragon 820 GPU from 1.3s to 0.4s, </span><br><span class="line">70%      of int8                 on Snapdargon 820 CPU (4 threads) from 1.7s to lesser than 0.5s. </span><br></pre></td></tr></table></figure>
<p>It is possible to export to tflite in a shape of the 16:9 aspect ratio like 320 : 192</p>
<p>Run</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python  models&#x2F;tf.py --weight weights&#x2F;yolov5s.pt --cfg models&#x2F;yolov5s.yaml --img 320 192 --no-tfl-detect --tfl-int8 --source &#x2F;data&#x2F;dataset&#x2F;coco&#x2F;coco2017&#x2F;train2017 --ncalib 100</span><br></pre></td></tr></table></figure>
<p>to export a TFLite model of 320 vertical by 192 horizontal input , and run one of</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python detect.py --weight weights&#x2F;yolov5s-int8.tflite --img 320 192 --tfl-detect --tfl-int8</span><br><span class="line">python detect.py --weight weights&#x2F;yolov5s-fp16.tflite --img 320 192 --tfl-detect</span><br></pre></td></tr></table></figure>
<p>To access current model anchors:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">model &#x3D; torch.load(&#39;yolov5s.pt&#39;)[&#39;model&#39;]</span><br><span class="line">m &#x3D; model.model[-1]  # Detect()</span><br><span class="line">m.anchors  # in stride units</span><br><span class="line">m.anchor_grid  # in pixel units</span><br><span class="line"></span><br><span class="line">print(m.anchor_grid.view(-1,2))</span><br><span class="line">#tensor([[ 10.,  13.],</span><br><span class="line">        [ 16.,  30.],</span><br><span class="line">        [ 33.,  23.],</span><br><span class="line">        [ 30.,  61.],</span><br><span class="line">        [ 62.,  45.],</span><br><span class="line">        [ 59., 119.],</span><br><span class="line">        [116.,  90.],</span><br><span class="line">        [156., 198.],</span><br><span class="line">        [373., 326.]])</span><br></pre></td></tr></table></figure>
<h1 id="YOLOV5-V-S-YOLOV4"><a href="#YOLOV5-V-S-YOLOV4" class="headerlink" title="YOLOV5 V.S. YOLOV4"></a>YOLOV5 V.S. YOLOV4</h1><p>YOLOV4 V.S. YOLOV5 的<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=H4Wa6QY28e8">对比视频</a>，从视频看，准确率可能相差不明显，但v5的帧率比v4快了很多。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/" rel="tag">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-MQTT"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/11/MQTT/"
    >MQTT</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/10/11/MQTT/" class="article-date">
  <time datetime="2020-10-11T02:47:20.000Z" itemprop="datePublished">2020-10-11</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%89%A9%E8%81%94%E7%BD%91/">物联网</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="MQTT简介"><a href="#MQTT简介" class="headerlink" title="MQTT简介"></a><strong>MQTT简介</strong></h1><p>MQTT(Message Queuing Telemetry Transport)</p>
<p>MQTT是一个基于TCP/IP协议簇实现的应用层协议（UDP版本的是MQTT-SN），在Mosquitto中底层的通讯是基于socket实现的。当然，你也可以使用Websocket。这里我不做赘述，虽然两者名字类似但就像Java和Javascript一样实际这是两个完全不同的概念。</p>
<p>topic的形式有点像路径，使用/分割，代表层级。</p>
<h2 id="MQTT的服务质量QoS（Quality-of-Service）"><a href="#MQTT的服务质量QoS（Quality-of-Service）" class="headerlink" title="MQTT的服务质量QoS（Quality of Service）"></a>MQTT的服务质量QoS（Quality of Service）</h2><p>MQTT发布消息QoS保证不是 发布端 到 订阅端 的，是客户端（发布端/订阅端）与服务器之间的，即QoS的作用域分为发布端的QoS 和 订阅端的QoS。订阅端收到MQTT消息的QoS级别，最终取决于发布消息的QoS和主题订阅的QoS。简单说，就是取最低的消息服务质量。</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0qHKWF.png" alt="0qHKWF.png" border="0" width=70%/></p>
<p><strong>QoS 0 消息发布订阅</strong></p>
<p>level 0：只发送一次，不保证消息一定送达。</p>
<p><img src=https://docs.emqx.net/broker/latest/cn/development/_assets/qos0_seq.png width=70%></p>
<p><strong>QoS 1 消息发布订阅</strong></p>
<p>level 1：保证信息会被接收端收到，但可能接收端会重复收到消息。<br>发送端会存储发送的publish信息，直到接收端返回puback应答。<br>publish与puback之间通过比较数据包中的packet identifier完成。<br>在特定的时间内（timeout），发送端没有接收到puback应答，那么发送端就会重新发送publish消息。[<a target="_blank" rel="noopener" href="http://blog.sina.com.cn/s/blog_a5e78d1d0102wqkr.html">来源</a>]</p>
<p><img src=https://docs.emqx.net/broker/latest/cn/development/_assets/qos1_seq.png width=70%></p>
<p><strong>QoS 2 消息发布订阅</strong></p>
<p>level 2：确保消息只被接收到一次，因频繁确认以至于效率低。</p>
<p><img src=https://docs.emqx.net/broker/latest/cn/development/_assets/qos2_seq.png></p>
<h2 id="MQTT连接保活心跳"><a href="#MQTT连接保活心跳" class="headerlink" title="MQTT连接保活心跳"></a>MQTT连接保活心跳</h2><p>MQTT客户端向服务器发起CONNECT请求时，通过KeepAlive参数设置保活周期。</p>
<p>客户端在无报文发送时，按KeepAlive周期定时发送2字节的PINGREQ心跳报文，服务端收到PINGREQ报文后，回复2字节的PINGRESP报文。</p>
<p>服务端在1.5个心跳周期内，既没有收到客户端发布订阅报文，也没有收到PINGREQ心跳报文时，主动心跳超时断开客户端TCP连接。</p>
<h1 id="MQTT服务端"><a href="#MQTT服务端" class="headerlink" title="MQTT服务端"></a>MQTT服务端</h1><p>MQTT系统中分服务端和客户端，我们先介绍服务端。MQTT的客户端有很多可以选（详细的MQTT Broker 选型请见<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/cf91f4bea071">这篇博客</a>。），如果是小项目，用mosquitto即可，如果是有很多客户端，数据传输量大，则可以考虑用国产的EMQ。我们这里主要介绍mosquitto。</p>
<h2 id="MOSQUITTO的安装使用"><a href="#MOSQUITTO的安装使用" class="headerlink" title="MOSQUITTO的安装使用"></a>MOSQUITTO的安装使用</h2><h3 id="将mosquitto安装在树莓派-Linux上"><a href="#将mosquitto安装在树莓派-Linux上" class="headerlink" title="将mosquitto安装在树莓派/Linux上"></a><strong>将mosquitto安装在树莓派/Linux上</strong></h3><p>[<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=4LnxB_iJ380">教程1</a>]<br>[<a target="_blank" rel="noopener" href="https://bitluni.net/simple-mqtt-broker-setup-on-a-raspberry-pi">教程2</a>]</p>
<p>打开树莓派终端，依次输入以下命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<p>安装mosquitto 和 mosquitto-clients，mosquitto的作用是充当服务端，mosquitto-clients的作用是充当客户端，为了后面方便调试，我们两者都安装，反正包大小也不到。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y mosquitto mosquitto-clients</span><br></pre></td></tr></table></figure><br>设置开机自动启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable mosquitto.service</span><br></pre></td></tr></table></figure><br>测试创建发布者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mosquitto_pub -t home_automation&#x2F;bedroom&#x2F;light -m &#39;helloWorld&#39;</span><br></pre></td></tr></table></figure><br>打开另一个终端，测试创建订阅者<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mosquitto_sub  -v -t home_automation&#x2F;bedroom&#x2F;light</span><br></pre></td></tr></table></figure></p>
<p>你还可以在windows上跟树莓派通过MQTT通信，<br>首先在windows上安装WSL或Debian GNU Linux，然后也是执行<code>sudo apt-get update</code>  和 <code>sudo apt install -y mosquitto mosquitto-clients</code></p>
<p>最后在<code>mosquitto_pub -t homeautomation/bedroom/light -m &#39;hello from my laptop&#39;</code>里面加入 <code>-h IP</code>， IP就是树莓派的局域网IP地址.</p>
<p>获取树莓派IP。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hostname -I</span><br></pre></td></tr></table></figure></p>
<p>在windows电脑上，除了在linux子系统安装mosquitto-clients可以用来模拟一个客户端，还可以通过安装桌面软件来模拟一个客户端。这类桌面软件有<a target="_blank" rel="noopener" href="http://mqtt-explorer.com/">MQTT explorer</a>、<a target="_blank" rel="noopener" href="https://github.com/emqx/MQTTX/blob/master/README-CN.md">MQTTX</a>，我用过MQTTX，挺不错的，推荐使用。</p>
<p><img src=https://s1.ax1x.com/2020/10/20/BpvDTx.png></p>
<h3 id="将mosquitto安装在windows电脑上"><a href="#将mosquitto安装在windows电脑上" class="headerlink" title="将mosquitto安装在windows电脑上"></a><strong>将mosquitto安装在windows电脑上</strong></h3><p><a target="_blank" rel="noopener" href="https://mosquitto.org/download/"><strong>mosquitto下载地址</strong></a></p>
<p>下载安装包：</p>
<p><img src="https://pic.downk.cc/item/5f8adb131cd1bbb86b10c1fa.jpg"></p>
<p>搜索“服务”：</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0LLMgU.png" alt="0LLMgU.png" border="0" /></p>
<p>点击启动此服务：</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0LLrbd.png" alt="0LLrbd.png" border="0" /></p>
<p>启动服务后的样子：</p>
<p><img src="https://s1.ax1x.com/2020/10/17/0LL7an.png" alt="0LL7an.png" border="0" /></p>
<p>这时候你再用MQTT传输大的图片（服务器记得改成’localhost’），速度就很快了。</p>
<h3 id="mosquitto-pub-命令参数说明"><a href="#mosquitto-pub-命令参数说明" class="headerlink" title="mosquitto_pub 命令参数说明"></a><strong>mosquitto_pub 命令参数说明</strong></h3><ol>
<li><p>-d  打印debug信息</p>
</li>
<li><p>-f  将指定文件的内容作为发送消息的内容</p>
</li>
<li><p>-h  指定要连接的域名  默认为localhost</p>
</li>
<li><p>-i  指定要给哪个clientId的用户发送消息</p>
</li>
<li><p>-I  指定给哪个clientId前缀的用户发送消息</p>
</li>
<li><p>-m  消息内容</p>
</li>
<li><p>-n  发送一个空（null）消息</p>
</li>
<li><p>-p  连接端口号</p>
</li>
<li><p>-q  指定QoS的值（0,1,2）</p>
</li>
<li><p>-t  指定topic</p>
</li>
<li><p>-u  指定broker访问用户</p>
</li>
<li><p>-P  指定broker访问密码</p>
</li>
<li><p>-V  指定MQTT协议版本</p>
</li>
<li><p>—will-payload  指定一个消息，该消息当客户端与broker意外断开连接时发出。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-qos  Will的QoS值。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-retain 指定Will消息被当做一个retain消息（即消息被广播后，该消息被保留起来）。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-topic  用户发送Will消息的topic</p>
</li>
</ol>
<h3 id="mosquitto-sub-命令参数说明"><a href="#mosquitto-sub-命令参数说明" class="headerlink" title="mosquitto_sub 命令参数说明"></a><strong>mosquitto_sub 命令参数说明</strong></h3><ol>
<li><p>-c  设定‘clean session’为无效状态，这样一直保持订阅状态，即便是已经失去连接，如果再次连接仍旧能够接收的断开期间发送的消息。</p>
</li>
<li><p>-d  打印debug信息</p>
</li>
<li><p>-h  指定要连接的域名  默认为localhost</p>
</li>
<li><p>-i 指定clientId</p>
</li>
<li><p>-I 指定clientId前缀</p>
</li>
<li><p>-k keepalive 每隔一段时间，发PING消息通知broker，仍处于连接状态。 默认为60秒。</p>
</li>
<li><p>-q 指定希望接收到QoS为什么的消息  默认QoS为0</p>
</li>
<li><p>-R 不显示陈旧的消息</p>
</li>
<li><p>-t 订阅topic</p>
</li>
<li><p>-v 打印消息</p>
</li>
<li><p>—will-payload  指定一个消息，该消息当客户端与broker意外断开连接时发出。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-qos  Will的QoS值。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-retain 指定Will消息被当做一个retain消息（即消息被广播后，该消息被保留起来）。该参数需要与—will-topic一起使用</p>
</li>
<li><p>—will-topic  用户发送Will消息的topic</p>
</li>
</ol>
<h2 id="使用密码和SSL给MQTT加密"><a href="#使用密码和SSL给MQTT加密" class="headerlink" title="使用密码和SSL给MQTT加密"></a>使用密码和SSL给MQTT加密</h2><p>给mosquitto broker设置密码，输入以下命令后<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mosquitto_passwd -c passwordfile username</span><br></pre></td></tr></table></figure></p>
<p>用户名username自己取一个，回车后命令行窗口会让你设置密码。</p>
<h2 id="另一个MQTT-服务器-EMQ-X"><a href="#另一个MQTT-服务器-EMQ-X" class="headerlink" title="另一个MQTT 服务器 EMQ X"></a>另一个MQTT 服务器 EMQ X</h2><p>除了mosquitto，我们还可以使用EMQ X来作为我们的服务端。使用方法是使用docker安装EMQX，然后打开浏览器，输入，既可以看到服务器的可视化数据面板了，这一点比无可视化界面的mosquitto好。</p>
<p><a target="_blank" rel="noopener" href="https://docs.emqx.net/broker/latest/cn/">EMQ X的官方文档</a> </p>
<h1 id="免费公共-MQTT-服务器"><a href="#免费公共-MQTT-服务器" class="headerlink" title="免费公共 MQTT 服务器"></a><strong>免费公共 MQTT 服务器</strong></h1><p>除了上面说的在自己设备上部署MQTT服务器，你还可以使用公共的MQTT服务器，这是一些相关公司免费提供给大家用来测试用的服务器，推荐以下三个免费公共 MQTT 服务器：</p>
<hr>
<ul>
<li>Broker: <strong>mqtt.eclipse.org</strong></li>
<li>1883 : MQTT over unencrypted TCP</li>
<li>8883 : MQTT over encrypted TCP</li>
<li>80 : MQTT over unencrypted WebSockets (note: URL must be /mqtt )</li>
<li>443 : MQTT over encrypted WebSockets (note: URL must be /mqtt )</li>
</ul>
<p>推荐指数：⭐⭐⭐，速度快，端口多</p>
<hr>
<ul>
<li>Broker: <strong>broker.hivemq.com</strong></li>
<li>TCP Port: 1883</li>
<li>Websocket Port: 8000</li>
</ul>
<p>推荐指数：⭐⭐，速度还行</p>
<hr>
<ul>
<li>Broker: <strong>broker.emqx.io</strong></li>
<li>TCP Port: 1883</li>
<li>Websocket Port: 8083</li>
</ul>
<p>推荐指数：⭐，速度慢，有时发送图片还会收不到</p>
<h1 id="MQTT客户端"><a href="#MQTT客户端" class="headerlink" title="MQTT客户端"></a><strong>MQTT客户端</strong></h1><p>如果实际应用中我们通过mosquitto_sub和mosquitto_pub的方式来订阅和发送，一个是每次都要输入麻烦，一个是无法执行复杂逻辑的代码，显然是不切实际的，于是就有了paho-mqtt，这个python包让你用python语言编写MQTT客户端的代码。安装很简单：<code>pip install paho-mqtt</code>。所以，mosquitto_sub和mosquitto_pub只是用来测试，实际生产还是用paho-mqtt来写订阅和发送的代码。</p>
<p>早期版本的paho-mqtt只有下图3个模块，这也代表了它最本质的功能。</p>
<p><img src=https://s1.ax1x.com/2020/10/20/BpzBi6.png></p>
<h2 id="paho-mqtt的基本使用"><a href="#paho-mqtt的基本使用" class="headerlink" title="paho-mqtt的基本使用"></a>paho-mqtt的基本使用</h2><p><strong>导入模块</strong>：<code>Import paho.mqtt.client as mqtt</code></p>
<p><strong>创建客户端实例</strong>：<br><code>client =mqtt.Client(client_name)</code></p>
<p>Client()的定义：<code>Client(client_id=&quot;&quot;, clean_session=True, userdata=None, protocol=MQTTv311, transport=&quot;tcp&quot;)</code>，但我们其实只要输入第一个client id就可以了,甚至我们不输入，系统也会自己为我们创建名称，当然实际使用中，有多个客户端，为了方便区分，还是自己定义一套命名规则比较好。而且万一id名重复了，服务器只会选一个连接。更多参数讲解，请见<a target="_blank" rel="noopener" href="http://www.steves-internet-guide.com/client-objects-python-mqtt/">这里</a>。</p>
<p><strong>连接到服务器 :</strong> <code>client.connect(HostIPAddress)</code></p>
<p>connect()方法的定义：<code>connect(host, port=1883, keepalive=60, bind_address=&quot;&quot;)</code>，我们可以保持它们默认，只输入服务器的ip地址就可以了。</p>
<p><strong>发布消息：</strong><code>client.publish(&quot;topic&quot;,&quot;message&quot;)</code></p>
<p>publish()方法的定义：<code>publish(topic, payload=None, qos=0, retain=False)</code>，一般我们是填写topic和message/payload两者就行了。</p>
<p><strong>订阅消息：</strong><code>client.subscribe(&quot;topic&quot;)</code><br>subscribe()方法的定义：<code>subscribe(topic, qos=0)</code>,一般我们只填写topic。</p>
<p><strong>接收信息并处理：</strong></p>
<p>订阅号主题还无法接受消息，还需要靠下面这个回调函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_message</span>(<span class="params">client, userdata, message</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message received &quot;</span> ,<span class="built_in">str</span>(message.payload.decode(<span class="string">&quot;utf-8&quot;</span>)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message topic=&quot;</span>,message.topic)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message qos=&quot;</span>,message.qos)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;message retain flag=&quot;</span>,message.retain)</span><br></pre></td></tr></table></figure>
<p>使用<code>client.on_message=on_message</code>把客户端绑定到上面的回调函数。</p>
<p><strong>使用Logging分析问题：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_log</span>(<span class="params">client, userdata, level, buf</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;log: &quot;</span>,buf)</span><br></pre></td></tr></table></figure>
<p>然后再使用<code>client.on_log=on_log</code>把客户端绑定到上面的回调函数。</p>
<p><strong>开始工作</strong></p>
<p>我们可以使用<code>client.loop_start()</code>来使客户端开始不断地订阅或者发布消息，最后可以使用<code>client.stop_start()</code>使其停止工作。或者你想偷懒，使用<code>client.loop_forever
()</code>使客户端一直工作下去。</p>
<p><strong>WebSocket连接</strong></p>
<p>MQTT协议除支持TCP传输层外，还支持WebSocket作为传输层。通过WebSocket浏览器可以直连MQTT消息服务器。</p>
<p>为了告诉客户端使用websockets，要像以下代码一样申明，并且修改websockets专门的端口号</p>
<p><code>client = mqtt.Client(transport=&quot;websockets&quot;)</code></p>
<h1 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h1><h2 id="0-传输文本"><a href="#0-传输文本" class="headerlink" title="0.传输文本"></a>0.传输文本</h2><h2 id="1-传输图片"><a href="#1-传输图片" class="headerlink" title="1.传输图片"></a>1.传输图片</h2><p>除了最基本的，用MQTT传输文本字符信息外，我们还可以用MQTT传输图片，文件结构如下，文件夹下有测试用的不同大小的图片和接收到的图片：</p>
<p><img src="https://pic.downk.cc/item/5f8adc811cd1bbb86b112de4.jpg"></p>
<p>使用上面介绍的几个免费公共 MQTT 服务器传输图片速度会比较慢，程序没有问题，只是等待订阅端收到图片的时间会比较长。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sub.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">MQTT_SERVER = <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;Image&quot;</span></span><br><span class="line">image_format = <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The callback for when the client receives a CONNACK response from the server.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_connect</span>(<span class="params">client, userdata, flags, rc</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connected with result code &quot;</span>+<span class="built_in">str</span>(rc))</span><br><span class="line">    <span class="comment"># 订阅建议放在 on_connect 里，因为如果与 broker 失去连接后重连，仍然会继续订阅该主题</span></span><br><span class="line">    client.subscribe(MQTT_TOPIC)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回调函数，当收到消息时，触发该函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_message</span>(<span class="params">client, userdata, msg</span>):</span></span><br><span class="line">    <span class="comment"># Create a file with write byte permission</span></span><br><span class="line">    start_time = time.strftime(<span class="string">&quot;%Y-%m-%d %H-%M-%S&quot;</span>, time.localtime())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Received Image at &#123;&#125;&quot;</span>.<span class="built_in">format</span>(start_time))</span><br><span class="line">    f = <span class="built_in">open</span>(start_time + image_format, <span class="string">&quot;wb&quot;</span>)</span><br><span class="line">    f.write(msg.payload)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">client = mqtt.Client()</span><br><span class="line">client.on_connect = on_connect</span><br><span class="line">client.on_message = on_message</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置遗嘱消息，当树莓派断电，或者网络出现异常中断时，发送遗嘱消息给其他客户端</span></span><br><span class="line">client.will_set(MQTT_TOPIC,  <span class="string">b&#x27;&#123;&quot;status&quot;: &quot;Off&quot;&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果broker设置了账号和密码，这里我们需要填写账号密码才能连上</span></span><br><span class="line"><span class="comment"># client.username_pw_set(username=&quot;username&quot;,password=&quot;password&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建连接，三个参数分别为 broker 地址，broker 端口号，保活时间</span></span><br><span class="line">client.connect(MQTT_SERVER, <span class="number">1883</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置网络循环堵塞，在调用 disconnect() 或程序崩溃前，不会主动结束程序</span></span><br><span class="line">client.loop_forever()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pub.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">MQTT_SERVER = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;Image&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 回调函数。当与 MQTT broker 建立连接时，触发该函数。</span></span><br><span class="line"><span class="comment"># rc 是响应码。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_connect</span>(<span class="params">client, userdata, flags, rc</span>):</span></span><br><span class="line">    <span class="keyword">if</span> rc == <span class="number">0</span>:</span><br><span class="line">        client.connected_flag = <span class="literal">True</span>  <span class="comment"># set flag</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;connected OK Returned code =&quot;</span>, rc)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Bad connection Returned code=&quot;</span>,rc)</span><br><span class="line">        client.bad_connection_flag = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client = mqtt.Client()</span><br><span class="line">client.connected_flag = <span class="literal">False</span></span><br><span class="line">client.bad_connection_flag = <span class="literal">False</span></span><br><span class="line">client.on_connect = on_connect  <span class="comment"># bind call back function</span></span><br><span class="line">client.loop_start()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Connecting to broker &quot;</span>, MQTT_SERVER)</span><br><span class="line">client.connect(MQTT_SERVER, <span class="number">1883</span>, <span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> client.connected_flag:  <span class="comment"># wait in loop</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;In wait loop : &#123;&#125; seconds&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    i = i+<span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> client.bad_connection_flag:</span><br><span class="line">    client.loop_stop()    <span class="comment"># Stop loop</span></span><br><span class="line">    sys.exit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> client.connected_flag:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;正常连接后开始做正事</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&quot;2086.jpg&quot;</span>, <span class="string">&quot;rb&quot;</span>)</span><br><span class="line">    fileContent = f.read()</span><br><span class="line">    byteArr = <span class="built_in">bytearray</span>(fileContent)</span><br><span class="line">    <span class="built_in">print</span>(byteArr)</span><br><span class="line"></span><br><span class="line">client.publish(MQTT_TOPIC, payload=byteArr, qos=<span class="number">0</span>, retain=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;send out&quot;</span>)</span><br><span class="line"></span><br><span class="line">client.loop_forever()</span><br><span class="line"></span><br><span class="line"><span class="comment"># client.disconnect()</span></span><br><span class="line"><span class="comment"># client.loop_stop()</span></span><br></pre></td></tr></table></figure>
<p>我们在 on_connect 函数里对响应码进行了判断，为 0 则输出 Connected success 表示连接成功。如果返回的是其它数字，我们就需要对照下面的响应码进行判断。</p>
<ul>
<li>0: 连接成功</li>
<li>1: 连接失败-不正确的协议版本</li>
<li>2: 连接失败-无效的客户端标识符</li>
<li>3: 连接失败-服务器不可用</li>
<li>4: 连接失败-错误的用户名或密码</li>
<li>5: 连接失败-未授权</li>
</ul>
<h2 id="传输视频"><a href="#传输视频" class="headerlink" title="传输视频"></a>传输视频</h2><p>我们先看看用MQTT来在局域网内传输视频流的效果怎么样。</p>
<p>这篇文章<a target="_blank" rel="noopener" href="https://medium.com/@pritam.mondal.0711/stream-live-video-from-client-to-server-using-opencv-and-paho-mqtt-674d3327e8b3">《Stream live video from client to server using OpenCV and Paho-MQTT》</a>给的代码很好用，复制过来后改一下Broker的地址就可以马上跑起来了，用windows电脑localhost来做broker传输速度可以，不卡顿，但用公共免费broker就会严重卡顿了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sender.py</span></span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Raspberry PI IP address</span></span><br><span class="line">MQTT_BROKER = <span class="string">&quot;192.168.x.x&quot;</span></span><br><span class="line"><span class="comment"># Topic on which frame will be published</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;home/server&quot;</span></span><br><span class="line"><span class="comment"># Object to capture the frames</span></span><br><span class="line">cap = cv.VideoCapture(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Phao-MQTT Clinet</span></span><br><span class="line">client = mqtt.Client()</span><br><span class="line"><span class="comment"># Establishing Connection with the Broker</span></span><br><span class="line">client.connect(MQTT_BROKER)</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="comment"># Read Frame</span></span><br><span class="line">        _, frame = cap.read()</span><br><span class="line">        <span class="comment"># Encoding the Frame</span></span><br><span class="line">        _, buffer = cv.imencode(<span class="string">&#x27;.jpg&#x27;</span>, frame)</span><br><span class="line">        <span class="comment"># Converting into encoded bytes</span></span><br><span class="line">        jpg_as_text = base64.b64encode(buffer)</span><br><span class="line">        <span class="comment"># Publishig the Frame on the Topic home/server</span></span><br><span class="line">        client.publish(MQTT_TOPIC, jpg_as_text)</span><br><span class="line">        end = time.time()</span><br><span class="line">        t = end - start</span><br><span class="line">        fps = <span class="number">1</span> / t</span><br><span class="line">        <span class="built_in">print</span>(fps)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    cap.release()</span><br><span class="line">    client.disconnect()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nNow you can restart fresh&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># receiver.py</span></span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paho.mqtt.client <span class="keyword">as</span> mqtt</span><br><span class="line"></span><br><span class="line">MQTT_BROKER = <span class="string">&quot;192.168.x.x&quot;</span></span><br><span class="line">MQTT_TOPIC = <span class="string">&quot;home/server&quot;</span></span><br><span class="line"></span><br><span class="line">frame = np.zeros((<span class="number">240</span>, <span class="number">320</span>, <span class="number">3</span>), np.uint8)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The callback for when the client receives a CONNACK response from the server.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_connect</span>(<span class="params">client, userdata, flags, rc</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Connected with result code &quot;</span>+<span class="built_in">str</span>(rc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Subscribing in on_connect() means that if we lose the connection and</span></span><br><span class="line">    <span class="comment"># reconnect then subscriptions will be renewed.</span></span><br><span class="line">    client.subscribe(MQTT_TOPIC)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The callback for when a PUBLISH message is received from the server.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">on_message</span>(<span class="params">client, userdata, msg</span>):</span></span><br><span class="line">    <span class="keyword">global</span> frame</span><br><span class="line">    <span class="comment"># Decoding the message</span></span><br><span class="line">    img = base64.b64decode(msg.payload)</span><br><span class="line">    <span class="comment"># converting into numpy array from buffer</span></span><br><span class="line">    npimg = np.frombuffer(img, dtype=np.uint8)</span><br><span class="line">    <span class="comment"># Decode to Original Frame</span></span><br><span class="line">    frame = cv.imdecode(npimg, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client = mqtt.Client()</span><br><span class="line">client.on_connect = on_connect</span><br><span class="line">client.on_message = on_message</span><br><span class="line"></span><br><span class="line">client.connect(MQTT_BROKER)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Starting thread which will receive the frames</span></span><br><span class="line">client.loop_start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    cv.imshow(<span class="string">&quot;Stream&quot;</span>, frame)</span><br><span class="line">    <span class="keyword">if</span> cv.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Stop the Thread</span></span><br><span class="line">client.loop_stop()</span><br></pre></td></tr></table></figure>
<p>但正如<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/53385855/python-mqtt-improving-publish-speed-for-image-numpy-array">这里所说</a>，MQTT适用于低带宽和不可靠的连接，而像实时视频传输这种需求，MQTT可能就不适合了，这种需求对应的解决方案有：RTSP、FFMPEG、GStreamer、ZeroZMQ，但是下面我们讲介绍的是ImageZMQ。</p>
<p>ImageZMQ和MQTT名字中都包含MQ，两者的含义都是一样的，都是Message Queuing的意思，都是采用客户端和中间代理服务器的工作模式。ImageZMQ是下图右侧人物在下图左侧人物创办的PyImageSearch Gurus学习课程中开发出来的，并在<a target="_blank" rel="noopener" href="https://www.pyimageconf.com/">PyImageConf</a> 2018上演讲介绍，ppt slide在<a target="_blank" rel="noopener" href="https://www.pyimageconf.com/static/talks/jeff_bass.pdf">这里</a>。ZeroMQ是专门针对高吞吐量和低延迟开发的应用程序，而ImageZMQ库基于ZeroMQ被设计用于通过网络高效地传输视频流。ImageZMQ是一个Python软件包，与OpenCV完美集成。</p>
<p>【<a target="_blank" rel="noopener" href="https://github.com/jeffbass/imagezmq">ImageZMQ Github 地址</a>】<br>【<a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/">ImageZMQ 介绍博客</a>】</p>
<center class="half">
    <img src="https://s1.ax1x.com/2020/10/19/0zaG5R.png" width="285"/>
    <img src="https://s1.ax1x.com/2020/10/19/0za0qe.png" width="280"/>
</center>

<p><img src=https://s3-us-west-2.amazonaws.com/static.pyimagesearch.com/imagezmq-opencv/imagezmq_demo.gif width=130%></p>
<h2 id="其他综合项目"><a href="#其他综合项目" class="headerlink" title="其他综合项目"></a>其他综合项目</h2><p>《<a target="_blank" rel="noopener" href="https://dzone.com/articles/opencv-and-esp32-moving-a-servo-with-my-face">OpenCV and ESP32: Moving a Servo With My Face</a>》本博客介绍的项目是用ESP32和私服电机作为客户端，订阅电脑那边发出的‘/servo’主题，根据人脸位置转动私服电机。文中的MQTT代码是用C++写的，运行在ESP32上面。</p>
<p>《<a target="_blank" rel="noopener" href="https://www.instructables.com/Smart-JPEG-Camera-for-Home-Security/">A Smart JPEG Camera for Home Security</a>》这篇博客介绍了在树莓派上使用Node-Red编程，使其作为MQTT客户端，通过wifi dongle（就是USB插口的无线网卡）发送摄像头和<a target="_blank" rel="noopener" href="https://learn.adafruit.com/pir-passive-infrared-proximity-motion-sensor/how-pirs-work?view=all">人体热释红外传感器</a>的数据给电脑端的服务器。文章是2016年写的，所以文中的python-mosquitto就是今天的paho。</p>
<p>这里提一下Node-Red，这是一个基本流的可视化编程工具，运行在浏览器上，现在已经被默认搭载在各种物联网设备上（比如树莓派），它的工作原理<a target="_blank" rel="noopener" href="https://nodered.org/about/">官网</a>是这么介绍的：</p>
<blockquote>
<p><strong>Runtime/Editor</strong><br><br>Node-RED consists of a Node.js based runtime that you point a web browser at to access the flow editor. Within the browser you create your application by dragging nodes from your palette into a workspace and start to wire them together. With a single click, the application is deployed back to the runtime where it is run.<br><br>The palette of nodes can be easily extended by installing new nodes created by the community and the flows you create can be easily shared as JSON files.</p>
</blockquote>
<p>一句话概括就是Node-Red会把你编写的流程转换成程序代码（json格式）交给Node.js环境运行。它强大的生命力有一点是因为已有大量的<a target="_blank" rel="noopener" href="https://flows.nodered.org/">Node、Flow、Collection</a>供人下载。国外有个专门指导NodeRed教学的网站<a target="_blank" rel="noopener" href="http://noderedguide.com/">noderedguide</a>可以看一下。</p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p>推荐一位国外MQTT领域的专家 - Steve 的博客，<a target="_blank" rel="noopener" href="http://www.steves-internet-guide.com/">他的博客</a>基本就是专门讲MQTT这块的（如下图所示）</p>
<p><img src="https://pic.downk.cc/item/5f8b13631cd1bbb86b1e873d.jpg"></p>
<p>这里就只列举几篇比较进阶的文章：</p>
<p><a target="_blank" rel="noopener" href="http://www.steves-internet-guide.com/multiple-client-connections-python-mqtt/">《Handling Multiple Client Connections-Python MQTT》</a></p>
<p><a target="_blank" rel="noopener" href="http://www.steves-internet-guide.com/mqtt-websockets/">《Using MQTT Over WebSockets with Mosquitto》</a></p>
<p>另外再推荐一下EMQ网站上的一个<a target="_blank" rel="noopener" href="https://www.emqx.io/cn/blog">博客集</a>，里面收集了很多博主编写的优质文章。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/IOT/" rel="tag">IOT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MQTT/" rel="tag">MQTT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Raspberry-Pi/" rel="tag">Raspberry Pi</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-图像处理算法"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/05/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95/"
    >图像处理算法</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/10/05/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95/" class="article-date">
  <time datetime="2020-10-05T13:22:01.000Z" itemprop="datePublished">2020-10-05</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h1><p>SIFT(Scale-Invariant Feature Transform，尺度不变特征转换)在目标识别、图像配准领域具有广泛的应用。</p>
<p>先对原始图像进行扩大一倍的处理（用双线性插值）</p>
<p>在SIFT算法中，采用的是高斯差分金字塔（DOG），具体为什么是高斯差分金字塔不知道，只知道大牛们通过实验得出：尺度归一化的高斯拉普拉斯算子能够得到最稳定的图像特征，但因为计算量太大，而高斯差分函数与高斯拉普拉斯算子很相似，所以通过高斯差分函数来近似的计算图像最稳定的特征。</p>
<p>下面看一个简单的高斯差分<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35239859/article/details/105160095">例子</a>：</p>
<p>第一步，先求出不同$\sigma$下的高斯滤波输出，并求出它们的DOG图像,如下图所示</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">original - $\sigma_1$</th>
<th style="text-align:center">$\sigma_1$-$\sigma_2$</th>
<th style="text-align:center">$\sigma_2$$\sigma_3$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="https://pic.downk.cc/item/5f7b2ecb160a154a67ba72ef.jpg"></td>
<td style="text-align:center"><img src="https://pic.downk.cc/item/5f7b2eed160a154a67ba7d52.jpg"></td>
<td style="text-align:center"><img src="https://pic.downk.cc/item/5f7b2f13160a154a67ba87c9.jpg"></td>
</tr>
</tbody>
</table>
</div>
<p>第二步，根据DOG,求角点。</p>
<p>三维图中的最大值和最小值点是角点。如图所示，标记当前像素点为红色，领域像素点标记为黄，可以看出，领域的黄色像素点一共有3×9-1=26个。如果它是所有邻接像素点的最大值或最小值点，则该红色像素点被标记为特征点，如此依次进行，则可以完成图像的特征点提取。</p>
<p><img src="https://pic.downk.cc/item/5f7b2f89160a154a67baa70a.jpg"></p>
<p>于是我们计算出三个DOG图中是极值的点，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f7b304b160a154a67bae034.jpg"></p>
<p>黑色为极小值，白色为极大值</p>
<p>因此，原始图像上显示的DOG角点检测结果，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f7b3076160a154a67baeec1.jpg"></p>
<p>在SIFT算法中，</p>
<p><img src=https://www.researchgate.net/profile/Liang-Bi_Chen/publication/325264930/figure/fig2/AS:655137697697799@1533208407234/Difference-of-Gaussian-DOG-Difference-of-Gaussian-DOG.png></p>
<p><a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Difference-of-Gaussian-DOG-Difference-of-Gaussian-DOG_fig2_325264930">[图片来源]</a></p>
<p><strong>DOG的定义</strong></p>
<p>高斯差（英语：Difference of Gaussians，简称“DOG”）就是将两个不同高斯模糊程度的图像相减的算法。我们知道高斯模糊用的卷积核里面的数值满足正态分布/高斯分布，其二维形式的公式为：</p>
<script type="math/tex; mode=display">G(x, y, \sigma)=\frac{1}{2 \pi \sigma^{2}} e^{-\left(x^{2}+y^{2}\right) / 2 \sigma^{2}}</script><p>那么高斯差二维形式的公式为：</p>
<script type="math/tex; mode=display">G(x, y, \sigma)=\frac{1}{2 \pi \sigma^{2}} \exp ^{-\left(x^{2}+y^{2}\right) /\left(2 \sigma^{2}\right)}-\frac{1}{2 \pi K^{2} \sigma^{2}} \exp ^{-\left(x^{2}+y^{2}\right) /\left(2 K^{2} \sigma^{2}\right)}</script><p><img src=https://www.olympus-lifescience.com.cn/data/olympusmicro/primer/java/digitalimaging/processing/diffgaussians/diffgaussiansfigure1.jpg></p>
<p><a target="_blank" rel="noopener" href="https://www.olympus-lifescience.com.cn/zh/microscope-resource/primer/java/digitalimaging/processing/diffgaussians/">[图片来源]</a></p>
<p>由于高斯差是两个不同的低通（low-pass）滤波图像之间的差异，DoG实际上是一个带通（band-pass）滤波器，它去除了代表噪声的高频分量，也去除了代表图像中均匀区域的一些低频分量。我们认为通带中的频率分量与图像中的边缘相关联。</p>
<p><strong>DOG的应用</strong></p>
<p>作为一个增强算法，DOG可以被用来增加边缘和其他细节的可见性，大部分的边缘锐化算子使用增强高频信号的方法，但是因为随机噪声也是高频信号，很多锐化算子也增强了噪声。DOG算法去除的高频信号中通常包含了随机噪声，所以这种方法是最适合处理那些有高频噪声的图像。这个算法的一个主要缺点就是在调整图像对比度的过程中信息量会减少。</p>
<p>当它被用于图像增强时，DOG算法中两个高斯核的半径之比通常为4:1或5:1。当设为1.6时，即为高斯拉普拉斯算子的近似。高斯拉普拉斯算子在多尺度多分辨率像片。用于近似高斯拉普拉斯算子两个高斯核的确切大小决定了两个高斯模糊后的影像间的尺度。</p>
<p>DOG也被用于尺度不变特征变换中的斑点检测。事实上，DOG算法作为两个多元正态分布的差通常总额为零，把它和一个恒定信号进行卷积没有意义。当K约等于1.6时它很好的近似了高斯拉普拉斯变换，当K约等于5时又很好的近似了视网膜上神经节细胞的视野。</p>
<p><a target="_blank" rel="noopener" href="http://www.360doc.com/content/19/0709/12/32196507_847629007.shtml">http://www.360doc.com/content/19/0709/12/32196507_847629007.shtml</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ronny/p/4028776.html">https://www.cnblogs.com/ronny/p/4028776.html</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/zddblog/article/details/7521424">https://blog.csdn.net/zddblog/article/details/7521424</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-YOLOV4"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/10/04/YOLOV4/"
    >YOLOV4</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/10/04/YOLOV4/" class="article-date">
  <time datetime="2020-10-04T05:09:48.000Z" itemprop="datePublished">2020-10-04</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>推荐阅读江大白的《<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143747206">深入浅出Yolo系列之Yolov3&amp;Yolov4核心基础知识完整讲解</a>》，我是在写了本篇博文很长时间后才阅读到他这篇宝藏博客的，由此引发了我的一个感想：我感觉我们不需要关注很多技术博主公众号，只需要关注几个优质的博主就行了，一般他们的作品都会比较优质。</p>
<h1 id="darknet在本地电脑的安装"><a href="#darknet在本地电脑的安装" class="headerlink" title="darknet在本地电脑的安装"></a>darknet在本地电脑的安装</h1><p>详细的过程请参照The AI Guy的<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=saDipJR14Lc&amp;t=992s">教学视频</a>，我这里只写一些重点的步骤。</p>
<p>yolo默认是使用darknet框架，darknet是一个类似tensorflow的框架，不过它的安装方式跟tensorflow不太一样。</p>
<p>linux版比较简单,可参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/Eric_Fisher/article/details/89884108">这篇博客</a>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;AlexeyAB&#x2F;darknet.git</span><br><span class="line">&#x2F;&#x2F;或者使用更快的镜像：https:&#x2F;&#x2F;gitee.com&#x2F;wwdok&#x2F;darknet.git</span><br><span class="line">cd darknet</span><br></pre></td></tr></table></figure>
<p>修改Makefile，添加对GPU，CUDNN，OpenCV等的支持。<br>如果你的CUDA没有使用默认的路径，请进行修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi Makefile</span><br></pre></td></tr></table></figure>
<p><img src=https://images3.pianshen.com/576/00/00e1bc11525ead2915af54e786d28c60.png></p>
<p>接下来,编译</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make</span><br></pre></td></tr></table></figure>
<p>如果都已正确编译，请尝试运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;darknet</span><br></pre></td></tr></table></figure>
<p>应该得到输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usage: .&#x2F;darknet &lt;function&gt;</span><br></pre></td></tr></table></figure>
<p>windows上的安装比较复杂一点：</p>
<p>首先用记事本打开darknet\build\darknet\darknet.vcxproj，确保里面的CUDA版本对应你电脑CUDA的版本，一共有两处需要修改，如图：</p>
<p><img src="https://pic.downk.cc/item/5f79b659160a154a6762986b.jpg"><br><img src="https://pic.downk.cc/item/5f79b671160a154a67629d51.jpg"></p>
<p>我还看到darknet.vcxproj里有 $(OPENCV_DIR) 和 $(CUDA_PATH) ，所以我还自己去设置了这两个环境变量，以防后面出什么差错，说明一下，opencv430的文件夹是从Windows下载，不是从Sources下载：</p>
<p><img src="https://pic.downk.cc/item/5f79b6eb160a154a6762bb31.jpg"><br><br><br><img src="https://pic.downk.cc/item/5f79b87a160a154a6763221f.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f79d0d6160a154a67694ebf.jpg"></p>
<p>接下来打开CMake，像下图配置两个路径，点击<strong>Configure</strong>后会报错显红，我按照视频的教程来配置了下图黄框两处的变量：</p>
<p><img src="https://pic.downk.cc/item/5f799a45160a154a6755d70a.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f799b36160a154a67560ac9.jpg" ></p>
<p>但配置好后再点击<strong>Configure</strong>e之后仍然报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CMake Error at C:&#x2F;Program Files&#x2F;CMake&#x2F;share&#x2F;cmake-3.17&#x2F;Modules&#x2F;CMakeDetermineCompilerId.cmake:367 (message):</span><br><span class="line">  No CUDA toolset found.</span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  C:&#x2F;Program Files&#x2F;CMake&#x2F;share&#x2F;cmake-3.17&#x2F;Modules&#x2F;CMakeDetermineCompilerId.cmake:32 (CMAKE_DETERMINE_COMPILER_ID_BUILD)</span><br><span class="line">  C:&#x2F;Program Files&#x2F;CMake&#x2F;share&#x2F;cmake-3.17&#x2F;Modules&#x2F;CMakeDetermineCUDACompiler.cmake:72 (CMAKE_DETERMINE_COMPILER_ID)</span><br><span class="line">  CMakeLists.txt:66 (enable_language)</span><br></pre></td></tr></table></figure>
<p>解决办法是将<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\visual_studio_integration\MSBuildExtensions</code>里面的4个文件全部拷贝到<code>C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Microsoft\VC\v160\BuildCustomizations</code>。</p>
<p><img src="https://pic.downk.cc/item/5f79ac97160a154a675fc9b0.jpg"></p>
<p><img src="https://pic.downk.cc/item/5f79ad3d160a154a676062ea.jpg"></p>
<p>再次点击<strong>Configure</strong>，已经没有上面那个报错了，但CMake仍然一片红。这时候再点开ENABLE，取消ENABLE_CUDNN_HALF的勾选，再点击configure后CMake就变成一片白色了，成功：<br><img src="https://pic.downk.cc/item/5f79b2ea160a154a6761c47e.jpg"></p>
<p>接下来点击<strong>Generate</strong>，很快CMake底部输出栏就输出了“Generating done”。这时我们通过Open Project打开build文件夹，可以看到生成了很多文件（夹）：</p>
<p><img src="https://pic.downk.cc/item/5f79b3ff160a154a67620664.jpg"></p>
<p>用 Visual Studio 打开上图中的Darknet.sln。选择“Release”+“x64”，右键点击“ALL_BUILD”，在右键菜单中，选择“生成”，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f79c2df160a154a6765fcaf.jpg"></p>
<p>接着再右键点击“INSTALL”，在右键菜单中，选择“生成”，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f79c351160a154a67661818.jpg"></p>
<p>当我想要运行darknet和yolo看看时，又遇到报错：“由于找不到pthreadVC2.dll,无法继续执行代码,重新安装程序可能会解决此问题”。</p>
<p>解决办法是：去网上下载pthreadvc2.dll，然后同时复制粘贴到<code>C:\Windows\System32</code>和<code>C:\Windows\SysWOW64</code>目录下。</p>
<p>最后一步，就是运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\MachineLearning\CV\darknet&gt;darknet.exe detect cfg&#x2F;yolov4.cfg weights&#x2F;yolov4.weights data&#x2F;dog.jpg</span><br></pre></td></tr></table></figure>
<p>我们还可以在命令行后面加<code>-thresh 0.25</code> 、<code>-i 0</code>、 <code>-ext_output dog.jpg</code>、<code>-out_filename results/yolov4.mp4</code>个性化输出结果，其中，i代表GPU指数，-ext_output代表输出坐标，-out_filename 代表保存成图片或视频。</p>
<p>yolov4的效果比yolov3还好，连盆栽都检测出来了：</p>
<p><img src="https://pic.downk.cc/item/5f79cbd8160a154a67681d63.jpg"></p>
<p>为了以后能在其他文件夹下面运行darknet，我把darknet.exe的路径加入了环境变量：</p>
<p><img src="https://pic.downk.cc/item/5f79cd35160a154a676877ff.jpg" width=80% style="zoom: 80%;" ></p>
<h1 id="使用YOLO"><a href="#使用YOLO" class="headerlink" title="使用YOLO"></a>使用YOLO</h1><p>除了上面说的检测图片，我们还可以这么玩yolo：</p>
<p><strong>1.检测本地电脑上的视频</strong></p>
<p>修改video_yolov4.sh里面的内容如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./darknet detector demo ./cfg/coco.data ./cfg/yolov4.cfg ./weights/yolov4.weights data/cars.mp4 -out_filename results/yolov4.mp4</span><br><span class="line"></span><br><span class="line"><span class="comment"># ./darknet detector demo ./cfg/coco.data ./cfg/yolov4-tiny.cfg ./weights/yolov4-tiny.weights data/cars.mp4 -out_filename results/yolov4-tiny.mp4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./darknet detector demo ./cfg/coco.data ./weights/Yolo-Fastest/COCO/yolo-fastest.cfg ./weights/Yolo-Fastest/COCO/yolo-fastest.weights -out_filename results/yolov4-fastest.mp4</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>打出<code>sh video_yolov4.sh</code>即可运行。保存成sh的好处是后面要跑的时候不需要再敲一遍长长的命令行。</p>
<p><strong>2. 检测电脑摄像头画面</strong></p>
<p>如果你想检测电脑摄像头画面里的物体，则运行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">darknet.exe detector demo cfg\coco.data cfg\yolov4.cfg weights&#x2F;yolov4.weights -c 0</span><br></pre></td></tr></table></figure>
<p><strong>3. 检测网络摄像头画面</strong></p>
<p>我这里要将手机拍摄的画面传给电脑，然后用电脑对手机传输过来的画面检测。<br>先下载一个IP摄像头App，然后点击“开启服务器”</p>
<p><img src="https://pic.downk.cc/item/5f7be000160a154a67dc583a.jpg" width=35%></p>
<p>你就可以看到手机画面进入了摄像头预览界面，如下图所示：</p>
<p><img src="https://pic.downk.cc/item/5f7be086160a154a67dc75ff.jpg"></p>
<p>然后记住屏幕上的IP地址，比如我这里是192.168.1.4:8080,那么我就运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">darknet.exe detector demo cfg&#x2F;coco.data cfg&#x2F;yolov4.cfg weights&#x2F;yolov4.weights http:&#x2F;&#x2F;192.168.1.4:8080&#x2F;video?dummy&#x3D;param.mjpg</span><br></pre></td></tr></table></figure>
<p><strong>4. 使用YOLO Fastesst</strong></p>
<p>有大佬开源了号称最快版的YOLO版本，把这个<a target="_blank" rel="noopener" href="https://github.com/dog-qiuqiu/Yolo-Fastest">仓库</a>下载下来后我们只需要取它的cfg文件和weights文件即可</p>
<p><img src="https://pic.downk.cc/item/5f7be8a2160a154a67de213e.jpg"></p>
<p>修改命令行后运行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">darknet.exe detector demo cfg&#x2F;coco.data weights&#x2F;Yolo-Fastest&#x2F;COCO&#x2F;yolo-fastest.cfg weights&#x2F;Yolo-Fastest&#x2F;COCO&#x2F;yolo-fastest.weights -c 0</span><br></pre></td></tr></table></figure></p>
<p><strong>5. 用OpenCV的DNN跑YOLO</strong></p>
<p>python版：【<a target="_blank" rel="noopener" href="https://github.com/hpc203/Yolo-Fastest-opencv-dnn">Github Repo 1</a>】【<a target="_blank" rel="noopener" href="https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO">Github Repo 2</a>】<br>java版：</p>
<p><strong>6. 在colab上跑YOLO和训练自己的数据集</strong></p>
<p>相关教程视频请见The AI GUy的<a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLKHYJbyeQ1a3tMm-Wm6YLRzfW1UmwdUIN">YOLOv4 Object Detection系列</a></p>
<p>打开<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg?usp=sharing">该colab笔记本</a>。</p>
<p><strong>7. 将yolo移植到tensorflow上面</strong></p>
<p>相关教程视频请见The AI GUy的<a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLKHYJbyeQ1a3tMm-Wm6YLRzfW1UmwdUIN">YOLOv4 Object Detection系列</a></p>
<p>这里简单复述一下重点，如果使用自定义类别和数据集，请查看<a target="_blank" rel="noopener" href="https://github.com/theAIGuysCode/tensorflow-yolov4-tflite">repo</a>：</p>
<p>使用以下命令行将.weights 格式转换为.pb格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Convert yolov4 darknet weights to tensorflow</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;yolov4.weights --output .&#x2F;checkpoints&#x2F;yolov4-416 --input_size 416 --model yolov4 </span><br><span class="line"></span><br><span class="line"># Convert yolov4-tiny darknet weights to tensorflow</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;yolov4-tiny.weights --output .&#x2F;checkpoints&#x2F;yolov4-tiny-416 --input_size 416 --model yolov4 --tiny</span><br><span class="line"></span><br><span class="line"># Convert custom yolov4 darknet weights to tensorflow</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;custom.weights --output .&#x2F;checkpoints&#x2F;custom-416 --input_size 416 --model yolov4 </span><br></pre></td></tr></table></figure>
<p>我自己电脑上的命令行是：<code>python save_model.py --weights ../../darknet/weights/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4</code><br>最后生成的文件长这样：</p>
<p><img src="https://pic.downk.cc/item/5f7bf98f160a154a67e22532.jpg"></p>
<p>运行yolo的tensorflow pb格式模型</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Run yolov4 tensorflow model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg</span><br><span class="line"></span><br><span class="line"># Run yolov4-tiny tensorflow model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-tiny-416 --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg --tiny</span><br><span class="line"></span><br><span class="line"># Run custom yolov4 tensorflow model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;custom-416 --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;car.jpg</span><br><span class="line"></span><br><span class="line"># Run yolov4 on video</span><br><span class="line">python detect_video.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --size 416 --model yolov4 --video .&#x2F;data&#x2F;video&#x2F;road.mp4 --output .&#x2F;detections&#x2F;results.avi</span><br><span class="line"></span><br><span class="line"># Run custom yolov4 model on video</span><br><span class="line">python detect_video.py --weights .&#x2F;checkpoints&#x2F;custom-416 --size 416 --model yolov4 --video .&#x2F;data&#x2F;video&#x2F;cars.mp4 --output .&#x2F;detections&#x2F;results.avi</span><br><span class="line"></span><br><span class="line"># Run yolov4 on webcam</span><br><span class="line">python detect_video.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --size 416 --model yolov4 --video 0 --output .&#x2F;detections&#x2F;results.avi</span><br></pre></td></tr></table></figure>
<p>上面生成的模型是pb格式，接下来是生成tflite格式模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Save tf model for tflite converting</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;yolov4.weights --output .&#x2F;checkpoints&#x2F;yolov4-416 --input_size 416 --model yolov4 --framework tflite</span><br><span class="line"></span><br><span class="line"># Save custom yolov4 tf model for tflite converting</span><br><span class="line">python save_model.py --weights .&#x2F;data&#x2F;custom.weights --output .&#x2F;checkpoints&#x2F;custom-416 --input_size 416 --model yolov4 --framework tflite</span><br></pre></td></tr></table></figure>
<p>我自己电脑上运行的命令行是<code>python save_model.py --weights ../../darknet/weights/yolov4.weights --output ./checkpoints/yolov4-416-tflite --input_size 416 --model yolov4 --framework tflite</code>。注意，如果要生成tflite，不能直接用前面那个pb文件，得在命令行后面加个<code>--framework tflite</code>重新生成pb文件，然后再用这个pb文件转换成tflite文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># yolov4</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --output .&#x2F;checkpoints&#x2F;yolov4-416.tflite</span><br><span class="line"></span><br><span class="line"># convert custom yolov4 tflite model</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;custom-416 --output .&#x2F;checkpoints&#x2F;custom-416.tflite</span><br><span class="line"></span><br><span class="line"># yolov4 quantize float16</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --output .&#x2F;checkpoints&#x2F;yolov4-416-fp16.tflite --quantize_mode float16</span><br><span class="line"></span><br><span class="line"># yolov4 quantize int8</span><br><span class="line">python convert_tflite.py --weights .&#x2F;checkpoints&#x2F;yolov4-416 --output .&#x2F;checkpoints&#x2F;yolov4-416-int8.tflite --quantize_mode int8 --dataset .&#x2F;data&#x2F;dataset&#x2F;val2017.txt</span><br></pre></td></tr></table></figure>
<p>我自己电脑上运行的命令行是：<code>python convert_tflite.py --weights ./checkpoints/yolov4-416-tflite --output ./checkpoints/yolov4-416-int8.tflite --quantize_mode int8 --dataset ./data/dataset/val2017.txt</code></p>
<p>最后测试一下yolov4 tflite的模型性能如何<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Run tflite model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-416.tflite --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg --framework tflite</span><br><span class="line"></span><br><span class="line"># Run tflite model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;yolov4-416-int8.tflite --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;kite.jpg --framework tflite</span><br><span class="line"></span><br><span class="line"># Run custom tflite model</span><br><span class="line">python detect.py --weights .&#x2F;checkpoints&#x2F;custom-416.tflite --size 416 --model yolov4 --images .&#x2F;data&#x2F;images&#x2F;car.jpg --framework tflite</span><br></pre></td></tr></table></figure></p>
<p>不过这个hunglc007的tensorflow-yolov4-tflite仓库的tflite版有问题，首先是它的pb格式模型虽然生成出来了，但我用一个视频测试了一下，视频中的汽车一个都没检测出来，然后要把pb模型转换成tflite模型时遇到了报错：“RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor input_1<br>Empty min/max for tensor input_1”。然后我在这个<a target="_blank" rel="noopener" href="https://github.com/hunglc007/tensorflow-yolov4-tflite/issues/207">issue</a>里看到有人说用这个<a target="_blank" rel="noopener" href="https://github.com/hhk7734/tensorflow-yolov4">repo</a>就没问题，我试了一下，生成的tflite模型大小有251M，准确度跟yolov4.weights差不多。</p>
<p><strong>8. 训练自己的yolo检测器</strong></p>
<p>要训练自己的YOLO检测器，首先要有数据集，The AI Guy介绍了两种制作数据集的方法：</p>
<p>第一种：从Google的 <a target="_blank" rel="noopener" href="https://storage.googleapis.com/openimages/web/index.html">Open Images Dataset</a> 下载图片，相关的脚本在这个<a target="_blank" rel="noopener" href="https://github.com/theAIGuysCode/OIDv4_ToolKit">repo</a>或<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_4A9inxGqRM">视频</a>。简单来说，就是先运行<code>python main.py downloader --classes Apple Orange --type_csv train --limit 1000 --multiclasses 1</code>下载需要的图片，然后在运行<code>python convert_annotations.py</code>之前，我们需要在它的同级目录下创建一个classes.txt文件，内容就是一个类别一行，convert_annotations.py会利用它把Labels文件夹的txt文件转换成yolo格式的txt文件，并且生成的txt文件和图片位于同一文件夹里面。</p>
<p>第二种：从Google的图片搜索里下载图片，相关的脚本在这个<a target="_blank" rel="noopener" href="https://github.com/theAIGuysCode/Download-Google-Images">repo</a>或<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=EGQyDla8JNU">视频</a>。简单来说，它是生成一个包含一堆图片链接的urls.txt，然后用<code>python download_images.py --urls urls.txt --output images</code>把这些图片下载下来，并用opencv测试打开这些图片，如果打不开，则说明格式可能不符合（比如webp），那么就将这些图片删除掉。得到图片后再用<a target="_blank" rel="noopener" href="https://github.com/tzutalin/labelImg">labelImg</a>标注图片，这个软件可以选保存为YOLO格式。</p>
<p>以此类推，再下载20%的验证集和测试集</p>
<p>完成上面的两者之一的步骤后，你应该拥有了两个文件夹，每个文件夹里面有很多图片和同名的txt文件，打包成obj.zip和test.zip之后上传到google drive。接下来，我们就是要处理custom .cfg, obj.data, obj.names, train.txt 和 test.txt 这些文件了，具体步骤看<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1_GdoqCJWXsChrOiY8sZMr_zbr_fH-0Fg?usp=sharing#scrollTo=A9mYUoKOWWlR">YOLOv4_Training_Tutorial.ipynb</a>。<br>注意，如果你要用这个笔记本训练自己的数据集，请先拷贝一份到自己的谷歌云端硬盘。</p>
<p><strong>9. 结合deepsort实现目标跟踪</strong></p>
<p>来自The AI Guy开源的<a target="_blank" rel="noopener" href="https://github.com/theAIGuysCode/yolov4-deepsort">repo</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Run yolov4 deep sort object tracker on video</span><br><span class="line">python object_tracker.py --weights ..&#x2F;..&#x2F;Object_Detection&#x2F;tensorflow-yolov4-tflite&#x2F;checkpoints&#x2F;yolov4-tiny-416 --video .&#x2F;data&#x2F;video&#x2F;test.mp4 --output .&#x2F;outputs&#x2F;yolov4-tiny-tf-track-result.avi --model yolov4</span><br><span class="line"></span><br><span class="line"># Run yolov4 deep sort object tracker on webcam (set video flag to 0)</span><br><span class="line">python object_tracker.py --video 0 --output .&#x2F;outputs&#x2F;webcam.avi --model yolov4</span><br></pre></td></tr></table></figure>
<p>实测结果是：<br>行人视频：<br>用CPU<br>yolov4-tf：1.6 FPS<br>yolov4-tiny-tf：1.8 FPS</p>
<p>汽车视频（cars.mp4）:<br>GPU（GTX 1660 Ti）<br>yolov4 : 17.4 FPS(输出视频) 18.8 FPS （不输出视频）<br>yolov4-tiny : 19.5 FPS(输出视频) 27.2 FPS（不输出视频）<br>yolo-Fastest ； 20.1 FPS (输出视频) 28.2FPS（不输出视频）</p>
<p>这个帧率跟你的命令行也有关系，比如你的命令行说要输出视频，那么帧率就会低一点。</p>
<p>The AI Guy还做了一版用colab跑的<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1zmeSTP3J5zu2d5fHgsQC06DyYEYJFXq1?usp=sharing">ipynb</a>和<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=_zrNUzDS8Zc&amp;list=PLKHYJbyeQ1a3tMm-Wm6YLRzfW1UmwdUIN&amp;index=10">视频</a>，FPS能达到10FPS，不过我们也看到，用另一个车辆视频测试时，FPS提高到了15FPS，所以可以得出结论，跟踪的物体越多，FPS越小。</p>
<p>关于如何过滤显示的类别，具体请查看它的仓库。</p>
<p><strong>10.用YOLOV4检测结果导出为txt文件作为预标注</strong></p>
<p>如果我们的数据集很多图片是未标注的，如果都是人工去标注，是很枯燥费时的，我们可以利用命令行让YOLOV4把检测结果导出成它训练时用的txt那种格式的文件，也是labelimg导出的txt文件，导出来后打开labelimg，把图片和标注txt的文件夹设置好，labelimg自动会对应上同名的图片和标注txt文件，自动把矩形框显示在界面上，这时候你再微调一下就可以了。</p>
<p>这个<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/issues/6240">issue</a>讨论了这个问题。<br>有一位网友biparnakroy还创建了一个相关项目 : <a target="_blank" rel="noopener" href="https://github.com/biparnakroy/pseudoLabelGeneratorForYOLO，">https://github.com/biparnakroy/pseudoLabelGeneratorForYOLO，</a></p>
<p>并提供了一个转换成VOC格式的仓库：<br><a target="_blank" rel="noopener" href="https://github.com/hai-h-nguyen/Yolo2Pascal-annotation-conversion，">https://github.com/hai-h-nguyen/Yolo2Pascal-annotation-conversion，</a></p>
<p>评论里还有人说有类别重映射的仓库：<a target="_blank" rel="noopener" href="https://github.com/sa7ina/YOLOv2-5_Class_Remap">https://github.com/sa7ina/YOLOv2-5_Class_Remap</a></p>
<p>运行的命令行是：<code>darknet.exe detector test cfg/coco.data cfg/yolov4.cfg weights/yolov4.weights -thresh 0.25 -dont_show -save_labels &lt; data/new_train.txt</code></p>
<p>关键在于<code>-save_labels &lt; data/new_train.txt</code>, 注意，符号<code>&lt;</code>不能少了，那new_train.txt里面是什么呢，里面是带检测图片的路径:</p>
<p><img src="https://pic.downk.cc/item/5fa16a541cd1bbb86bdfa678.jpg"></p>
<p>我是用以下脚本生成这个new_train.txt的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">txt_file = <span class="string">r&quot;C:\MachineLearning\CV\darknet\data\new_train.txt&quot;</span></span><br><span class="line">reference_folder = <span class="string">r&quot;D:\MachineLearning\DataSet\iGuardDataset\IMAGE\temp&quot;</span></span><br><span class="line"><span class="built_in">print</span>(os.listdir(reference_folder))</span><br><span class="line">prefix = <span class="string">r&#x27;D:/MachineLearning/DataSet/iGuardDataset/IMAGE/temp/img&#x27;</span></span><br><span class="line">extension = <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">start_num = <span class="number">384</span></span><br><span class="line">num_files = <span class="built_in">len</span>([name <span class="keyword">for</span> name <span class="keyword">in</span> os.listdir(reference_folder) <span class="keyword">if</span> os.path.isfile(os.path.join(reference_folder, name))])  <span class="comment"># 排除将文件夹也计数</span></span><br><span class="line"></span><br><span class="line">f = <span class="built_in">open</span>(txt_file, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(start_num, start_num+num_files):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;%s%d%s&#x27;</span> % (prefix, index, extension), file=f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;写入一行：&#123;&#125;&#123;&#125;&#123;&#125;&quot;</span>.<span class="built_in">format</span>(prefix, index, extension))</span><br></pre></td></tr></table></figure>
<p>运行正常结束后，就会在图片的同一个文件夹下面生成同名的txt：</p>
<p><img src="https://pic.downk.cc/item/5fa16b7c1cd1bbb86bdfe1d2.jpg"></p>
<p>注意看，图中一个文件是classes.txt，这个东西的用处是labelimg软件根据标注框txt里的类别索引整数索引到这个classes.txt里的真实类别名称，这个文件不是代码生成的，因为我是用coco类别检测的，所以这个classes.txt是我从data/coco.names拷贝过来重命名的。</p>
<p>看，这就是YOLOV4帮你标注好的矩形框：</p>
<p><img src="https://pic.downk.cc/item/5fa16d671cd1bbb86be041a2.jpg"></p>
<p>如果觉得误检的比较多，那么你就调高threshold。又或者你觉得不要给我检测那么多类出来，还要我一个一个去删，解决思路是自己写个python脚本，对txt文件里你不想要的类别id所在的行删除掉。</p>
<h1 id="YOLOV4模型可视化"><a href="#YOLOV4模型可视化" class="headerlink" title="YOLOV4模型可视化"></a>YOLOV4模型可视化</h1><p>使用netron就可以查看yolov4的模型结构，通过打开cfg文件，没错，cfg文件你既可以用VS code打开用文本形式查看，也可以用netron打开，用图形形式查看。</p>
<h1 id="YOLO-cfg配置参数讲解"><a href="#YOLO-cfg配置参数讲解" class="headerlink" title="YOLO cfg配置参数讲解"></a>YOLO cfg配置参数讲解</h1><p>详细的解释请查看darknet Github Wiki：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-%5Bnet%5D-section">CFG Parameters in the [net] section</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-different-layers">CFG Parameters in the different layers  </a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/hrsstudy/article/details/65447947">中文博客解释</a>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[net]</span><br><span class="line">batch&#x3D;64</span><br><span class="line">subdivisions&#x3D;16  # 如果内存不够大，将batch分割为subdivisions个子batch,batch固定的前提下，subdivision越小，训练出来的模型精度越高</span><br><span class="line">momentum&#x3D;0.949</span><br><span class="line">decay&#x3D;0.0005  # 权重衰减正则项，防止过拟合</span><br><span class="line">angle&#x3D;0  # 通过旋转角度来生成更多训练样本</span><br><span class="line">saturation &#x3D; 1.5</span><br><span class="line">exposure &#x3D; 1.5</span><br><span class="line">hue&#x3D;.1</span><br><span class="line"></span><br><span class="line">learning_rate&#x3D;0.001</span><br><span class="line">burn_in&#x3D;1000  # current_learning rate &#x3D; learning_rate * pow(steps &#x2F; burn_in, power) ，，其中，learning_rate&#x3D;0.001，power&#x3D;4，所以 burn_in &#x3D; 0.001 * pow(steps&#x2F;1000, 4) ，从公式可以看出，当steps小于1000时，steps&#x2F;1000小于1，这样的数再做4次方，那就更小了，所以第1到第1000个steps时，学习率远远小于0.001，1000个steps之后，学习率才等于0.001,有点像yolov5的warm up。</span><br><span class="line"></span><br><span class="line">max_batches &#x3D; 6000  # 训练达到max_batches后停止学习</span><br><span class="line">policy&#x3D;steps  # 调整学习率的policy，有如下policy：CONSTANT, STEP, EXP, POLY, STEPS, SIG, RANDOM</span><br><span class="line">steps&#x3D;4800,5400  #  这两个数值代表着steps进行到这两个时间点时，学习率会乘以下面对应的两个scales</span><br><span class="line">scales&#x3D;.1,.1  # 以此类推，如果steps&#x3D;8000,9000,12000, scales&#x3D;.1,.1,.1 且当前迭代步数为10000 ，那么当前学习率为 &#x3D; learning_rate * scales[0] * scales[1] &#x3D; 0.001 * 0.1 * 0.1 &#x3D; 0.00001</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[yolo]</span><br><span class="line">mask &#x3D; 6,7,8</span><br><span class="line">anchors &#x3D; 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401</span><br><span class="line">classes&#x3D;2</span><br><span class="line">num&#x3D;9  # anchors或者说mask的数量</span><br><span class="line">jitter&#x3D;.3  # 随意更改图像大小和宽高比从 x(1 - 2*jitter) 到 x(1 + 2*jitter)</span><br><span class="line">ignore_thresh &#x3D; .7</span><br><span class="line">truth_thresh &#x3D; 1</span><br><span class="line">random&#x3D;1  # random为1时会启用Multi-Scale Training，随机使用不同尺寸的图片进行训练。</span><br><span class="line">scale_x_y &#x3D; 1.05</span><br><span class="line">iou_thresh&#x3D;0.213</span><br><span class="line">cls_normalizer&#x3D;1.0</span><br><span class="line">iou_normalizer&#x3D;0.07</span><br><span class="line">iou_loss&#x3D;ciou</span><br><span class="line">nms_kind&#x3D;greedynms</span><br><span class="line">beta_nms&#x3D;0.6</span><br><span class="line">max_delta&#x3D;5</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="YOLO-V4的tricks"><a href="#YOLO-V4的tricks" class="headerlink" title="YOLO V4的tricks"></a>YOLO V4的tricks</h1><p>相信我们已经被上面yolo v4的检测效果所惊喜，那yolo v4到底用了什么技巧达到如此准确的检测呢？</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103070923">https://zhuanlan.zhihu.com/p/103070923</a></p>
<p><a target="_blank" rel="noopener" href="https://aistudio.baidu.com/aistudio/education/group/info/1617">百度目标检测7日打卡营</a></p>
<h1 id="如何训练YOLOV4来检测自定义目标"><a href="#如何训练YOLOV4来检测自定义目标" class="headerlink" title="如何训练YOLOV4来检测自定义目标"></a>如何训练YOLOV4来检测自定义目标</h1><ol>
<li><p>要训练<code>cfg/yolov4-custom.cfg</code>，请下载预训练的权重文件（162 MB）：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137">yolov4.conv.137</a>。</p>
</li>
<li><p>创建文件yolo-obj.cfg，其内容与yolov4-custom.cfg中的内容相同（或将yolov4-custom.cfg复制到yolo-obj.cfg），并：</p>
<p>将batcch更改为<code>batch= 64</code></p>
<p>将subdivisions更改为<code>subdivisions=16</code></p>
<p>将max_batches更改为class * 2000（但不少于训练图像的数量，但不少于训练图像的数量且不少于6000），f.e。 如果您训练3个类别，<code>max_batches = 6000</code></p>
<p>将steps更改为max_batches的80％和90％，例如 <code>steps=4800,5400</code></p>
<p>设置网络大小width = 416 height = 416或任何32的值的倍数</p>
<p>在3个<code>[yolo]</code>层中将<code>classes=80</code>更改为实际类别数</p>
<p>在3个<code>[yolo]</code>层之前的 <code>[convolutional]</code>中将<code>[filters = 255]</code>更改为<code>[filters =（classs + 5）x3]</code>，请记住，只需要改每个[yolo]层之前的那个<code>[convolutional]</code> 。因此，如果classes = 1，则应该是filter = 18。 如果class = 2，则filter = 21。详细的公式为：<code>filters=(classes + coords + 1)*&lt;number of mask&gt;</code></p>
<p>使用<code>[Gaussian_yolo]</code>层时，请在每个<code>[Gaussian_yolo]</code>层之前的3个<code>[convolutional]</code>中更改<code>[filters = 57]</code> 为 <code>filter =（classs + 9）x3</code></p>
</li>
<li><p>在目录build \ darknet \ x64 \ data \中创建文件obj.names，内容是一个目标名称一个行。</p>
</li>
<li><p>在目录build \ darknet \ x64 \ data \中创建文件obj.data，内容是：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">classes &#x3D; 2</span><br><span class="line">train  &#x3D; data&#x2F;train.txt</span><br><span class="line">valid  &#x3D; data&#x2F;test.txt</span><br><span class="line">names &#x3D; data&#x2F;obj.names</span><br><span class="line">backup &#x3D; backup&#x2F;</span><br></pre></td></tr></table></figure>
<ol>
<li>给数据集标注好矩形框，标签文件是格式<code>&lt;object-class&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code>的txt文件。其中：</li>
</ol>
<ul>
<li><code>&lt;object-class&gt;</code> - 代表目标类别的整数</li>
<li><code>&lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> - 相对于图像宽度高度的浮点值，位于 (0.0 to 1.0]区间。比如: <code>&lt;x&gt; = &lt;absolute_x&gt; / &lt;image_width&gt;</code> &amp; <code>&lt;height&gt; = * &lt;absolute_height&gt; / &lt;image_height&gt;</code></li>
<li>注意 <x_center> <y_center> - 是矩形框的中心，不是左上角。</li>
</ul>
<ol>
<li>Create file <code>train.txt</code> in directory <code>build\darknet\x64\data\</code>, with filenames of your images, each filename in new line, with path relative to <code>darknet.exe</code>, for example containing:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data&#x2F;obj&#x2F;img1.jpg</span><br><span class="line">data&#x2F;obj&#x2F;img2.jpg</span><br><span class="line">data&#x2F;obj&#x2F;img3.jpg</span><br></pre></td></tr></table></figure>
<ol>
<li>Download pre-trained weights for the convolutional layers and put to the directory <code>build\darknet\x64</code></li>
</ol>
<ul>
<li>for <code>yolov4.cfg</code>, <code>yolov4-custom.cfg</code> (162 MB): <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137">yolov4.conv.137</a> (Google drive mirror <a target="_blank" rel="noopener" href="https://drive.google.com/open?id=1JKF-bdIklxOOVy-2Cr5qdvjgGpmGfcbp">yolov4.conv.137</a> )</li>
<li>for <code>yolov4-tiny.cfg</code>, <code>yolov4-tiny-3l.cfg</code>, <code>yolov4-tiny-custom.cfg</code> (19 MB): <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29">yolov4-tiny.conv.29</a></li>
<li>for <code>csresnext50-panet-spp.cfg</code> (133 MB): <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/16yMYCLQTY_oDlCIZPfn_sab6KD3zgzGq/view?usp=sharing">csresnext50-panet-spp.conv.112</a></li>
<li>for <code>yolov3.cfg, yolov3-spp.cfg</code> (154 MB): <a target="_blank" rel="noopener" href="https://pjreddie.com/media/files/darknet53.conv.74">darknet53.conv.74</a></li>
<li>for <code>yolov3-tiny-prn.cfg , yolov3-tiny.cfg</code> (6 MB): <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/18v36esoXCh-PsOKwyP2GWrpYDptDY8Zf/view?usp=sharing">yolov3-tiny.conv.11</a></li>
<li>for <code>enet-coco.cfg (EfficientNetB0-Yolov3)</code> (14 MB): <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1uhh3D6RSn0ekgmsaTcl-ZW53WBaUDo6j/view?usp=sharing">enetb0-coco.conv.132</a></li>
</ul>
<ol>
<li>Start training by using the command line: <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137</code></li>
</ol>
<p>To train on Linux use command: <code>./darknet detector train data/obj.data yolo-obj.cfg yolov4.conv.137</code> (just use <code>./darknet</code> instead of <code>darknet.exe</code>)</p>
<ul>
<li>(file <code>yolo-obj_last.weights</code> will be saved to the <code>build\darknet\x64\backup\</code> for each 100 iterations)</li>
<li>(file <code>yolo-obj_xxxx.weights</code> will be saved to the <code>build\darknet\x64\backup\</code> for each 1000 iterations)</li>
<li>(to disable Loss-Window use <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -dont_show</code>, if you train on computer without monitor like a cloud Amazon EC2)</li>
<li>(to see the mAP &amp; Loss-chart during training on remote server without GUI, use command <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -dont_show -mjpeg_port 8090 -map</code> then open URL <code>http://ip-address:8090</code> in Chrome/Firefox browser)</li>
</ul>
<p>8.1. For training with mAP (mean average precisions) calculation for each 4 Epochs (set <code>valid=valid.txt</code> or <code>train.txt</code> in <code>obj.data</code> file) and run: <code>darknet.exe detector train data/obj.data yolo-obj.cfg yolov4.conv.137 -map</code></p>
<ol>
<li>After training is complete - get result <code>yolo-obj_final.weights</code> from path <code>build\darknet\x64\backup\</code></li>
</ol>
<ul>
<li><p>After each 100 iterations you can stop and later start training from this point. For example, after 2000 iterations you can stop training, and later just start training using: <code>darknet.exe detector train data/obj.data yolo-obj.cfg backup\yolo-obj_2000.weights</code></p>
<p>(in the original repository <a target="_blank" rel="noopener" href="https://github.com/pjreddie/darknet">https://github.com/pjreddie/darknet</a> the weights-file is saved only once every 10 000 iterations <code>if(iterations &gt; 1000)</code>)</p>
</li>
</ul>
<p><strong>Note:</strong> If during training you see <code>nan</code> values for <code>avg</code> (loss) field - then training goes wrong, but if <code>nan</code> is in some other lines - then training goes well.</p>
<p><strong>Note:</strong> If you changed width= or height= in your cfg-file, then new width and height must be divisible by 32.</p>
<p><strong>Note:</strong> After training use such command for detection: <code>darknet.exe detector test data/obj.data yolo-obj.cfg yolo-obj_8000.weights</code></p>
<p><strong>Note:</strong> if error <code>Out of memory</code> occurs then in <code>.cfg</code>-file you should increase <code>subdivisions=16</code>, 32 or 64。</p>
<h1 id="如何训练YOLOV4-tiny来检测自定义目标"><a href="#如何训练YOLOV4-tiny来检测自定义目标" class="headerlink" title="如何训练YOLOV4 - tiny来检测自定义目标"></a>如何训练YOLOV4 - tiny来检测自定义目标</h1><p>Do all the same steps as for the full yolo model as described above. With the exception of:</p>
<ul>
<li>Download file with the first 29-convolutional layers of yolov4-tiny: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29">https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29</a> (Or get this file from yolov4-tiny.weights file by using command: <code>darknet.exe partial cfg/yolov4-tiny-custom.cfg yolov4-tiny.weights yolov4-tiny.conv.29 29</code></li>
<li>Make your custom model <code>yolov4-tiny-obj.cfg</code> based on <code>cfg/yolov4-tiny-custom.cfg</code> instead of <code>yolov4.cfg</code></li>
<li>Start training: <code>darknet.exe detector train data/obj.data yolov4-tiny-obj.cfg yolov4-tiny.conv.29</code></li>
</ul>
<p>For training Yolo based on other models (<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/densenet201_yolo.cfg">DenseNet201-Yolo</a> or <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/resnet50_yolo.cfg">ResNet50-Yolo</a>), you can download and get pre-trained weights as showed in this file: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd">https://github.com/AlexeyAB/darknet/blob/master/build/darknet/x64/partial.cmd</a> If you made you custom model that isn’t based on other models, then you can train it without pre-trained weights, then will be used random initial weights.</p>
<h1 id="怎样改善检测结果"><a href="#怎样改善检测结果" class="headerlink" title="怎样改善检测结果"></a>怎样改善检测结果</h1><p><a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet#how-to-improve-object-detection">翻译</a></p>
<p><strong>训练前</strong>：</p>
<ul>
<li><p>在您的.cfg文件中设置flag <code>random = 1</code>, 这样它会通过训练Yolo不同的分辨率来提高精度：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L788">链接</a></p>
</li>
<li><p>增加.cfg文件中的网络分辨率（高度= 608，宽度= 608或任何32的倍数）-这将提高精度</p>
</li>
<li><p>检查您要检测的每个目标是否在数据集中都有被标记以及是否被正确标记，如果您想检查目标是否别正确标记，可以在训练的命令行后面加上<code>-show_imgs</code>。</p>
</li>
<li><p>对于您要检测的目标，应该有它：不同比例，不同角度(间隔30度），不同照明，不同背景的图像， 每个类别最好拥有2000张不同的图像，并且您应训练2000 *class以上的迭代。从神经网络的内部角度来看，这些都是不同的对象。因此，要检测的对象越不同，应使用越复杂的网络模型。</p>
</li>
<li><p>希望您的训练数据集包含没有检测目标的图像（对应空的.txt文件）-使用与带有对象的图像一样多的负样本图像</p>
</li>
<li><p>针对图像中有大量对象的情况，训练前请在cfg文件的最后一个[yolo]层或[region]层中添加参数max = 200或更高的值（YoloV3可以检测到的最多目标数量是0,0615234375 <em>（width </em> height），其中width和height是cfg文件中[net]部分的参数）。</p>
</li>
<li><p>要训练小目标（将图像调整为416x416后小于16x16的目标称为小目标），设置<code>layers = 23</code>,而不是<code>layers = 54</code>(<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L895">https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L895</a>)</p>
<p>设置<code>stride = 4</code>而不是<code>stride = 2</code>(<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L892">https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L892</a>)</p>
<p>设置<code>stride = 4</code>而不是<code>stride = 2</code>(<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L989">https://github.com/AlexeyAB/darknet/blob/6f718c257815a984253346bba8fb7aa756c55090/cfg/yolov4.cfg#L989</a>)</p>
</li>
<li><p>对于既要训练大目标，又要训练小目标，请使用修改后的模型：</p>
<p>Full-model：5个yolo层：<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov3_5l.cfg</a></p>
<p>Tiny-model：3个yolo层：<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny_3l.cfg">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-tiny_3l.cfg</a></p>
<p>YOLOv4：3个yolo层：<a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-custom.cfg">https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4-custom.cfg</a>  (比yolov4.cfg多了stopbackward=800，这句代码AlexeyAB原话是这么解释的：put stopbackward=800 before some layer meaning that all the layers’ weights before current layer will not be updated only for the first 800 iterations. )</p>
</li>
<li><p>如果您训练的模型将左对象和右对象区分为单独的类（左/右手，左/右转道路标志，…），则禁用翻转数据增强<code>flip = 0</code>：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17">https://github.com/AlexeyAB/darknet/blob/3d2d0a7c98dbc8923d9ff705b81ff4f7940ea6ff/cfg/yolov3.cfg#L17</a></p>
</li>
<li><p>训练集和测试集中都要有相对图像大小差不多的目标</p>
</li>
<li><p>加快训练速度（伴随降低检测精度）在cfg文件中为第136层设置参数<code>stopbackward = 1</code></p>
</li>
<li><p>为了使检测到的边界框更准确，您可以向每个[yolo]层添加3个参数<code>ignore_thresh = .9</code> <code>iou_normalizer = 0.5</code> <code>iou_loss = giou</code>并进行训练，它将增加mAP@0.9，但减小mAP@0.5。</p>
</li>
<li><p>仅当您是神经检测网络专家时再这么做 ：在cfg文件中重新计算数据集的锚框的宽度和高度：<code>darknet.exe detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416</code>然后设置相同的9个锚点在cfg文件的3个[yolo]图层中的每个图层中。但是您应该为每个[yolo]层更改anchor的遮罩 <code>masks</code>，因此对于YOLOv4，第一层[yolo]层的锚点小于30x30，第二层小于60x60，剩下的第3层，以此类推。同样，您应该在每个[yolo]层之前更改<code>filter =（classs + 5）* &lt;mask的数量&gt;</code>。如果许多计算出的锚不适合在适当的图层下, 那就使用默认锚框即可。</p>
</li>
</ul>
<p><strong>训练后，用于检测：</strong></p>
<ul>
<li>通过在.cfg文件中设置（<code>height= 608</code> 和<code>width= 608</code>）或（高度= 832和宽度= 832）或（任何32的倍数）来提高网络分辨率-这可以提高精度，并可以检测到小物件：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L8-L9">链接</a>。而且若要升高分辨率的话，无需再次从头训练网络，只需使用已经针对416x416分辨率进行训练的.weights文件。如果发生错误，内存不足，则在.cfg文件中，您应该增加<code>subdivisions= 16</code>、32或64：<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet/blob/0039fd26786ab5f71d5af725fc18b3f521e7acfd/cfg/yolov3.cfg#L4">链接</a></li>
</ul>
<h1 id="YOLO-V4的issues-amp-comments"><a href="#YOLO-V4的issues-amp-comments" class="headerlink" title="YOLO V4的issues&amp;comments"></a>YOLO V4的issues&amp;comments</h1><p><strong>Issue 1</strong>. Can Yolo3 take different width-height-ratio images as training input? #800</p>
<p><a target="_blank" rel="noopener" href="https://github.com/pjreddie/darknet/issues/800#issuecomment-390352607">comment 1</a> ，<a target="_blank" rel="noopener" href="https://github.com/pjreddie/darknet/issues/800#issuecomment-609566695">comment 2</a></p>
<p>概括：可以使用不同宽高比的图像进行训练，但这会使得模型的检测效果变糟糕，在yolov4.cfg里，可以使用<code>jitter</code>和<code>random</code>两个超参数来数据增强，减少这个影响。同时，YOLOv4会把训练图像保持宽高比缩放到长边等于416，短边用黑边填充到416。</p>
<p>这带给我们一个启示：“数据集图片尺寸不要跟网络的输入尺寸差距很大”。举个例子，咱们用手机相机拍摄的照片一般都很清晰，我的是4000多×2000多，但是<strong>越大越高分辨率的图片对神经网络来说是越不清晰</strong>！因为网络在预处理是会把图片尺寸除以原始图片的长边，比如我这里是除以4000多像素，那假如我的目标是100×200，转换成608×608的图片后，它的大小变成了15×30，也就是15×30的像素要表达我的那个目标，可想而知这丢失了很多目标信息，这样模型的小目标检测效果就会特别差！所以我建议把数据集的图片裁剪变小，推荐一个<a target="_blank" rel="noopener" href="https://watermarkly.com/crop-photo/">裁剪工具</a>。</p>
<h1 id="智能标注工具"><a href="#智能标注工具" class="headerlink" title="智能标注工具"></a>智能标注工具</h1><p>训练前的数据标注是一个简单重复又意义重要的工作，现在就诞生了几个不错的数据标注工具，它们会帮你比如OPENCV/openvinotoolkit出品的CVAT（Computer Vision Annotation Tool），这里我用它的<a target="_blank" rel="noopener" href="https://cvat.org/">在线版</a>演示一下单张图片单个类别的目标检测标注：</p>
<p>左侧工具栏有个魔术棒图标，我们可也利用它自己生成标注框：</p>
<p><img src="https://pic.downk.cc/item/5f7d93fd1cd1bbb86b6ac487.jpg"></p>
<p>标注结束后记得点一下save，最后就是点menu -&gt; Export as a dataset导出你想要的格式的数据集了：</p>
<p><img src="https://pic.downk.cc/item/5f7db1f41cd1bbb86b7255c1.jpg"></p>
<p>下载下来的数据集如下图所示，我导出的就是YOLO格式：</p>
<p><img src="https://pic.downk.cc/item/5f7d98a21cd1bbb86b6bbcdc.jpg"></p>
<p>这款工具还可能标注实例分割，数据源是视频等，功能还是比较强大的，但是缺点是因为是网页版，所以上传大的数据集容易网络断开。</p>
<p>具有半自动标注的工具了还有：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/virajmavani/semi-auto-image-annotation-tool">semi-auto-image-annotation-tool</a>，不过似乎维护得比较不频繁了，但我觉得他们当时的创新之举还是值得一提的。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/microsoft/VoTT">VOTT</a>，在学位帽图标这里设置，Model Provider让你设置用什么模型预测，是SSD还是自定义模型，Predict让你设置预测出来框是否还给你带上标签Tag，Auto Detect让你设置点击下一张图片后是否自动检测<br><img src="https://pic.downk.cc/item/5f9d724a1cd1bbb86bc764e1.jpg" width=90%></p>
</li>
</ul>
<p>如果不自动检测的话，就得自己手动点下面这个同样学位帽的图标：<br> <img src="https://pic.downk.cc/item/5f9d72741cd1bbb86bc775db.jpg" ></p>
<p> 目前（2020/10/31）VOTT还不支持导出YOLO格式的功能，但这个<a target="_blank" rel="noopener" href="https://github.com/microsoft/VoTT/issues/994">issue</a>里官方给的建议是用他们的<a target="_blank" rel="noopener" href="https://roboflow.com/convert/vott-json-to-yolo-darknet-txt">Roboflow</a>转换工具把VoTT Json格式转换成YOLO txt格式。</p>
<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><p><strong>Tiny YOLOv4</strong> V.S. <strong>MobileNet SSD</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IAL480andOQ&amp;list=PL_Nji0JOuXg2E6QVMwCrLOzzTmr36fFcH&amp;index=2&amp;ab_channel=AugmentedStartups">https://www.youtube.com/watch?v=IAL480andOQ&amp;list=PL_Nji0JOuXg2E6QVMwCrLOzzTmr36fFcH&amp;index=2&amp;ab_channel=AugmentedStartups</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/YOLO/" rel="tag">YOLO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-电机类型与FOC控制"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/29/%E7%94%B5%E6%9C%BA%E7%B1%BB%E5%9E%8B%E4%B8%8EFOC%E6%8E%A7%E5%88%B6/"
    >电机类型与FOC控制</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/09/29/%E7%94%B5%E6%9C%BA%E7%B1%BB%E5%9E%8B%E4%B8%8EFOC%E6%8E%A7%E5%88%B6/" class="article-date">
  <time datetime="2020-09-29T11:25:07.000Z" itemprop="datePublished">2020-09-29</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>RMF（Rotating Magnetic Field）</p>
<p>感应电机工作原理：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=AQqyGNOP_3o">YouTube视频</a></p>
<p>永磁同步电机工作原理：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Vk2jDXxZIhs">YouTube视频</a></p>
<p>同步磁阻电动机：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=vvw6k4ppUZU">YouTube视频</a></p>
<p>B站UP主稚晖君做过的一期<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV11V41127pq"><strong>视频</strong></a>里演示了FOC算法控制直流无刷电机的效果，对应地，他也写了一篇<a target="_blank" rel="noopener" href="http://pengzhihui.xyz/2020/07/02/foc/#more"><strong>博客</strong></a>，博客里更多的是介绍FOC控制电机的原理。</p>
<p>视频中他提到他设计的Ctrl FOC驱动器的板子是参考了VESC和ODrive,VESC名字里的V是这块板子的创造者Benjamin Vedder的姓氏首字母是V，ESC代表Electronic Speed Controller。<br><img src="https://s1.ax1x.com/2020/09/29/0ejdSS.png" alt="0ejdSS.png" border="0" /></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/FOC/" rel="tag">FOC</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%94%B5%E6%9C%BA-%E9%A9%AC%E8%BE%BE/" rel="tag">电机/马达</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-损失函数"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/26/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"
    >损失函数</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/09/26/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" class="article-date">
  <time datetime="2020-09-26T03:00:25.000Z" itemprop="datePublished">2020-09-26</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a> / <a class="article-category-link" href="/categories/CV/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>目标检测任务的损失函数由Classificition Loss和Bounding Box Regeression Loss两部分构成。本文介绍目标检测任务中近几年来Bounding Box Regression Loss Function的演进过程，其演进路线是Smooth L1 Loss → IoU Loss → GIoU Loss → DIoU Loss → CIoU Loss，本文按照此路线进行讲解。</p>
<h2 id="1-Smooth-L1-Loss"><a href="#1-Smooth-L1-Loss" class="headerlink" title="1. Smooth L1 Loss"></a>1. <strong>Smooth L1 Loss</strong></h2><p>在Faster R-CNN以及SSD中对边框的回归使用的损失函数都是Smooth L1 作为损失函数，其定义为：</p>
<script type="math/tex; mode=display">SmoothL_1(x) = 
\begin{cases}
    0.5x^2 \qquad if|x|<1\\
    |x| - 0.5   \quad otherwise
\end{cases}</script><p>其中，$x=f(x_i)−y_i$ 为预测值和真实值的差值。</p>
<p>实际目标检测框回归任务中的损失loss为</p>
<script type="math/tex; mode=display">L(g,p) = \sum_{i\in{(x,y,w,h)}}smooth_{L1}(g_i - p_i)</script><p>其中 $g=(g_x,g_y,g_w,g_h)$ 表示Ground Truth的框坐标，  $p=(p_x,p_y,p_w,p_h)$ 表示Prediction的框坐标，即分别求4个点的loss，然后相加作为Bounding Box Regression Loss。</p>
<p><strong>对比 L1 Loss 和 L2 Loss</strong></p>
<p>L1 loss可以看做是平均绝对误差（Mean Absolute Error,MAE) 是指模型预测值f(x)和真实值y之间距离的平均值，其公式如下：</p>
<script type="math/tex; mode=display">M A E=\frac{\sum_{n=1}^{n}\left|f\left(x_{i}\right)-y_{i}\right|}{n}</script><p>其中，$y_i$和$f(x_i)$分别表示第i个样本的真实值及其对应的预测值，n为样本的个数。</p>
<p>忽略下标 i ，设n=1，以f(x)−y为横轴，MAE的值为纵轴，得到函数的图形如下：</p>
<p><img src="https://pic.downk.cc/item/5f905bbd1cd1bbb86b8f3fec.png"></p>
<p>相比于MSE，MAE有个优点就是，对于离群点不那么敏感。</p>
<p><img src="https://pic.downk.cc/item/5f905bd71cd1bbb86b8f479a.png"></p>
<p>L2 loss可以看作是均方误差（Mean Square Error,MSE）是模型预测值f(x) 与真实样本值y 之间差值平方的平均值，其公式如下;</p>
<script type="math/tex; mode=display">M S E=\frac{\sum_{i=1}^{n}\left(f_{x_{i}}-y_{i}\right)^{2}}{n}</script><p>忽略下标 i ，设n=1，以f(x)−y为横轴，MSE的值为纵轴，得到函数的图形如下：</p>
<p><img src="https://pic.downk.cc/item/5f905fd11cd1bbb86b90638f.png"></p>
<p>如果样本中存在离群点，MSE会给离群点更高的权重，这就会牺牲其他正常点数据的预测效果，最终降低整体的模型性能。 如下图：</p>
<p><img src="https://pic.downk.cc/item/5f905bfe1cd1bbb86b8f5299.png"></p>
<p>可见，使用 MSE 损失函数，受离群点的影响较大，虽然样本中只有 5 个离群点，但是拟合的直线还是比较偏向于离群点。</p>
<p>假设$x=f(x_i)−y_i$ ，则上面的公式可以概括为：</p>
<script type="math/tex; mode=display">L_1(x) = ± x</script><script type="math/tex; mode=display">L_2(x) = x^2</script><script type="math/tex; mode=display">SmoothL_1(x) = 
\begin{cases}
    0.5x^2 \qquad if|x|<1\\
    |x| - 0.5   \quad otherwise
\end{cases}</script><p>上面损失函数对$x$的导数为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L_{1}(x)}{\partial x} &=\left\{\begin{array}{cl}
1 & \text { if } x \geq 0 \\
-1 & \text { otherwise }
\end{array}\right.\\

\frac{\partial L_{2}(x)}{\partial x} &=2 x \\

\frac{\partial \operatorname{smooth}_{L_{1}}(x)}{\partial x} &=\left\{\begin{array}{ll}
x & \text { if }|x|<1 \\
\pm 1 & \text { otherwise }
\end{array}\right.
\end{aligned}</script><p>上面导数可以看出：</p>
<p>对于L1导数,L1对x的导数为常数，在<strong>训练后期</strong>，预测值与ground truth差异很小时，L1的导数的绝对值仍然为1，而 learning rate 如果不变，损失函数将在稳定值附近波动，难以继续收敛以达到更高精度。</p>
<p>对于L2导数，当x增大时，L2的损失也增大。 这就导致在<strong>训练初期</strong>，预测值与 groud truth 差异过于大时，损失函数对预测值的梯度十分大，训练不稳定。</p>
<p>对于Smooth L1导数，Smotth L1在x较小时，对x的梯度也会变小。 而当x较大时，对x的梯度的上限为1，也不会太大以至于破坏网络参数。Smooth L1完美的避开了L1和L2作为损失函数的缺陷。</p>
<p><strong>L1 Loss ,L2 Loss以及SmoothL1 放在一起的函数曲线对比</strong></p>
<p><img src="https://pic.downk.cc/item/5f905c111cd1bbb86b8f5910.png"></p>
<p>从上面可以看出，该函数实际上就是一个分段函数，在[-1,1]之间实际上就是L2损失，这样解决了L1的不光滑问题，在[-1,1]区间外，实际上就是L1损失，这样就解决了离群点梯度爆炸的问题。</p>
<h2 id="IOU-Loss"><a href="#IOU-Loss" class="headerlink" title="IOU_Loss"></a><strong>IOU_Loss</strong></h2><p><img src=https://pic3.zhimg.com/80/v2-c812620791de642ccb7edcde9e1bd742_720w.jpg></p>
<p>可以看到IOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题:</p>
<p><img src=https://picb.zhimg.com/80/v2-e3d9a882dec6bb5847be80899bb98ea3_720w.jpg></p>
<p>问题1：即上图中状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。</p>
<p>问题2：即上图中状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。</p>
<p>因此2019年出现了GIOU_Loss来进行改进。</p>
<h2 id="GIOU-Loss-Generalized-IOU"><a href="#GIOU-Loss-Generalized-IOU" class="headerlink" title="GIOU_Loss(Generalized IOU)"></a><strong>GIOU_Loss(Generalized IOU)</strong></h2><p><img src=https://pic1.zhimg.com/80/v2-443123f1aa540f7dfdc84b233edcdc67_720w.jpg></p>
<p>GIOU的创新之处在于考虑了<strong>最小外接矩形</strong>和<strong>差集</strong>这两个指标，用差集除以最小外接矩形就能区分开IOU问题里的状态2和状态3了，状态3的差集除以最小外接矩形的值更小，所以状态3的GIOU更大，GIOU_LOSS更小。</p>
<p>但是GIOU还存在一种不足：</p>
<p><img src=https://pic1.zhimg.com/80/v2-49024c2ded9faafe7639c5207e575ed6_720w.jpg></p>
<p>问题：状态1、2、3都是 <strong>预测框在目标框内部</strong> 且 <strong>预测框大小一致</strong> 的情况，这时预测框和目标框的差集和最小外接矩形都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。<br>基于这个问题，2020年的AAAI出现了DIOU_Loss。</p>
<h2 id="DIOU-Loss-Distance-IoU"><a href="#DIOU-Loss-Distance-IoU" class="headerlink" title="DIOU_Loss(Distance-IoU)"></a><strong>DIOU_Loss(Distance-IoU)</strong></h2><p><img src=https://pic4.zhimg.com/80/v2-029f094658e87f441bf30c80cb8d07d0_720w.jpg></p>
<p>DIOU的定义和GIOU很像，相当于把 “差集面积/最小外接矩形面积” 替换成 “中心点距离平方/最小外接矩形对角线距离平方”，公式长这个样子：</p>
<script type="math/tex; mode=display">D I o U=I o U-\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}</script><p>但是DIOU仍然有缺点，那就是没有考虑到长宽比。</p>
<p><img src=https://pic4.zhimg.com/80/v2-22bf2e9c8a2fbbbb877e0f1ede69009f_720w.jpg></p>
<p>比如上面三种情况，目标框包裹预测框，预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的，但是这3种状态中，看起来应该是状态1最好吧。那应该怎么区别这3种状态的好坏呢？</p>
<p>针对这个问题，又提出了CIOU_Loss，不得不说，好的解决方案都是前人用时间一点一点积累起来的！</p>
<h2 id="CIOU-Loss-Complete-IoU"><a href="#CIOU-Loss-Complete-IoU" class="headerlink" title="CIOU_Loss(Complete-IoU)"></a><strong>CIOU_Loss(Complete-IoU)</strong></h2><p>CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子$\alpha v$，将预测框和目标框的长宽比都考虑了进去。</p>
<p>其中$v$是衡量长宽比一致性的参数，定义为：</p>
<script type="math/tex; mode=display">v=\frac{4}{\pi^{2}}\left(\arctan \frac{w^{g t}}{h^{g t}}-\arctan \frac{w}{h}\right)^{2}</script><p>$\alpha$ 是用于做trade-off的参数:</p>
<script type="math/tex; mode=display">\alpha=\frac{v}{(1-I o U)+v}</script><p>完整的定义为$L_{C I o U}=1-I o U+\frac{\rho^{2}\left(b, b^{g t}\right)}{c^{2}}+\alpha v$，</p>
<p>或者在我们这里是：$\mathrm{ClOU}_{-} \mathrm{Loss}=1 - \mathrm{CIOU}=1-\left(\mathrm{I} 0 \mathrm{U}-\frac{\text { Distance } 2^{2}}{\text { Distance }-\mathrm{C}^{2}}-\frac{v^{2}}{(1-\mathrm{IOU})+v}\right)$</p>
<p>这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。</p>
<p>再来综合的看下各个Loss函数的不同点：</p>
<p>IOU_Loss：主要考虑检测框和目标框重叠面积。</p>
<p>GIOU_Loss：在IOU的基础上，考虑了非包围情况下，两个矩形框形成的几何面积的信息。</p>
<p>DIOU_Loss：在IOU和GIOU的基础上，考虑包围情况下，两个矩形框中心点距离的信息。</p>
<p>CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的信息。</p>
<p>Yolov4中采用了CIOU_Loss的回归方式，使得预测框回归的精度更高一些。</p>
<p>上面的各种IOU不仅可以用来基于先验框的预测框的回归训练，还可以用在NMS上。常用的目标检测算法中，一般采用普通的NMS来进行预测框的筛选，Yolov4则借鉴上面D/CIOU来进行预测框的筛选，论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.08287.pdf">https://arxiv.org/pdf/1911.08287.pdf</a></p>
<p>将其中计算IOU的部分替换成DIOU的方式：</p>
<p>来看下实际的案例</p>
<p><img src=https://pic4.zhimg.com/80/v2-ddb336d26adb2a2e37415b6266c88ec6_720w.jpg></p>
<p>在上图重叠的摩托车检测中，黄色箭头所指的中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。</p>
<p>因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。</p>
<p>注意：有读者会有疑问，这里为什么不用CIOU_nms，而用DIOU_nms?</p>
<p>答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。</p>
<p>但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。</p>
<h2 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a><strong>Focal Loss</strong></h2><p>Focal loss主要是为了解决one-stage目标检测中正负样本比例严重失衡的问题。<br>我们知道，anchor-based算法会产生大量的背景简单负样本 、少量的目标简单正样本、中量的背景目标过渡区域困难正、负样本。现在考虑数量最多的简单背景负样本，这些简单背景负样本很容易被分类为背景，所以它们的损失很小，但毕竟负样本数量太多，模型还是不得不考虑这些样本，所以最终的结果是，模型优化了大量背景负样本的损失，而忽视了优化那些目标样本的损失，也就是说优化重点跑偏了。同时因为背景负样本很容易分类，它们的损失已经低到不能再低了，所以还会导致模型的loss后期很难继续收敛。</p>
<p><img src="https://s1.ax1x.com/2020/10/02/0QAJG8.png" alt="0QAJG8.png" border="0" /></p>
<p>OHEM（online hard example mining）是后来针对正负样本不均衡提出的一种筛选example的方法，OHEM的主要思想可以用原文的一句话概括：In OHEM each example is scored by its loss, non-maximum suppression (NMS) is then applied, and a minibatch is constructed with the highest-loss examples。OHEM算法虽然增加了困难样本的权重，但是却忽略了容易分类的样本，结果它把所有的easy example都去除掉，造成无法用easy positive example进一步提升训练的精度。</p>
<p>于是后来2017年何凯明等人提出了Focal loss，他提出只要通过将原先训练回归任务常用的 交叉熵误差CE (Cross Entropy) 改为 FL (Focal Loss) 即可，首先回顾二分类交叉上损失：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{Cross Entropy}(y,y')=-\mathrm{ylog} y^{\prime}-(1-y) \log \left(1-y^{\prime}\right)=\left\{\begin{array}{ll}
-\log y^{\prime} \quad & y=1 \\
-\log \left(1-y^{\prime}\right) & y=0
\end{array}\right.</script><p>$y’$是样本属于正样本的概率值。可见普通的交叉熵对于正样本(y=1)而言，$y’$越大损失越小。对于负样本(y=0)而言，$y’$越小则损失越小。</p>
<p>下面我们来看一下Focal Loss的公式：</p>
<script type="math/tex; mode=display">
\mathrm{L}_{FocalLoss}(y,y')=\left\{\begin{array}{ll}
-\alpha\left(1-y^{\prime}\right)^{\gamma} \log y^{\prime} & y=1 \\
-(1-\alpha) y^{\prime \gamma} \log \left(1-y^{\prime}\right) & y=0
\end{array}\right.</script><p>在公式中，α主要负责调节正负样本的失衡，γ主要负责调节难易样本的失衡。</p>
<p>先不管α，先思考一下γ的作用逻辑。对于正样本（y=1），其易分类的样本的$y’$趋近于1，所以$(1-y’)^\gamma$很小；对于负样本（y=0），其易分类的样本$y’$趋近于0，所以$y’^\gamma$也很小。合起来说就是，对于易分类的（well-classified）样本的权值都被调低了。</p>
<p>文章也对于 $\gamma,\alpha$ 的取值做了一番实验：</p>
<p><img src="https://s1.ax1x.com/2020/10/02/01EBm4.png" alt="01EBm4.png" border="0" /></p>
<p>在实验中，发现当γ=0的时候，α=0.75的效果最好，但γ不为0时，则是 $\gamma = 2, \alpha = 0.25$ 的取值组合效果最好，这一点是有点反直觉，因为正样本竟然被分配了一个更小的权重，我的理解是可能$y’^\gamma$对负样本的权重影响更大，$y’^\gamma$使得简单负样本的权重仍然很小。</p>
<p>为此，FAIR团队还专门写了一个简单的one-stage detector来验证focal loss的强大。并将该网络结构起名RetinaNet：</p>
<p><img src=https://pic1.zhimg.com/v2-f3ef4c6e92c2f6d9e2782e7e32361042_1440w.jpg></p>
<p>最后赋个具体的值来感受一下这个公式的作用：</p>
<p>假设我们有1000000个 $p_t = 0.99$ 的简单负样本（根据下图可以算出这个负样本的正样本概率为p=0.01，awesome）和 10个 $p_t=0.01$ 的困难正样本（根据下图可以算出这个正样本的正样本的概率只有p=0.01，awful）。alpha取0.25，则负样本的$\alpha_t$=1-α=0.75，正样本的$\alpha_t$=α=0.25。γ取2。负样本的$\alpha_{t}\left(1-p_{t}\right)^{\gamma}=0.000075$，正样本的$\alpha_{t}\left(1-p_{t}\right)^{\gamma}=0.245025$。</p>
<p><img src="https://s1.ax1x.com/2020/10/03/01umfs.png" alt="01umfs.png" border="0" width=70%/><br><img src="https://s1.ax1x.com/2020/10/03/01KpEF.png" alt="01KpEF.png" border="0" width=40%/></p>
<p>在使用普通交叉熵损失的情况下，来自负样本的损失为 1000000$\cdot$log(0.99)=1000000×0.0043648054=4364 正面例子的损失是 10$\cdot$log(0.01)=10×2=20。正例的损失贡献为20/(4364+20)=0.0046。几乎可以忽略不计。</p>
<p>在使用焦点损失的情况下，来自负面示例的损失为1000000×0.0043648054×0.000075=0.3274 正面例子的损失是 10×2×0.245025=4.901。正例的损失贡献为4.901/(4.901+0.3274)=0.9374！现在正例主宰了总损失！</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wangguchangqing/p/12021638.html">《回归损失函数1：L1 loss, L2 loss以及Smooth L1 Loss的对比》</a><br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143747206">《深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5核心基础知识完整讲解》</a><br><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/JNingWei/article/details/80038594">《论文阅读: RetinaNet》</a><br><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.02002.pdf">《Focal Loss for Dense Object Detection》</a><br><br><a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Focal-Loss-Explained/">《Use Focal Loss To Train Model Using Imbalanced Dataset》</a></p>
</blockquote>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" rel="tag">损失函数</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
    <article
  id="post-掌握期权交易"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/09/12/%E6%8E%8C%E6%8F%A1%E6%9C%9F%E6%9D%83%E4%BA%A4%E6%98%93/"
    >掌握期权交易</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/09/12/%E6%8E%8C%E6%8F%A1%E6%9C%9F%E6%9D%83%E4%BA%A4%E6%98%93/" class="article-date">
  <time datetime="2020-09-12T10:16:40.000Z" itemprop="datePublished">2020-09-12</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%90%86%E8%B4%A2/">理财</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>想要交易期权的门槛比较高，有几条要求是存款大于50万，通过期权知识测试，提供过往期权模拟交易证明等。现在我暂时交易不了，就当先储备知识。</p>
<p>推荐视频：《<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=IunTigepa1I&amp;list=PLN5R8P-UTWKOZqWpONYRTrjVc1FySLW03">秒懂期权系列-看完就赚！</a>》</p>
<h2 id="期权基本概念"><a href="#期权基本概念" class="headerlink" title="期权基本概念"></a>期权基本概念</h2><p><HR><br><strong>期权（Option）</strong>，是一种选择权，指是一种能在未来某特定时间（<strong>到期日：expiration data</strong>）以特定价格（<strong>行权价/敲定价：strike price</strong>）买入或卖出一定数量的某种特定商品（<strong>标的资产：underlying asset</strong>）的权利。期权的分类</p>
<p><strong>1、按期权的权利划分，有看涨期权和看跌期权两种类型。</strong></p>
<p>　　看涨期权（Call Options）是指期权的买方向期权的卖方支付一定数额的权利金后，即拥有在期权合约的有效期内，按事先约定的价格<strong>向期权卖方买入</strong>一定数量的期权合约规定的特定商品的权利，但不负有必须买进的义务。而<strong>期权卖方</strong>有义务在期权规定的有效期内，应期权买方的要求，以期权合约事先规定的价格<strong>卖出</strong>期权合约规定的特定商品。</p>
<p>　　看跌期权（Put Options）是指期权的买方向期权的卖方支付一定数额的权利金后，即拥有在期权合约的有效期内，按事先约定的价格<strong>向期权卖方卖出</strong>一定数量的期权合约规定的特定商品的权利，但不负有必须卖出的义务。而<strong>期权卖方</strong>有义务在期权规定的有效期内，应期权买方的要求，以期权合约事先规定的价格<strong>买入</strong>期权合约规定的特定商品。</p>
<p><strong>2、按期权的交割时间划分，有美式期权、欧式期权和百慕大期权三种类型。</strong></p>
<p>美式期权是指在期权合约规定的有效期内任何时候都可以行使权利。欧式期权是指在期权合约规定的到期日方可行使权利，期权的买方在合约到期日之前不能行使权利，过了期限，合约则自动作废。百慕大期权（Bermuda option）是一种可以在到期日前所规定的一系列时间段行权的期权，百慕大期权可以被视为美式期权与欧式期权的混合体，如同百慕大群岛混合了美国文化和英国文化一样。目前中国新兴的外汇期权业务，类似于欧式期权，但又有所不同。</p>
<p><strong>3、按期权合约上的标的划分，有股票期权、股指期权、利率期权、商品期权以及外汇期权等种类。</strong></p>
<h2 id="期权的构成要素"><a href="#期权的构成要素" class="headerlink" title="期权的构成要素"></a>期权的构成要素</h2><p>　　(1)执行价格，又称履约价格（strike price）：期权的买方行使权利时事先规定的标的物买卖价格。</p>
<p>　　(2)权利金（premium）： 期权的买方支付的期权价格，即买方为获得期权而付给期权卖方的费用。</p>
<p>　　(3)保证金：期权卖方必须存入交易所用于履约的财力担保。</p>
<p>　　(4)看涨期权（call option）和看跌期权（put option）：看涨期权，是指在期权合约有效期内按执行价格买进一定数量标的物的权利；看跌期权，是指卖出标的物的权利。</p>
<p>当期权买方预期<strong>标的物</strong>价格会<strong>超出执行价格</strong>时，他就会<strong>买进看涨期权</strong>，这样他就可以<strong>从期权卖方低价买入，再高价卖出到市场</strong>；</p>
<p>当期权买方预期<strong>标的物</strong>价格会<strong>低于执行价格</strong>时，他就会<strong>买进看跌期权</strong>，这样他就可以<strong>从市场低价买入，再高价卖出给期权卖方</strong>。</p>
<h2 id="期权的价格-Market-Price-of-Option"><a href="#期权的价格-Market-Price-of-Option" class="headerlink" title="期权的价格 (Market Price of Option)"></a>期权的价格 (Market Price of Option)</h2><p>期权的价格称为“权利金”或者“期权费”。权利金是期权合约中的唯一变量，期权合约上的其他要素，如：执行价格、合约到期日、交易品种、交易金额、交易时间、交易地点等要素都是在合约中事先规定好的，是标准化的，而期权的价格是是由交易者在交易所里竞价得出的。</p>
<p><strong>期权价格主要由内涵价值、时间价值两部分组成。</strong></p>
<p>　　1、内涵价值：（Intrinsic Value)</p>
<p>　　内涵价值指<strong>买方</strong>立即履行合约时可获取的总利润。具体来说，可以分为实值期权、虚值期权和两平期权。</p>
<p>　　（1）实值期权</p>
<p>　　当看涨期权的执行价格低于当时的实际价格时，或者当看跌期权的执行价格高于当时的实际价格时，该期权为实值期权。</p>
<p>　　（2）虚值期权</p>
<p>　　当看涨期权的执行价格高于当时的实际价格时，或者当看跌期权的执行价格低于当时的实际价格时，该期权为虚值期权。当期权为虚值期权时，内涵价值为零。</p>
<p>　　（3）两平期权</p>
<p>　　当看涨期权的执行价格等于当时的实际价格时，或者当看跌期权的执行价格等于当时的实际价格时，该期权为两平期权。当期权为两平期权时，内涵价值为零。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">看涨期权</th>
<th style="text-align:center">看跌期权</th>
<th style="text-align:center">结论</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">期权执行价格&lt;实际价格</td>
<td style="text-align:center">期权执行价格&gt;实际价格</td>
<td style="text-align:center">实值期权</td>
</tr>
<tr>
<td style="text-align:center">期权执行价格&gt;实际价格</td>
<td style="text-align:center">期权执行价格&lt;实际价格</td>
<td style="text-align:center">虚值期权</td>
</tr>
<tr>
<td style="text-align:center">期权执行价格=实际价格</td>
<td style="text-align:center">期权执行价格=实际价格</td>
<td style="text-align:center">两平期权</td>
</tr>
</tbody>
</table>
</div>
<p>2、时间价值(Time Value)</p>
<p>　　期权距到期日时间越长，大幅度价格变动的可能性越大，期权买方执行期权获利的机会也越大。与较短期的期权相比，期权买方对较长时间的期权的应付出更高的权利金。</p>
<p><img src=https://wiki.mbalib.com/w/images/5/53/%E6%9C%9F%E6%9D%83%E7%9A%84%E6%97%B6%E9%97%B4%E4%BB%B7%E5%80%BC1.jpg></p>
<p>　　期权的时间价值随着到期日的临近而减少，期权到期日的时间价值为零。</p>
<p>　　期权的时间价值反映了期权交易期间时间风险和价格波动风险，当合约0%或100%履约时，期权的时间价值为零。</p>
<p>综上所述，三种期权的价格表为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">看涨期权</th>
<th style="text-align:center">期权价格（未到期）</th>
<th style="text-align:center">期权价格（到期）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">实值期权</td>
<td style="text-align:center">内涵价值+时间价值</td>
<td style="text-align:center">内涵价值</td>
</tr>
<tr>
<td style="text-align:center">虚值期权</td>
<td style="text-align:center">时间价值</td>
<td style="text-align:center">零</td>
</tr>
<tr>
<td style="text-align:center">两平期权</td>
<td style="text-align:center">时间价值</td>
<td style="text-align:center">零</td>
</tr>
</tbody>
</table>
</div>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>　中国银行期权宝7月11日报价</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">标的汇率</th>
<th>期权类别</th>
<th>协定价格</th>
<th>到期日</th>
<th>期权费率（%）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">澳元/美元</td>
<td>看跌</td>
<td>0.6820</td>
<td>2003-7-23</td>
<td>4.34</td>
</tr>
<tr>
<td style="text-align:center">澳元/美元</td>
<td>看涨</td>
<td>0.6820</td>
<td>2003-7-23</td>
<td>0.30</td>
</tr>
</tbody>
</table>
</div>
<p>以上是澳元从7月5日0.6845大幅下跌至0.6486后的中国银行“期权宝”7月11日的报价。由于澳元出现了大幅下挫，7月11日执行价格为 0.6820、到期日为7月23日的澳元/美元看跌期权，由于离到期日只有12天，而期权实值非常大，因而其期权费报价达到4.34%的高价；而同样执行价格和到期日的看涨期权，由于其内涵价值为虚值，期权费报价却只有0.30%。从另一个角度来看，澳元/美元在大幅下跌后，市场普遍认为在短期内澳元/美元回到0.6820的可能性非常低，因此到期日非常近、执行价格较高的看跌期权的期权费自然高，而看涨期权的期权费则较低。</p>
<h2 id="期权基本交易方式"><a href="#期权基本交易方式" class="headerlink" title="期权基本交易方式"></a>期权基本交易方式</h2><p>从前面我们知道，期权可以分为看涨期权和看跌期权两种类型，而期权交易者又可有买入期权或者卖出期权两种操作，所以期权交易有四种基本策略：买进看涨期权、卖出看涨期权、买进看跌期权、卖出看跌期权。</p>
<p>　　1、买入看涨期权</p>
<p>　　若交易者买进看涨期权，之后市场价格果然上涨，且升至执行价格之上，则交易者可执行期权从而获利。从理论上说，价格可以无限上涨，所以买入看涨期权的盈利理论上是无限大。若到期一直未升到执行价格之上，则交易者可放弃期权，其最大损失为期权费（下图中的左上图）。</p>
<p>　　2、卖出看涨期权</p>
<p>　　若交易者卖出看涨期权，在到期日之前没能升至执行价格之上，则作为看涨期权的买方将会放弃期权，而看涨期权的卖方就会取得期权费的收入。反之，看涨期权的买方将会要求执行期权，期权的卖方的损失为：（市场价格-执行价格）-期权费（下图中的右上图）。</p>
<p>　　3、买入看跌期权</p>
<p>　　若交易者买进看跌期权，之后市场价格果然下跌，且跌至执行价格之下，则交易者可执行期权从而获利，由于价格不可能跌到负数，所以买入看跌期权的最大盈利为执行价格减去标的价格再减去期权费。若到期一直涨到执行价格之上，则交易者可放弃期权，其最大损失为期权费。投资者如果买了一只预感会下跌的股票，则可以同时买看跌期权对冲。</p>
<p>　　4、卖出看跌期权</p>
<p>　　若交易者卖出看跌期权，在到期日之前没能跌至执行价格之下，则作为看跌期权的买方将会放弃期权，而看跌期权的卖方就会取得期权费的收入。反之，看跌期权的买方将会要求执行期权，期权的卖方将损失执行价格减去市场价格和期权费的差。</p>
<p>期权交易盈亏图</p>
<p><img src="https://pic.downk.cc/item/5f7f2ce41cd1bbb86bd28cde.jpg"></p>
<p>总结来说：作为期权买入方，最大亏损为期权费。作为期权卖出方，最大盈利为期权费。<br>所以期权有点像保险产品，你买一份保险最大亏损为保险费，万一你出什么事了，就可以获取数倍理赔的收益。保险公司在大家没集体出事时，就是赚取众人交的保险费。</p>
<p>具体交易案例可见<a target="_blank" rel="noopener" href="http://baijiahao.baidu.com/s?id=1661565680231378352&amp;wfr=spider&amp;for=pc">这篇文章</a>。</p>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://wiki.mbalib.com/wiki/%E6%9C%9F%E6%9D%83">《期权-智库百科》</a><br><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=FXFz3zJGLvg">《期权交易 | 应付下跌+把握上涨，同时抓！颠覆你对于投资的认知！》</a><br><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xgFAKDdW1po">《股市太高，机会所剩无几？会赚钱的人，都在玩期权！4步骤，搞定期权交易！》</a><br><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=tozJLdS7FdA">《散户必看期权策略，教你正确抵抗下跌风险！Covered call &amp; Beta Hedging》</a></p>
</blockquote>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%9F%E6%9D%83/" rel="tag">期权</a></li></ul>

    </footer>
  </div>

    
 
   
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2021
        <i class="ri-heart-fill heart_icon"></i> Wade Wang
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/yoga.png" alt="Hello World"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="http://wwdok.lofter.com">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯果汁吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>