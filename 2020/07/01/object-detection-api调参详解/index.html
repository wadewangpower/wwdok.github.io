<!DOCTYPE html>


<html lang="ch">


<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    object detection api调参详解 |  Hello World
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/planets.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-172408389-1', 'auto');
ga('send', 'pageview');

</script>



  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?da92a6672e51fa2d1c3bacf2dba555c6";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

</html>

<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-object-detection-api调参详解"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  object detection api调参详解
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/07/01/object-detection-api%E8%B0%83%E5%8F%82%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time datetime="2020-07-01T13:53:45.000Z" itemprop="datePublished">2020-07-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/CV/">CV</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">7.4k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">29 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a><strong>一、引言</strong></h2><p>使用谷歌提供的<code>object detection api</code>图像识别框架，我们可以很方便地重新训练一个预训练模型，用于自己的具体业务。以我所使用的ssd_mobilenet_v1预训练模型为例，训练所需参数都在<code>research\object_detection\samples\configs</code>文件夹下的<code>ssd_mobilenet_v1_coco.config</code>中预先配置了，只需对少量参数做修改即可训练。</p>
<p>但是如果你只会修改里面的类别数目和文件路径，那么就有很多缺点：一是无法理解训练参数背后的原理，不利于技术积累；二是一旦遇到需要优化的问题时，不知道如何调整训练参数。例如，我使用默认配置的训练参数对模型进行长期训练后，发现模型始终无法收敛，loss值一直在3~5的范围内波动，没有继续下降。但在没有弄清楚训练参数如何调整之前，我一直没能解决该问题。</p>
<p>所以，我们必须弄清楚每个训练参数的出处、含义、数值调整范围，才能自行对训练文件做合理配置，从而灵活解决各类训练问题。</p>
<p>本文以ssd_mobilenet_v1预训练模型为例，详细解释其训练参数的含义及调整范围。对其它预训练模型的训练参数的分析方法类似，不再逐一展开。</p>
<h2 id="二、正文"><a href="#二、正文" class="headerlink" title="二、正文"></a><strong>二、正文</strong></h2><p>首先简单解释一下，object_detection api框架将训练参数的配置、参数的可配置数值的声明、参数类的定义，分开放置在不同文件夹里。训练参数的配置放在了<code>object_detection\samples\configs</code>文件夹下的.config文件中，参数的可配置数值的声明写在了<code>object_detection\protos</code>文件夹下对应参数名的.proto文件中，参数类的定义则放在了object_detection总文件夹下对应参数名的子文件夹里或者是<code>object_detection\core</code>文件夹里。</p>
<p><em>*</em>.config文件里包含5个部分：model, train_config, train_input_reader, eval_config, eval_input_reader。以ssd_mobilenet_v1_pets.config为例，打开该文件，按照从上到下、从外层到内层的顺序，依次解释各训练参数。</p>
<span id="more"></span>
<p><strong>2.1  model{ }</strong></p>
<p>包含了ssd{ }。</p>
<p><strong>2.1.1  ssd { }</strong></p>
<p>包含了SSD算法使用的各类训练参数。从2.1.2开始逐个展开解释。</p>
<p><strong>2.1.2  num_classes</strong></p>
<p>类别数目。例如，我的数据集包含摩托车、自行车、小轿车，那么它就等于3。</p>
<p><strong>2.1.3  box_coder、faster_rcnn_box_coder</strong></p>
<pre><code>box_coder &#123;
    faster_rcnn_box_coder &#123;
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
    &#125;
&#125;
</code></pre><p>这部分参数用于设置box编解码的方式。可选参数值：</p>
<pre><code>FasterRcnnBoxCoder faster_rcnn_box_coder = 1;
MeanStddevBoxCoder mean_stddev_box_coder = 2;
SquareBoxCoder square_box_coder = 3;
KeypointBoxCoder keypoint_box_coder = 4;
</code></pre><p>SSD算法借鉴了Faster-RCNN的思想，所以这里的设置应该选faster_rcnn_box_coder。</p>
<p>Faster-RCNN中，bounding box的坐标值可以用两种不同的坐标系表示：一种坐标系以图片左上角作为原点，我称其为绝对坐标系；另一种坐标系以用于参考的anchor boxes的中心点位置作为原点，我称其为相对坐标系。</p>
<p>所谓box编码就是以anchor box为参照系，将box的绝对坐标值和绝对尺寸，转换为相对于anchor box的坐标值和尺寸。所谓box解码就是将box的相对坐标值和相对尺寸，转换回绝对坐标值和绝对尺寸。</p>
<p>在SSD算法中，box编解码中的box，是指预测框（predicted box）和真实框（ground-truth box）。SSD中的box编码，就是以anchor box为参照系，将predicted box和ground-truth box转换为用相对于anchor box的数值来表示；SSD中的box解码，则是将predicted box和ground-truth box转换回用绝对坐标系数值表示。</p>
<p>faster_rcnn_box_coder的解释详见<code>object_detection\box_coders</code>文件夹下的faster_rcnn_box_coder.py。重点分析一下下面这组转换公式：</p>
<pre><code>ty = (y - ya) / ha
tx = (x - xa) / wa
th = log(h / ha)
tw = log(w / wa)
</code></pre><p>x, y, w, h是待编码box的中心点坐标值、宽、高，即训练集中ground-truth box的数值；xa, ya, wa, ha是anchor box的中心点坐标值、宽、高; tx, ty, tw, th则是编码后的相对中心点坐标值、宽、高。在实际转换中，还会用缩放系数实现了归一化（normalization），归一化简单来说就是把数据分布变成标准正态分布（standard normal distribution），从中可以看出normalization就是来自标准正态分布的英文名词，而中文翻译成归一化是因为标准正态分布的大部分数据分布在（-1，1）之间。</p>
<p>faster_rcnn_box_coder{}里的y_scale、x_scale、height_scale、width_scale是对ty、tx、th、tw的放大比率，根据faster_rcnn_box_coder.py源码里编码/解码的语句可以看出，喂给模型的ty,tx,th,tw还会乘以/除以10和5。</p>
<p>关于上面谈到的归一化和放大比率，可阅读<a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/">这篇博客</a>的[Representation Encoding With Variance]部分。</p>
<p><strong>2.1.4  matcher、argmax_matcher</strong></p>
<pre><code>matcher &#123;
    argmax_matcher &#123;
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
    &#125;
&#125;
</code></pre><p>这部分参数用于设置anchor box和ground-truth box之间的匹配策略。  </p>
<p>可选参数值：</p>
<pre><code>ArgMaxMatcher argmax_matcher = 1;
BipartiteMatcher bipartite_matcher = 2;
</code></pre><p>SSD算法中，采用了ArgMaxMatcher策略，所以这里选择argmax_matcher。所谓ArgMaxMatcher策略，就是选取最大值策略。在<code>object_detection\matchers</code>文件夹的argmax_matcher.py源码里有详细解释。</p>
<p>在SSD算法中，用anchor box和ground-truth box的IOU值（Intersection Over Union）来作为阈值，设置了matched_threshold、unmatched_threshold两个阈值，这两个阈值把anchor box分成了3段，然后再根据negatives_lower_than_unmatched决定某个低于matched_threshold的anchor box是 负样本 还是 被忽略：</p>
<ul>
<li><p>（1） 当IOU &gt;= matched_threshold时：anchor box和ground-truth box匹配，该anchor box记为正样本。</p>
</li>
<li><p>（2） 当matched_threshold &gt; IOU &gt;= unmatched_threshold时，取决于     negatives_lower_than_unmatched。如果negatives_lower_than_unmatched=true时：所有中间态anchor box被忽略，低于unmatched_threshold的anchor box被记为负样本；negatives_lower_than_unmatched=false时：所有中间态anchor box被记为负样本，低于unmatched_threshold则被忽略。上述参数例中两个阈值都是0.5，故没有中间态。</p>
</li>
<li><p>（3） 当IOU &lt;= unmatched_threshold 时，也取决于     negatives_lower_than_unmatched, 规则跟上一条一样，由negatives_lower_than_unmatched来决定该anchor box是 负样本 还是 被忽略。</p>
</li>
</ul>
<p>ignore_thresholds: 源码<code>object_detection/protos/argmax_matcher.proto</code>的解释:Whether to construct ArgMaxMatcher without thresholds.这么看肯定是false的啦，怎么能不用阈值构造呢。</p>
<p>force_match_for_each_row：设置为true，以确保每个ground-truth box都至少有一个anchor box与之对应，防止有些ground-truth没有anchor box对应。否则，这些ground-truth box最后将没有bounding box回归对应，也就是产生了漏检。</p>
<p>在SSD算法中，将所有ground-truth box按行排列、将所有anchor box按列排列，形成一个矩阵。矩阵的每一格记录ground-truth box和anchor box的匹配结果。匹配分两步：</p>
<p>（1） 对每行的ground-truth box，采用ArgMax策略，选择与它的IOU最大的anchor box进行匹配；</p>
<p>（2） 对剩余的每一个没有匹配到ground-truth box的anchor box，选择所有与它的IOU大于match threshold的ground-truth box进行匹配。</p>
<p>这样，每个ground-truth box至少有一个anchor box与它匹配。</p>
<p><strong>2.1.5  similarity_calculator、iou_similarity</strong></p>
<pre><code>similarity_calculator &#123;
    iou_similarity &#123;
    &#125;
&#125;
</code></pre><p>这个参数选择使用何种标准计算相似度。在2.1.4小节中，已经解释了SSD算法是采用IOU值来定量衡量相似度的，故这里选择数值iou_similarity。</p>
<p><strong>2.1.6  anchor_generator、ssd_anchor_generator</strong></p>
<pre><code>anchor_generator &#123;
    ssd_anchor_generator &#123;
        num_layers: 6
        min_scale: 0.2
        max_scale: 0.95
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.3333
    &#125;
&#125;
</code></pre><p>Anchor box的生成器可以有如下选择：</p>
<pre><code>GridAnchorGenerator grid_anchor_generator = 1;
SsdAnchorGenerator ssd_anchor_generator = 2;
MultiscaleAnchorGenerator multiscale_anchor_generator = 3;
</code></pre><p>ssd_anchor_generator这部分设置了生成anchor box所需的一些参数。详细解释可参考SSD的论文。这里只解释一下各参数的基本含义。</p>
<p>num_layers: 数值为6，代表提取特征用的6个层。SSD算法借鉴了特征金字塔的思想，从6个feature map层同步提取特征,其大小分别是(38,38),(19,19),(10,10),(5,5),(3,3),(1,1)。</p>
<p><img src="https://s1.ax1x.com/2020/07/01/NHYlh6.png" alt="NHYlh6.png"></p>
<p><img src="https://s1.ax1x.com/2020/07/04/Nv5OIO.jpg" alt="Nv5OIO.jpg"></p>
<p>图像特征提取的原理是Receptive Field（感受域）。如下图所示，由于图像中央位置的特征被kernel扫描的次数最多，所以提取到的特征最多、颜色最深，而图像四个角只被kernel扫描过1、2次，所以提取到的特征最少、颜色最浅。因此，CNN对图像中越靠近中央位置、体型越大的物体的判断越准确。换句话说，边缘位置的物体识别难度更大。<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/8d894605bb06">[来源]</a></p>
<p><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201129234224.png" alt="img"></p>
<p>min_scale和max_scale：我目前的理解是：scale是同层的anchor boxes中的小正方形宽相对于resize的输入图像宽的比率。在SSD论文中，min_scale是指该比率在最低层feature map的值，max_scale是指该比率在最高层feature map的值。至于中间4层feature map上的比率值，论文中是以线性插值的方式来获得的（但参考代码中不是这样确定各层比率的）。更多关于SSD锚框生成的原理见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33544892">这篇知乎文章</a>。</p>
<p>aspect_ratios：指定了同一个feature map层上的anchor box的宽长比。例如，这里aspect ratios指定了5种宽长比，利用图1的公式（Sk: scale*resize width; ar: aspect_ratios）可以计算出6种不同宽长的anchor boxes（包括2种正方形、4种长方形）。注意：某些feature map层不使用3和1/3这一对aspect_ratios，故只生成4个anchor boxes。详细解释可参考SSD的论文。另外这些值的设置一定要根据数据集的特点来，比如检测目标是电线杆这种细长的东西，原本的那6个值就不适用了，如何使用kmeans方法聚类得到合适的anchor，可参见<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/2CAA4i9Nml43g5oc4p2-0Q">这篇微信推文</a>。</p>
<p><strong>2.1.7  image_resizer、fixed_shape_resizer</strong>   </p>
<pre><code>image_resizer &#123;
    fixed_shape_resizer &#123;
    height: 300
    width: 300
    &#125;
&#125;
</code></pre><p>这部分参数设置了对输入图像的resize策略。可选参数：</p>
<pre><code>KeepAspectRatioResizer keep_aspect_ratio_resizer = 1;
FixedShapeResizer fixed_shape_resizer = 2;
</code></pre><p>传统SSD300模型中，输入图像被统一resize为300*300。故这里选择fixed_shape_resizer，且height和width均设置为300。</p>
<p><strong>2.1.8  box_predictor</strong></p>
<pre><code>box_predictor &#123;
    convolutional_box_predictor &#123;
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.8
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
        conv_hyperparams &#123;
            activation: RELU_6,
            regularizer &#123;
                l2_regularizer &#123;
                    weight: 0.00004
                &#125;
            &#125;
            initializer &#123;
                truncated_normal_initializer &#123;
                    stddev: 0.03
                    mean: 0.0
                &#125;
            &#125;
            batch_norm &#123;
                train: true,
                scale: true,
                center: true,
                decay: 0.9997,
                epsilon: 0.001,
            &#125;
        &#125;
    &#125;
&#125;
</code></pre><p>Box predictors输入高层的feature map，输出两类预测：（1）编码后的predicted box的相对位置；（2）predicted box里的物体类别。</p>
<p>Box predictor的可选参数范围：</p>
<pre><code>ConvolutionalBoxPredictor convolutional_box_predictor = 1;
MaskRCNNBoxPredictor mask_rcnn_box_predictor = 2;
RfcnBoxPredictor rfcn_box_predictor = 3;
WeightSharedConvolutionalBoxPredictor weight_shared_convolutional_box_predictor = 4;
</code></pre><p>这里选用了convolutional_box_predictor，其含义是在所有feature maps后额外增加一个中间1x1卷积层。选择原因不明。</p>
<p>关于convolutional_box_predictor{ }内的许多参数的含义及数值范围，详见predictors文件夹下的convolutional_box_predictor.py文件（此文件在models 1.x和models 2.x里的内容不一样）。下面逐个简单说明一下。</p>
<p>min_depth和max_depth：在位置回归和类别分类之前额外插入的feature map层深度的最小值和最大值。当max_depth=0时，表示在位置回归和类别分类之前，不额外插入feature map。</p>
<p>num_layers_before_predictor：在检测器之前的额外convolutional层的层数。</p>
<p>use_dropout：对class prediction是否使用dropout来防止过拟合。</p>
<p>dropout_keep_probability：如果使用了dropout，dropout的数值保留概率。</p>
<p>kernel_size：最后的卷积核的尺寸。</p>
<p>box_code_size：我的理解是box需要编码的参数个数。SSD算法里是cx, cy, w, h这4个参数需要编码，所以box_code_size=4。</p>
<p>apply_sigmoid_to_scores：最后class prediction输出时是否采用sigmoid。</p>
<p>conv_hyperparams{ }：卷积操作超参数的设置。box_predictor和feature_extractor都有conv_hyperparams{}，conv_hyperparams{}里有activation、regularizer、initializer、batch_norm。源码在<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/451906e4e82f19712455066c1b27e2a6ba71b1dd/research/object_detection/builders/hyperparams_builder.py#L27:7">hyperparams_builder.py</a>。</p>
<p>activation：激活函数。目前可以选择NONE、RELU、RELU_6。这里选择了RELU_6。关于激活函数的细节，请自行查阅资料。</p>
<p>regularizer：正则化操作。目前可以选择l1_regularizer、l2_regularizer。这里选择了l2_regularizer。例子里L2_regularizer的weight值只有0.00004，所以正则化操作对loss的影响较小。</p>
<p>Initializer{ }：随机数初始化机制设置。可选参数如下：</p>
<pre><code>TruncatedNormalInitializer truncated_normal_initializer = 1;
VarianceScalingInitializer variance_scaling_initializer = 2;
RandomNormalInitializer random_normal_initializer = 3; 
</code></pre><p>这里选择了truncated_normal_initializer。</p>
<p>truncated_normal_initializer：截断的正态分布随机数初始化机制。如果数值超过mean两个stddev，则丢弃。</p>
<p>batch_norm{ }：关于Batch Normalization（批归一化）的一些参数设置。Batch Normalization可以强制将输入激活函数的数值分布拉回标准正态分布，以防止训练过程中产生反向梯度消失，加速训练收敛。批归一化中会用到两个参数γ和β，分别定量化缩放和平移操作。细节请自行参阅相关资料。这里解释一下batch_norm的一些参数：：</p>
<p>train: true——如果为true，则在训练过程中batch norm变量的值会更新，也就是得到了训练。如果为false，则在训练过程中batch norm变量的值不会更新。</p>
<p>scale: true——如果为true,则乘以γ；如果为false,则不使用γ。</p>
<p>center: true——如果为true,则加上β的偏移量；如果为false,则忽略β。</p>
<p>decay: 0.9997——衰减率。作用存疑。</p>
<p>epsilon: 0.001——添加到方差的小浮点数,以避免除以零。</p>
<p>关于Box predictor的源码解释详见core文件夹下的box_predictor.py。</p>
<p><strong>2.1.9  feature_extractor</strong></p>
<pre><code>feature_extractor &#123;
    type: &#39;ssd_mobilenet_v1&#39;
    min_depth: 16
    depth_multiplier: 1.0
    conv_hyperparams &#123;
    activation: RELU_6,
        regularizer &#123;
            l2_regularizer &#123;
                weight: 0.00004
            &#125;
        &#125;
        initializer &#123;
            truncated_normal_initializer &#123;
                stddev: 0.03
                mean: 0.0
            &#125;
        &#125;
        batch_norm &#123;
            train: true,
            scale: true,
            center: true,
            decay: 0.9997,
            epsilon: 0.001,
        &#125;
    &#125;
&#125;
</code></pre><p>这部分设置了特征提取器相关层的参数。</p>
<p>不同模型的feature extractor的基类定义，详见meta_architectures文件夹下的对应文件。例如SSD模型的SSDFeatureExtractor基类，定义在<code>object_detection\meta_architectures</code>文件夹下的ssd_meta_arch.py文件里。</p>
<p>不同预训练模型的feature extractor的类定义，详见models文件夹下的对应文件。例如ssd_mobilenet_v1预训练模型的feature extractor类，定义在<code>object_detection\models</code>文件夹下的</p>
<p>ssd_mobilenet_v1_feature_extractor.py文件里。</p>
<p>models文件夹下的feature extractor类，是meta_architectures文件夹下的feature extractor基类的子类。</p>
<p>下面解释一下相关参数。</p>
<p>type：例子中使用了预训练模型ssd_moiblenet_v1的feature_extractor，所以这里填写’ssd_mobilenet_v1’。</p>
<p>min_depth：最小的特征提取器的深度。这里填16。原因不明。</p>
<p>depth_multiplier：是施加在每个input通道上的卷积核的数目。用于计算深度卷积分离后的输出通道总数。这里是一个浮点数。</p>
<p>conv_hyperparams超参数的含义和配置同2.1.8小节，不再重复解释。</p>
<p><strong>2.1.10  loss</strong></p>
<pre><code>loss &#123;
    classification_loss &#123;
        weighted_sigmoid &#123;
        &#125;
    &#125;
    localization_loss &#123;
        weighted_smooth_l1 &#123;
        &#125;
    &#125;
    hard_example_miner &#123;
        num_hard_examples: 3000
        iou_threshold: 0.99
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 0
    &#125;
    classification_weight: 1.0
    localization_weight: 1.0
&#125;
</code></pre><p>这部分设置了损失函数loss相关的参数，详细源码解释见<code>object_detection\core\losses.py</code>或者github上的<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/420a7253e034a12ae2208e6ec94d3e4936177a53/research/object_detection/core/losses.py">源码</a>。</p>
<p>SSD算法的loss分为目标分类损失函数(classification loss)和目标位置损失函数（localization loss）：</p>
<p><img src="https://s1.ax1x.com/2020/07/04/NvoM1H.png" alt="NvoM1H.png"></p>
<p>这里设置了classification_loss、localization_loss、hard_example_miner、classification_weight、localization_weight这5个参数。下面解释一下这5个参数的具体配置。</p>
<p>classification_loss：可选参数：</p>
<pre><code>WeightedSigmoidClassificationLoss weighted_sigmoid = 1;

WeightedSoftmaxClassificationLoss weighted_softmax = 2;

WeightedSoftmaxClassificationAgainstLogitsLoss weighted_logits_softmax = 5;

BootstrappedSigmoidClassificationLoss bootstrapped_sigmoid = 3;

SigmoidFocalClassificationLoss weighted_sigmoid_focal = 4;
</code></pre><p>这里用到的是weighted_sigmoid, <a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/8518d053936aaf30afb9ed0a4ea01baddca5bd17/research/object_detection/core/losses.py#L217">源码</a>。论文里是说用softmax cross entropy，但这里config用的是sigmoid cross entropy，这个问题在<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/issues/2213">github issue</a>也有人提到，有人说用weighted_softmax，准确度会降低。我在tensorflow/models的github里搜了一下weighted_softmax，发现没有一个config用了weighted_softmax，可能是它真的不好用吧。</p>
<p>定义分类输出的激活函数。具体含义请自行查阅资料。这里的设置和前面Box predictor部分的apply_sigmoid_to_scores设置是否有冲突，暂时没能确认。</p>
<p>localization_loss：可选参数：</p>
<pre><code>WeightedL2LocalizationLoss weighted_l2 = 1;

WeightedSmoothL1LocalizationLoss weighted_smooth_l1 = 2;

WeightedIOULocalizationLoss weighted_iou = 3;
</code></pre><p>这里用到的是weighted_smooth_l1,<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/8518d053936aaf30afb9ed0a4ea01baddca5bd17/research/object_detection/core/losses.py#L145">源码</a>里的定义是：Smooth L1 localization loss function aka Huber Loss. The smooth L1_loss is defined elementwise as .5 x^2 if |x| &lt;= delta and delta <em> (|x|- 0.5</em>delta) otherwise, where x is the difference between predictions and target.中文关于Huber loss的介绍见这篇<a target="_blank" rel="noopener" href="https://www.cnblogs.com/nowgood/p/Huber-Loss.html">博客</a>。</p>
<p>定义了用于localization loss的正则化方法。具体含义请自行查阅资料。这里的设置和前面Box predictor部分的正则化设置是否有冲突，暂时没能确认。</p>
<p>hard_example_miner：难样本挖掘策略。<br>因为ssd-300共有8732个先验框，前期训练的时候，并不是每一张图片输进去，就基于这8732个先验框给一个损失函数，然后反向传播。而是在8732个先验框中有针对性的选择一部分来作为训练数据，包括正例（含有目标的框）和反例（背景框），反例（背景框）的选取还是有讲究的，毕竟大部分框都是背景框，如果都拿来训练，那么会造成严重的类别不平衡。SSD算法随机抽取一定数量的负样本，按loss_type的损失量进行降序排列，选择前3N个作为训练用负样本，以保证训练时的正负样本比例接近1:3，N为正样本数量。那这些筛选过的样本去训练后会出现许多false positive。把其中得分较高的这些false positive当做所谓的Hard negative，既然mining出了这些Hard negative，就把这些扔进网络再训练一次，从而加强分类器判别假阳性的能力。训练hard negative对提升网络的分类性能具有极大帮助，因为它相当于一个错题集。下面解释一下它的具体子参数含义。</p>
<p>num_hard_examples: 3000——难样本数目。</p>
<p>iou_threshold: 0.99——在NMS（非极大抑制）阶段，如果一个样本相对于最大loss样本的IOU值比此阈值高，则认为此样本是重复的、可丢弃。SSD里设置成0.99 几乎相当于不进行NMS，不丢弃任何负样本，只是单纯的按损失排序。这个参数尽量不要动，否则就违背了SSD模型的原意。</p>
<p>loss_type: CLASSIFICATION——挖掘策略是否只使用classification loss，或只使用localization loss，或都使用。可选参数：</p>
<pre><code>BOTH = 0; 
CLASSIFICATION = 1; 
LOCALIZATION = 2;
</code></pre><p>max_negatives_per_positive: 3   —-  每1个正样本对应的最大负样本数。</p>
<p>min_negatives_per_image: 0  —  在ssd_mobilenet_v2_coco.config里，该值=3，在图片没有正样本的极端情况下，如果把这个值设置为一个正数，可以避免模型在图片上检测出目标来，防止了误检出。</p>
<p>classification_weight：用于配置classifiation loss在总loss中的权重。</p>
<p>localization_weight：用于配置localization loss在总loss中的权重。</p>
<p><strong>2.1.11  normalize_loss_by_num_matches</strong></p>
<p>我的理解：如果选true,则根据匹配的样本数目归一化总loss，也就是总loss公式中，用加权的classification loss和加权的localization loss计算出总loss后，还要再除以一个正样本总数N。如果选false，则计算出总loss后，不用再除以正样本总数N。</p>
<p><strong>2.1.12  post_processing</strong></p>
<pre><code>post_processing &#123;
batch_non_max_suppression &#123;
    score_threshold: 1e-8
    iou_threshold: 0.6
    max_detections_per_class: 100
    max_total_detections: 100
&#125;
score_converter: SIGMOID
&#125;
</code></pre><p>这部分配置了SSD算法的后处理阶段的参数。下面逐一解释。</p>
<p>batch_non_max_suppression{ }：这部分配置了批次的NMS策略的参数。先简单解释下NMS策略的目的。以目标检测为例，在最后阶段，一个目标上可能有很多个bounding box，但是最终目标检测框只有一个。因此，我们可以用NMS策略，逐次过滤掉其余的bounding box，最终只保留一个bounding box作为结果。下面简单解释一下具体参数含义：</p>
<p>score_threshold: 1e-8 —— 分数低于此阈值的box被过滤掉,想参与NMS的门都没有。</p>
<p>iou_threshold: 0.6 —— 和置信度最高的box的IOU值超过此阈值的box被过滤掉，如果场景是目标比较密集，那这个值要设大一点，如果场景比较稀疏，这个值要设小一点。</p>
<p>max_detections_per_class: 10 —— 每个类别可保留的检测框的最大数目。</p>
<p>max_total_detections: 100 —— 所有类别可保留的检测框的最大数目，一般这个值应该等于max_detections_per_class乘以类别数。</p>
<p>score_converter：检测分数的转换器类型选择。可选参数：</p>
<pre><code>// Input scores equals output scores.
IDENTITY = 0;

// Applies a sigmoid on input scores.
SIGMOID = 1;

// Applies a softmax on input scores.
SOFTMAX = 2;
</code></pre><p><strong>2.2  train_config{ }</strong></p>
<p>训练用参数的配置。详见protos文件夹下的train.proto。下面解释.config例中的参数。</p>
<ul>
<li><p><strong>2.2.1  batch_size</strong></p>
<p>每个批次的训练样本数。batch_size × num_steps = num_examples × epochs。batch size不是说你硬件能力强就能设置得很大，太大的batch size会导致参数的更新次数变少，模型的质量（泛化能力）会变差。</p>
</li>
<li><p><strong>2.2.2  optimizer{ }</strong></p>
<p>优化器的参数配置部分。</p>
<p>由于优化器的配置很关键，所以这部分想更详细展开一些。首先介绍一下参数的含义及可选范围，然后贴出一个优化器配置的例子。</p>
<p>目前可选的优化器参数：</p>
<pre><code>  RMSPropOptimizer rms_prop_optimizer
  MomentumOptimizer momentum_optimizer
  AdamOptimizer adam_optimizer
</code></pre><p>关于这三种优化器的特性，可查看我的另一篇<a target="_blank" rel="noopener" href="https://wwdok.github.io/2020/06/27/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">文章</a>。</p>
<p> rms_prop_optimizer的可选参数：</p>
<pre><code>  LearningRate learning_rate = 1;
  float momentum_optimizer_value = 2 [default = 0.9];
  float decay = 3 [default = 0.9];
  float epsilon = 4 [default = 1.0];
</code></pre><p>momentum_optimizer的可选参数：</p>
<pre><code>  LearningRate learning_rate = 1;
  float momentum_optimizer_value = 2 [default = 0.9];
</code></pre><p>adam_optimizer的可选参数：</p>
<pre><code>  LearningRate learning_rate = 1;
</code></pre><p>3种优化器都有学习率learning_rate，它的的可选参数有：</p>
<pre><code>  ConstantLearningRate constant_learning_rate = 1;
  ExponentialDecayLearningRate exponential_decay_learning_rate = 2;
  ManualStepLearningRate manual_step_learning_rate = 3;
  CosineDecayLearningRate cosine_decay_learning_rate = 4;
</code></pre><p>  解释如下：</p>
<p>  ============（1）============</p>
<p>  <strong>constant_learning_rate</strong>：恒定学习率。恒定学习率太小则收敛很慢；太大则在极值附近震荡难以收敛。故一般不会使用。</p>
<p>  ============（2）============</p>
<p>  <strong>exponential_decay_learning_rate</strong>：学习率按照指数规律衰减。</p>
<p>  exponential_decay_learning_rate可选参数：</p>
<pre><code>  float initial_learning_rate [default = 0.002];
  uint32 decay_steps [default = 4000000];
  float decay_factor [default = 0.95];
  bool staircase [default = true];
  float burnin_learning_rate [default = 0.0];
  uint32 burnin_steps [default = 0];
  float min_learning_rate [default = 0.0];
</code></pre><p>  initial_learning_rate：初始学习率数值。</p>
<p>  decay_steps：衰减周期。即每隔decay_steps步衰减一次学习率。下面例1中写的是800720步，而总的训练步数不过才200000步，显然decay_steps的设置偏大了，导致在整个训练过程中，学习率实际上没有明显的指数衰减。这个设置不合理。这个decay_steps应该等于训练总步数num_steps。</p>
<p>  decay_factor：每次衰减的衰减率。</p>
<p>  staircase：是否阶梯性更新学习率，也就是每次衰减结果是向下取整还是float型。</p>
<p>  burnin_learning_rate：采用burnin策略进行调整的学习率（初始值？）。SSD算法中，是否有burnin策略、buinin策略又是如何调整学习率的，目前我还不太清楚。存疑。参考：在yolov3所用的darknet中，当学习率更新次数小于burnin参数时，学习率从小到大变化；当更新次数大于burnin参数后，学习率按照配置的衰减策略从大到小变化。</p>
<p>  burnin_steps：按照字面意思，是burnin策略的调整周期。即每隔burnin_steps步调整一次burnin_learning_rate。</p>
<p>  min_learning_rate：最小学习率。采用衰减策略变小的学习率不能小于该值。</p>
<p>  ============（3）============</p>
<p>  <strong>manual_step_learning_rate</strong>：学习率按照人工设置的step逐段变小。</p>
<p>  manual_step_learning_rate可选参数：</p>
<pre><code>  float initial_learning_rate = 1 [default = 0.002];
  message LearningRateSchedule &#123;
  　　optional uint32 step = 1;
  　　optional float learning_rate = 2 [default = 0.002];
  &#125;
  repeated LearningRateSchedule schedule = 2;
  optional bool warmup = 3 [default = false];
</code></pre><p>  简单解释如下：</p>
<p>  initial_learning_rate：初始学习率数值。</p>
<p>  schedule：人工规划策略。包含两个参数：</p>
<p>  step——当前阶梯从全局的第step步开始。</p>
<p>  learning_rate——当前阶梯的学习率。</p>
<p>  warmup：对于全局步数区间[0, schedule.step]之间的steps，是否采用线性插值法来确定steps对应的学习率。缺省是false。</p>
<p>  ============（4）============</p>
<p>  <strong>cosine_decay_learning_rate</strong>：学习率按照余弦规律衰减。<br>  <img src=https://3.bp.blogspot.com/-fAN358JEMLc/Wrv1iH17eiI/AAAAAAAAChg/0djM3boHeJA_V_JfBHH8dMS32ekgtic7QCLcBGAs/s1600/image1.png></p>
<p>  它有四个配置参数：</p>
<pre><code>    learning_rate_base: .2
    total_steps: 50000
    warmup_learning_rate: 0.06
    warmup_steps: 2000
</code></pre><p>  =============================</p>
<p>  优化器还有3个独立参数：</p>
<p>  momentum_optimizer_value： momentum超参数。通过引入这个超参数（公式中一般记为γ），可以使得优化在梯度方向不变的维度上的更新速度变快，在梯度方向有所改变的维度上的更新速度变慢，从而加快收敛并减小震荡。</p>
<p>  decay：衰减率。含义和出处不明。</p>
<p>  epsilon：可能是迭代终止条件。</p>
</li>
</ul>
<p><strong>优化器配置例子：</strong></p>
<p>优化器使用rms_prop_optimizer。</p>
<p>采用指数衰减策略来调整学习率。</p>
<pre><code>optimizer &#123;
    rms_prop_optimizer: &#123;
        learning_rate: &#123;
            exponential_decay_learning_rate &#123;
                initial_learning_rate: 0.0001
                decay_steps: 800720
                decay_factor: 0.95
            &#125;
        &#125;
        momentum_optimizer_value: 0.9
        decay: 0.9
        epsilon: 1.0
    &#125;
&#125;
</code></pre><p><strong>2.2.3  fine_tune_checkpoint</strong></p>
<p>用于设置预训练模型的参数文件model.ckpt的路径。该参数文件用于精调。当训练开始时，导入已预训练好的模型参数，可以缩短训练过程。从零开始训练时，由于没有预训练模型的参数文件，故可以屏蔽这个路径参数。</p>
<p><strong>2.2.4  fine_tune_checkpoint_type</strong></p>
<p>fine_tune_checkpoint_type：用来确定fine tune checkpoint使用的是分类模型参数还是检测模型参数。可选参数值：“”，“classification”，“detection”。 </p>
<p><strong>2.2.5  load_all_detection_checkpoint_vars</strong></p>
<p>用于确定是否导入所有和模型变量名字和大小相符的detection checkpoint变量。只在使用检测模型时有效。</p>
<p><strong>2.2.6  num_steps</strong></p>
<p>训练总步数。如果设置为0，则训练步数为无穷大。</p>
<p><strong>2.2.7  data_augmentation_options</strong></p>
<p>这个例子里数据增强使用了两个具体参数：</p>
<p>random_horizontal_flip——随机水平翻转。</p>
<p>ssd_random_crop——SSD算法图像随机裁剪。</p>
<p>常用的还有random_adjust_brightness、<br>random_adjust_contrast、<br>random_adjust_saturation。</p>
<p>更多可选参数详见protos文件夹下的<a target="_blank" rel="noopener" href="https://github.com/tensorflow/models/blob/master/research/object_detection/protos/https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto">preprocessor.proto</a>。</p>
<p><strong>2.3  train_input_reader{ }</strong></p>
<p>训练集数据的路径配置。可选参数详见protos文件夹下的input_reader.proto。</p>
<ul>
<li><p><strong>2.3.1  tf_record_input_reader、input_path</strong></p>
<p>训练用tf_record格式数据集的路径配置。</p>
</li>
<li><p><strong>2.3.2  label_map_path</strong></p>
<p>labelmap.pbtxt文件的路径配置。labelmap.pbtxt文件定义了待分类目标的id号和标签名称之间的映射关系。</p>
</li>
</ul>
<p><strong>2.4  eval_config{ }</strong></p>
<p>测试用参数的配置。可选参数详见protos文件夹下的eval.proto。</p>
<ul>
<li><p><strong>2.4.1  metrics_set</strong></p>
<p>用于配置评估模型性能的标准。</p>
<p>可选参数详见框架总目录下eval_util.py里的EVAL_METRICS_CLASS_DICT。目前有8种。</p>
<p>例子中使用的是coco_detection_metrics, 是使用coco数据集进行目标检测时评估模型性能的标准。</p>
</li>
<li><p><strong>2.4.2  num_examples</strong></p>
<p>测试集的样本数量。</p>
</li>
</ul>
<p><strong>2.5  eval_input_reader{ }</strong></p>
<p>测试集数据的路径配置。可选参数详见protos文件夹下的input_reader.proto。</p>
<ul>
<li><p><strong>2.5.1  tf_record_input_reader、input_path</strong></p>
<p>测试用tf_record格式数据集的路径配置。</p>
</li>
<li><p><strong>2.5.2  label_map_path</strong></p>
<p>labelmap.pbtxt文件的路径配置。labelmap.pbtxt文件定义了待分类目标的id号和标签名称之间的映射关系。</p>
</li>
<li><p><strong>2.5.3  shuffle</strong></p>
<p>随机排序操作配置。如果选false，则对测试样本不进行随机排序操作。</p>
</li>
<li><p><strong>2.5.4  num_readers</strong></p>
<p>用于配置可并行读入的文件分片的数目。</p>
</li>
</ul>
<p>==================================================</p>
<p>以上的配置只是针对ssd_mobilenet_v1_coco.config，再阅读后来的config时，比如ssd_mobilenet_v2_quantized_300x300_coco.config，我们还会看到一些稍微不同的配置：</p>
<ul>
<li><p>对于一些想要量化的模型，它的配置文件最后还会有下面的内容：</p>
  <figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph_rewriter &#123;</span><br><span class="line">    quantization &#123;</span><br><span class="line">        delay: 48000  </span><br><span class="line">        activation_bits: 8 </span><br><span class="line">        weight_bits: 8</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>  delay的源码解释是“ Number of steps to delay before quantization takes effect during training.”<br>  delay值一般等于num_steps的90% ~ 96%，90%是偏向那些总步数比较少的情况，比如2千步。96%是偏向于总步数比较多的情况，比如5万步。</p>
</li>
<li><p>classification_loss 由weighted_sigmoid { }变成了weighted_sigmoid_focal {<br>alpha: 0.75, gamma: 2.0 },focal loss的定义如下：<br><img src="https://s1.ax1x.com/2020/07/16/U05Aeg.png" alt="U05Aeg.png" border="0" /><br><img src="https://gitee.com/wwdok/my-image-bed/raw/master/img/20201129234351.png" alt="img"></p>
<p>  从数学公式可以看出，focal loss是scale版的cross entropy，-(1-pt)^γ是可训练的scale值。 Focal loss对well-classified examples降权，降低它们的loss值，也就是减少参数更新值，把更多优化空间留给预测概率较低的样本。</p>
<p>  当gamma == 0时，focal loss就相当于corss entropy(CE)，如蓝色曲线所示，即使probability达到0.6，loss值还是&gt;= 0.5，就好像说：“我判断它不是类别B的概率是60%，恩，我还要继续努力优化参数，我行的，其他事情不要来烦我，我要跟它死磕到底”。而当gamma == 2时，同样是probability达到0.6，loss值却接近于0，就好像是说：“我判断它不是类别B的概率是60%，恩，根据我多年断案经验，它一定不是分类B，虽然判断依据不是很高，但我宣布，结案了，这页翻过去了，接下来我要把精力投入到那些预测准确率还很低的案子”。[<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ad20e1a429fc">来源</a>]</p>
<p>  另外如果用了focal loss，就不能再用HardExampleMiner 。<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/293369755/answer/862562443">有人说</a>用了focal loss后，建议把unmatched_threshold调低到0.3,为了忽略掉中间状态的样本，这样focal loss 就不会给这些中间状态（非困难样本）赋予太大的权重。</p>
</li>
<li><p><strong>freeze_batchnorm</strong>: whether to freeze batch norm parameters during training</p>
<pre><code>  or not. When training with a small batch size (e.g. 1), it is desirable
  to freeze batch norm update and use pretrained batch norm params.
</code></pre></li>
<li><strong>inplace_batchnorm_update</strong>: whether to update batch norm moving average<pre><code>  values inplace. When this is false train op must add a control
  dependency on tf.graphkeys.UPDATE_OPS collection in order to update
  batch norm statistics.
</code></pre></li>
<li><strong>use_depthwise</strong> 在SSDLite的config中，box_predictor 和feature_extractor里会多出来 use_depthwise=true。</li>
<li><strong>kernel_size</strong> 为了配合use_depthwise=true，kernel_size会从1变成3。StackOverFlow上有关于SSD和SSDLIte的<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/50674448/what-is-the-different-between-ssd-and-ssd-lite-tensorflow">提问</a>。两者的对比：</li>
</ul>
<p><img src=https://pic2.zhimg.com/80/v2-e9d9684b673410de83a3ada1315a82b0_720w.jpg width=60%></p>
<p><strong>实战心得</strong></p>
<p>在我使用自定义的ssd_mobilenet_v2_quantized_300x300_coco.config训练自己的数据集时，发现它对小目标的检测正确率几乎为0，后来在TensorBoard里点开测试图片，才发现原来我的数据集图片被resize成300×300后，bbox里的目标有多“马赛克”，比如下面这张：</p>
<p><img src="https://s1.ax1x.com/2020/08/16/dEZOQU.png" alt="dEZOQU.png"></p>
<p>放大后看是这样的：</p>
<p><img src="https://s1.ax1x.com/2020/08/16/dEePW6.png" alt="dEePW6.png"></p>
<p>数一下，左边那个目标的宽度只有11个像素！而且凭感觉，也很难看出这么少的像素能提取出多好的特征！我看我下次要试试320×320了！</p>
<blockquote>
<p>本文主要基于<a target="_blank" rel="noopener" href="https://www.cnblogs.com/hillsea/p/13216716.html"><strong>这篇博客</strong></a>加上自己的理解思考而来。<br><br>拓展阅读：<br><br><a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/">《Bounding Box Encoding and Decoding in Object Detection》</a><br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33544892">《目标检测|SSD原理与实现》</a><br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31427288">《SSD目标检测》</a><br><br><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/2CAA4i9Nml43g5oc4p2-0Q">《新手也能彻底搞懂的目标检测Anchor是什么？怎么科学设置？》</a><br><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/lliuye/p/9354972.html">《L1正则化与L2正则化的理解》</a><br><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/red_stone1/article/details/80755144">《机器学习中 L1 和 L2 正则化的直观解释》</a><br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35356992">《L1正则化与L2正则化》</a><br><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/22159946">《Sigmoid vs Softmax 输出层选择》</a><br><br><a target="_blank" rel="noopener" href="https://docs.google.com/presentation/d/1rtfeV_VmdGdZD5ObVVpPDPIODSDxKnFSU0bsN_rgZXc/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.g178a005570_0_21143">《Single shot MultiBox Detector<br>》俄文版</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70703846">《轻量级神经网络“巡礼”（二）—— MobileNet，从V1到V3》</a><br><br><a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Focal-Loss-Explained/">《Use Focal Loss To Train Model Using Imbalanced Dataset》</a><br><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1396369">《COCO 数据集目标检测等相关评测指标》</a></p>
</blockquote>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://example.com/2020/07/01/object-detection-api%E8%B0%83%E5%8F%82%E8%AF%A6%E8%A7%A3/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SSD/" rel="tag">SSD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%B0%83%E5%8F%82/" rel="tag">调参</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2020/07/04/%E7%99%BE%E7%A7%91%E7%9F%A5%E8%AF%86%E5%A4%A7%E6%94%B6%E8%97%8F/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            百科知识大收藏
          
        </div>
      </a>
    
    
      <a href="/2020/06/30/%E7%94%A8python%E6%8A%95%E8%B5%84%E7%90%86%E8%B4%A2/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">用python投资理财</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "Y8oqscHrnLuIwp1649iHgWjM-gzGzoHsz",
    app_key: "IjneyzqTD2fkFsPSEFKEW0lN",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "有什么想说的请在此留言~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2021
        <i class="ri-heart-fill heart_icon"></i> Wade Wang
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        Powered by <a href="https://hexo.io" target="_blank">Hexo</a>
        <span class="division">|</span>
        Theme - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/yoga.png" alt="Hello World"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="http://wwdok.lofter.com">摄影</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯果汁吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true
  }
</script>

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>